\newpage
\appendix
\section{Omitted Proofs}

This section shows the omitted (but non-trivial) proofs.

\subsection{Proof Theorem~\ref{them:PG}}
\begin{proof}
The gradient of $V$ function can be written in terms of $Q$ function,

{\footnotesize
    \begin{equation}
        \begin{aligned}
            \nabla V^{\pi}(s) &= \nabla\left [ \sum_a \pi(a|s) Q^\pi (s,a)\right ] \\
            &=\sum_a\left [ \nabla\pi(a|s) Q^\pi (s,a) +\pi(a|s) \nabla Q^\pi (s,a) \right] \\
            &=\sum_a\left [ \nabla\pi(a|s) Q^\pi (s,a) +\pi(a|s) \nabla \sum_{s'}P(s'|s, a)(r+V^\pi(s')) \right] \\
            &\stackrel{\text{(i)}}{=}\sum_a\left [ \nabla\pi(a|s) Q^\pi (s,a) +\pi(a|s) \sum_{s'}P(s'|s, a)\nabla V^\pi(s') \right ], \\
        \end{aligned}\nonumber
    \end{equation}
}
where we can have the derivation (i) since $r$ only depends on the environment dynamics.

Let $\phi(s) = \sum_a \nabla\pi(a|s) Q^\pi (s,a)$. We use $\rho^\pi(s\rightarrow x, k)$ to represent the probability of transitioning from state $s$ to $x$ with policy $\pi$ after $k$ steps, i.e., $\rho^\pi(s \rightarrow s', 1)=\sum_a \pi(a|s) P(s'|s, a)$. Thus, we can unroll the recursive form as below,

{\footnotesize
    \begin{equation} 
        \begin{aligned}
            \nabla V^{\pi}(s) &=\phi(s) +\sum_a \pi(a|s) \sum_{s'}P(s'|s, a)\nabla V^\pi(s') \\
            &= \phi(s) + \sum_{s'}\rho^\pi(s \rightarrow s', 1) \nabla V^\pi(s') \\
            &= \phi(s) + \sum_{s'}\rho^\pi(s \rightarrow s', 1)\left[\phi(s') + \sum_{s''}\rho^\pi(s' \rightarrow s'', 1) \nabla V^\pi(s'')\right] \\
            &= \phi(s) + \sum_{s'}\rho^\pi(s \rightarrow s', 1) \phi(s') + \sum_{s''}\rho^\pi(s \rightarrow s'', 2) \nabla V^\pi(s'')  \\
            &= \phi(s) + \sum_{s'}\rho^\pi(s \rightarrow s', 1) \phi(s') + 
             \sum_{s''}\rho^\pi(s \rightarrow s'', 2) \phi(s'') +
            \sum_{s'''}\rho^\pi(s \rightarrow s''', 3) \nabla V^\pi(s''')  \\
            & \qquad \qquad \vdots \\
            &= \sum_{k=0}^{\infty}\sum_{x}\rho^\pi(s\rightarrow x, k)\phi(x)\\
        \end{aligned}\nonumber
    \end{equation}
}

We use $\eta(s)$ to represent the expected number of visits for $s$ in a single episode (in episodic case $\sum_s \eta(s)$ is the averaged length of an episode; in continuous case $\sum_s \eta(s)=1$). By plugging it into the object function $J$,
\begin{equation}
    \begin{aligned}
        \nabla J(\theta) &= \nabla V^\pi (s_0)   \\
        &= \sum_s \left( \sum_{k=0}^\infty \rho^\pi(s_0 \rightarrow s, k) \right) \sum_a \nabla\pi(a|s) Q^\pi (s,a) & \\
        &= \sum_s \eta(s) \sum_a \nabla\pi(a|s) Q^\pi (s,a)  \\
        &\stackrel{\text{norm}}{=} \left(\sum_s \eta(s) \right) \left(\sum_s \frac{\eta(s)}{\sum_s \eta(s)} \right )\sum_a \nabla\pi(a|s) Q^\pi (s,a) \\
        &\propto \sum_s d^\pi(s)\sum_a \nabla\pi(a|s) Q^\pi (s,a)
    \end{aligned}\nonumber
\end{equation}
\end{proof}

\subsection{Proof of Theorem~\ref{them:PG-baseline}}

\begin{proof}
We first prove PG with baseline is unbiased,
    \begin{equation}
        \begin{aligned}
            &\quad\mathbb{E}_{ d^\pi}\left [\sum_a (Q^\pi(s,a) -b(s)) \nabla\ln\pi(a|s)\right]\\
            &=\mathbb{E}_{ d^\pi}\left [\sum_a Q^\pi(s,a)\nabla\ln\pi(a|s)\right] -\mathbb{E}_{ d^\pi}\left [\sum_a b(s) \nabla\ln\pi(a|s)\right]\\
            &= \mathbb{E}_{ d^\pi}\left [\sum_a Q^\pi(s,a)\nabla\ln\pi(a|s)\right] - \mathbb{E}_{ d^\pi}\left [b(s) \nabla\sum_a \ln\pi(a|s)\right]\\
            &=\mathbb{E}_{ d^\pi}\left [\sum_a Q^\pi(s,a)\nabla\ln\pi(a|s)\right] - \mathbb{E}_{ d^\pi}\left [b(s) \nabla 1\right]\\
            &=\mathbb{E}_{ d^\pi}\left [\sum_a Q^\pi(s,a)\nabla\ln\pi(a|s)\right],
        \end{aligned} \nonumber
    \end{equation}
and the variance of PG with baseline is,
    \begin{equation}
        \begin{aligned}
            &\quad \mathbb{V}_{ d^\pi}
            \left [\sum_a (Q^\pi(s,a) -b(s)) \nabla\ln\pi(a|s)\right]\\ 
            &\stackrel{\text{(i)}}{\gtrapprox} \sum_a \mathbb{E}_{ d^\pi} \left [ \left((Q^\pi(s,a) -b(s)) \nabla\ln\pi(a|s)\right)^2\right] - \left(\mathbb{E}_{ d^\pi} \left [\sum_a (Q^\pi(s,a) -b(s)) \nabla\ln\pi(a|s)\right]\right)^2 \\
             &\stackrel{\text{(ii)}}{\approx} \sum_a \mathbb{E}_{ d^\pi} \left [\left(Q^\pi(s,a) -b(s)\right)^2\right] \mathbb{E}_{ d^\pi}\left[\left(\nabla\ln\pi(a|s)\right)^2\right] - \left(\mathbb{E}_{d^\pi} \left [\sum_a Q^\pi(s,a) \nabla\ln\pi(a|s)\right]\right)^2 \\
             &< \sum_a \mathbb{E}_{ d^\pi} \left [\left(Q^\pi(s,a)\nabla\ln\pi(a|s)\right)^2\right] - \left(\mathbb{E}_{ d^\pi} \left[\sum_a Q^\pi(s,a) \nabla\ln\pi(a|s)\right]\right)^2 \\
             &\stackrel{\text{(iii)}}{\lessapprox} \mathbb{E}_{ d^\pi} \left [\left(\sum_a Q^\pi(s,a)\nabla\ln\pi(a|s)\right)^2\right] - \left(\mathbb{E}_{ d^\pi} \left[\sum_a Q^\pi(s,a) \nabla\ln\pi(a|s)\right]\right)^2 \\
             &= \mathbb{V}_{ d^\pi} \left[ \sum_a Q^\pi(s,a)\nabla \ln \pi(a|s)\right].
        \end{aligned} \nonumber
    \end{equation}
    
    In approximations (i) and (iii), we only keep the quadratic term and omit the products, but this won't affect the property of the inequality because the deduction loss caused by $\prod_a (Q^\pi(s,a) -b(s)) \nabla\ln\pi(a|s)$ is less than the increase we compensate for $\prod_a Q^\pi(s,a) \nabla\ln\pi(a|s)$. In approximation (ii), we assume independence among the values involved in the expectation for factorization. This reveals that REINFORCE has less variance when using a baseline; and when $b(s)\approx V^\pi(s)$, the variance reaches optimal.
\end{proof}

\newpage
\section{Other Forms of PG}

Since the major purpose of this article is to introduce PPO methods from PG, we omit some other important forms of PG in the main body. Here are the complements.

\subsection{Deterministic PG}

Sometimes we hope the policy function to be deterministic to reduce the gradient estimation variance and improve the exploration efficiency for continuous action space \footnote{The deterministic PG is a special case of the stochastic PG, with $\sigma=0$ in the re-parameterization $\pi_{\mu_{\theta}, \sigma}$.} (i.e., a decision $a=\mu_{\theta}(s)$). PG for a deterministic policy in continuous action space is,
\begin{equation}\label{equ:pgthem-deterministic}
        \begin{aligned}  
            \nabla_{\theta} J(\theta) &=\nabla_{\theta} \left(\int_s d^\mu(s) V^\mu(s) ds\right)\\
            &=\nabla_{\theta} \left(\int_s d^\mu(s) Q^\mu(s,a)|_{a=\mu_{\theta}(s)}ds\right)\\
            &\stackrel{\text{(i)}}{=}\int_s d^\mu(s) \nabla_{\theta} \mu_{\theta}(s) \nabla_a Q^\mu(s,a)|_{a=\mu_{\theta}(s)} ds 
            \\
            &=\mathbb{E}_{d^\mu} [ \nabla_{\theta} \mu_{\theta}(s) \nabla_a Q^\mu(s,a)|_{a=\mu_{\theta}(s)}],
        \end{aligned}
\end{equation}
The derivation (i) the state distribution is non-differentiable w.r.t. $\theta$ (i.e., derivation (i))\footnote{A small change in $\theta$ can cause a substantial change in the trajectory, and the state visitation distribution can exhibit non-smooth behavior as a function of $\theta$.}, To guarantee enough exploration of determinant PG, We can either add noise into the policy 
\begin{equation}
    \mu'(s) = \mu_{\theta}(s) + \mathcal{N},
\end{equation} 
or learn it off-policy-ly by following a different stochastic behavior $\beta(a|s)$ policy to collect samples,
\begin{equation}\label{equ:pgthem-deterministic-offpolicy}
        \begin{aligned}  
            \nabla_{\theta} J(\theta) &=\nabla_{\theta} \left(\int_s d^\beta(s) Q^\mu(s,a)|_{a=\mu_{\theta}(s)}ds\right)\\
            &=\mathbb{E}_{d^\beta} [ \nabla_{\theta} \mu_{\theta}(s) \nabla_a Q^\mu(s,a)|_{a=\mu_{\theta}(s)}],
        \end{aligned}
\end{equation}

\subsection{Distributed PG}

Due to the efficiency of the GPU-cluster in training, some workers (machines or processes) are employed in a distributed manner to generate rollouts and compute policy gradients in PG methods \cite{brenner2023ppo}. The distributed advancement can also be extended to any PG extension, like Actor-Critic (AC), PPO, and deterministic PG methods.

\paragraph{Centralized v.s. Decentralized} These workers can either share a central parameter server or update their own weights in a decentralized manner, where aggregation techniques such as AllReduce may be utilized. Rather than merely collecting rollouts and calculating the gradient according to its replay buffer, the workers can be further decentralized into \textit{agents} with their parameters, which is closely related to PG in multi-agent setting.

\paragraph{Synchronous v.s. Asynchronous} In the centralized paradigm, weight updates can be conducted synchronously, where gradients from all workers are aggregated (typically through summation or averaging) before updating the model parameters. This ensures a globally consistent update but may introduce inefficiencies due to synchronization delays. Alternatively, asynchronous updating allows each worker to update the global parameters independently, without waiting for all gradients to be collected. This method can improve computational throughput but may lead to stale gradients and slower convergence. The difference between these 2 approaches is exemplified in Advantage Actor-Critic (A2C) and Asynchronous Advantage Actor-Critic (A3C).


