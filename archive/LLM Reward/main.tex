
\documentclass{article} % For LaTeX2e
\usepackage{style,times}
\usepackage{amsmath,amsfonts,bm,amssymb}


%%%%% customized by shuo begin
\usepackage{amsthm, hyperref, url, csquotes, dsfont}
\usepackage{tikz}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\softmax}{softmax}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
%%%% customized by shuo end

\title{Reward Modeling in LLM Alignment}
\author{Shuo Liu \\
Computer Science\\
Northeastern University\\
\texttt{shuo.liu2@northeastern.edu} \\
}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
	Constructing rewards is crucial yet challenging to achieve RL objectives. This post explores how to guide the LLM optimization via a proper RLHF reward model.
\end{abstract}

\section{Reward Models in RLHF}

As LLMs scale, their raw outputs (optimized primarily for next-token prediction) often diverge from expected traits. To enable RL fine-tuning from human feedbacks (RLHF), reward models are introduced as trainable proxies for human preference. Once trained, it can generalize preference signals to unseen inputs, making alignment more scalable by reducing reliance on slow and costly human annotations. It also allows flexible fine-tuning toward different objectives, such as helpfulness, truthfulness, or safety.

A typical alignment pipeline consists of 3 stages: supervised fine-tuning (SFT), reward modeling, and RL. After an initial SFT based on base transformer with curated human-labeled data, a reward model is constructed to predict human preferences over model-generated responses. This model is then used to guide further optimization by encouraging outputs that maximize the predicted reward. For example, \texttt{ChatGPT} employs reward models trained on ranked annotations to guide its generation toward preferred outputs \cite{openai2024gpt4technicalreport}; \texttt{DeepSeek} and \texttt{LLaMA 2} include explicit reward modeling components in their alignment pipelines, using pairwise preferences to train reward models that inform subsequent learning \cite{shao2024deepseekmathpushinglimitsmathematical, llama}.

\begin{figure}[hbt]
\centering
  \includegraphics[width=0.6\linewidth,height=0.44\linewidth]{RLHF.png}
  \caption{Role of the reward models in RLHF (reward model \blue{in blue}).}
\end{figure}

\input{asym.tex}
\input{sym.tex}
\input{reward_hacking.tex}

\bibliography{ref}
\bibliographystyle{ref}
%\input{appendix.tex}

\end{document}
