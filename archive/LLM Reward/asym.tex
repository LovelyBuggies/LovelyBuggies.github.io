\section{Anti-Symmetric Preference Modeling}

In this section, we introduce the mainstream methods that model rewards of LLM responses through preference comparison.

\subsection{BT Model and Its Ranking Extension}

The original Bradley-Terry (BT) model posits that, given a pair of options $i$ and $j$ drawn from some population, the probability of selecting $i$ is,
\begin{equation}
	\Pr(i \succ j) = \frac{u_i}{u_i + u_j}=\frac{\exp(r(i))}{\exp(r(i)) + \exp(r(j))}, \label{eq:bt}
\end{equation}
where $u_i$ and $u_j$ are the respective utility or preference of options $i$ and $j$, which commonly parameterized in the exponential form via rewards $r(i)$ and $r(j)$. It can be further extended to rank $N$ options, known as Plackett-â€“Luce (PL) model,
\begin{equation}
	\Pr(i_1 \succ i_2 \succ \cdots \succ i_N) = \prod_{k=1}^{N} \frac{\exp(r(i_k))}{\sum_{j=k}^{N} \exp(r(i_j))}. \label{eq:pl}
\end{equation}
BT model is anti-symmetric, which means the preference between two responses depends only on the difference in their reward values. It satisfies $\Pr(y_i \succ y_j) = 1 - \Pr(y_j \succ y_i)$, and the log-odds of preference is anti-symmetric: $\log \left( \frac{\Pr(y_i \succ y_j)}{\Pr(y_j \succ y_i)} \right) = r(x, y_i) - r(x, y_j)$. This structure ensures consistent and transitive pairwise comparisons, making BT suitable for preference modeling. This is initially used to rank sports teams and players, i.e., Elo rating system.

\subsection{Reward Modeling with Pairwise Preferences (BT Model)}

Inferring a reward model using the BT framework can be formulated as a parameter estimation problem, where the objective is to recover latent reward values for candidate responses based on observed pairwise comparisons.

Suppose a prompt $x$ is associated with $N$ candidate responses $\{y_1, \cdots, y_N\}$, and human annotators provide preference labels between some pairs. Ideally, given sufficiently many comparisons under deterministic preference, the true reward values can be accurately inferred.\footnote{$O(N \log N)$ comparisons are sufficient for modeling rewards of $N$ responses.} In practice, however, this inference is challenged by stochastic human behavior and sparse annotation.

\paragraph{Modeling Assumptions} 
To make BT applicable to reward modeling under the practical conditions, we adopt the following assumptions \cite{sun2025rethinkingbradleyterrymodelspreferencebased}:
\begin{enumerate}
    \item \texttt{Deterministic responses and rewards:} For a given prompt $x$, a response $y$ is deterministically generated by a model. The oracle reward $r(x, y)$ associated with each prompt-response pair is fixed.
    \item \texttt{Deterministic annotators with bounded bias:} When a an annotator $A$ compares responses, their preferences are deterministically depending on the comparison of their biased evaluation of the reward,
    \begin{equation}
        \mathds{1} ( \underbrace{y_i \succ y_j}_\text{decision} | x, A ) = \mathds{1} (\underbrace{r(x,y_i) + b(x, y_i; A) > r(x,y_j) + b(x, y_j; A)}_\text{biased preference} ).
    \end{equation}
    
    \item \texttt{Order-preserving shaping:} To address sparsity in comparisons, a known embedding function $\Psi$ is assumed to map each $(x, y)$ pair into a feature space, with the constraint that $\Psi$ is order-preserving and does not affect optimization. High-dimensional embeddings can be beneficial because they increase the likelihood that semantically similar prompt-response pairs are mapped to nearby regions in the embedding space, thereby enabling comparisons to generalize more effectively. Though, it also introduces potential issues such as \textit{reward hacking} (Section~\ref{app:reward_hacking}).

    \item \texttt{Imperfect human annotations}: The annotator function $h(x_1, x_2, y_1, y_2)$ provides feedback that possibly aligns with the oracle reward $r(x, y)$. And it is harder to assign correctly when the reward difference between two pairs is small,
\begin{equation}
\mathbb{P}\left( h(x_1, x_2, y_1, y_2)(r(x_1, y_1) - r(x_2, y_2)) > 0 \,\middle|\, \Delta r \right) = \xi(\Delta r), 
\end{equation}
where $\Delta r := |r(x_1, y_1) - r(x_2, y_2)|$ is the reward difference, and $\xi$ is any monotonic increasing function to $[0.5, 1]$.
\end{enumerate}

\paragraph{Likelihood and Estimation}
Let each observed preference be denoted as a pair $(i \succ j)$, indicating that $y_i$ is preferred over $y_j$ for prompt $x$. Under the BT model, the probability of this outcome is,
\begin{equation}
    \Pr(y_i \succ y_j | x) = \frac{\exp(r(x, y_i))}{\exp(r(x, y_i)) + \exp(r(x, y_j))}.
\end{equation}
Given a dataset of $M$ annotated comparisons $\mathcal{C} = \{(i_m, j_m)\}_{m=1}^M$, the likelihood of observing these preferences is,
\begin{equation}
    \mathcal{L}(r) = \prod_{m=1}^M \frac{\exp(r(x, y_{i_m}))}{\exp(r(x, y_{i_m})) + \exp(r(x, y_{j_m}))},
\end{equation}
and the corresponding log-likelihood is,
\begin{equation}
    \log \mathcal{L}(r) = \sum_{m=1}^M \left[ r(x, y_{i_m}) - \log\left(\exp(r(x, y_{i_m})) + \exp(r(x, y_{j_m}))\right) \right].
\end{equation}
The optimal reward model is then obtained by maximizing the log-likelihood (MLE),
\begin{equation}
    r^* = \argmax_r \log \mathcal{L}(r).
\end{equation}
The solution is identifiable up to an additive constant and converges to the ground-truth rewards under the assumptions above.

\subsection{Reward Modeling with Ranked Preferences (PL Model)}

While the BT model provides a principled way to infer rewards from pairwise preferences, real-world systems can collect richer feedback in the form of ranked lists. Human annotators are presented with multiple responses to the same prompt and asked to provide an overall ranking. The reward model is then trained to assign scores that agree with these rankings.

PL model Equation.~\ref{eq:pl} is used in a similar way for MLE, just like BT model. For a ranking of alternatives $(y_{i_1} \succ y_{i_2} \succ \ldots \succ y_{i_N})$ for prompt $x$, the probability under the PL model is,
\begin{equation}
	\Pr(y_{i_1} \succ y_{i_2} \succ \ldots \succ y_{i_N} | x) = \prod_{k=1}^{N-1} \frac{\exp(r(x, y_{i_k}))}{\sum_{j=k}^{N} \exp(r(x, y_{i_j}))}.
\end{equation}
Given a dataset of $M$ rankings $\mathcal{C} = \{(y_{i_1^m}, y_{i_2^m}, \ldots, y_{i_{N_m}^m})\}_{m=1}^M$, the likelihood is,
\begin{equation}
	\mathcal{L}(r) = \prod_{m=1}^{M} \prod_{k=1}^{N_m-1} \frac{\exp(r(x, y_{i_k^m}))}{\sum_{j=k}^{N_m} \exp(r(x, y_{i_j^m}))}.
\end{equation}
And the corresponding log-likelihood is,
\begin{equation}
\log \mathcal{L}(r) = \sum_{m=1}^{M} \sum_{k=1}^{N_m-1} \left[ r(x, y_{i_k^m}) - \log\left(\sum_{j=k}^{N_m} \exp(r(x, y_{i_j^m}))\right) \right].
\end{equation}
The optimal reward model is then obtained by MLE,
\begin{equation}
r^* = \underset{r}{\arg\max} \log \mathcal{L}(r).
\end{equation}

Preference modeling with the PL model is also anti-symmetric, swapping the order of two responses in the ranking inverts the relative score difference in the likelihood expression.

