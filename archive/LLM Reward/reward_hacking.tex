\newpage
\section{Takeaways in Reward Design} \label{app:reward_hacking}

Effective reward design is critical in guiding RL agents towards desirable behaviors. Several considerations and specialized techniques help ensure robust and aligned outcomes. Here are some specific takeaways should be considered when designing rewards.

\paragraph{Potential-based Reward Shaping}
Reward shaping accelerates learning by augmenting the original reward with additional informative signals. Potential-based shaping ensures optimal policy invariance by defining additional rewards through a state-dependent potential function $\Phi(s)$. Specifically, the shaping reward is formulated as $F(s, a, s') = \gamma \Phi(s') - \Phi(s)$, thereby avoiding the introduction of unintended behaviors or reward hacking vulnerabilities.
\begin{quote}
\scriptsize
	\enquote{\texttt{\textit{Rubric engineering is the new prompt engineering.}}} \hfill -- \texttt{\textit{Will Brown}}.
\end{quote}

\paragraph{Sparse vs. Dense Rewards}
The frequency and clarity of feedback significantly influence learning efficiency and agent behaviors. Sparse rewards, given infrequently and typically at task completion, mitigate reward hacking risks but pose exploration challenges \cite{weng2024rewardhack}.\footnote{The ambiguous or underspecified rewards design that creates vulnerabilities for agents' exploitation.} Conversely, dense rewards provide frequent feedback that facilitates exploration yet may inadvertently encourage reward exploitation or unintended optimization behaviors.

\paragraph{Human-in-the-loop Reward Design}
Incorporating human judgments directly into reward signals enables iterative refinement, capturing nuanced and complex objectives not easily formalized. Human-in-the-loop methods mitigate reward hacking by continuously aligning rewards with intended outcomes through active human oversight.

\paragraph{Intrinsic vs. Extrinsic Rewards}
Intrinsic rewards originate internally, driven by agent behaviors such as curiosity or novelty-seeking, promoting adaptive and exploratory capabilities. Extrinsic rewards come externally, explicitly set by designers. A balanced integration of intrinsic and extrinsic rewards supports robust agent behaviors through diversified motivational signals.
