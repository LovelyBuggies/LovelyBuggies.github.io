
\documentclass{article} % For LaTeX2e
\usepackage{style,times}
\usepackage{amsmath,amsfonts,bm,amssymb}


%%%%% customized by shuo begin
\usepackage{amsthm}
\DeclareMathOperator*{\argmax}{argmax}
% \usepackage[hidelinks]{hyperref}
\usepackage{hyperref}
\usepackage{url}

\newcommand{\obshist}{\vec{\theta}}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

\newcommand{\red}[1]{\textcolor{red}{#1}}
%%%% customized by shuo end

\title{Optimal Q-Value Functions for Dec-POMDP}

\author{Shuo Liu \\
Computer Science\\
Northeastern University\\
\texttt{shuo.liu2@northeastern.edu} \\
}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
This article discusses the optimal Q-value function definition in Dec-POMDP.
\end{abstract}

\section{Notions}

\renewcommand{\arraystretch}{1.5}
\begin{table}[h!]
    \begin{tabular}{ll}
    \hline \hline
        $s^t$ & the state at $t$ with problem horizon $h$\\
        $o^t$ & the joint observation of agents $o^t=\langle o_1^t, \dots, o_n^t \rangle$ at $t$\\
        $\mathcal{O}$ & the joint observation space\\
        $\obshist^t$ & the joint observation-action history until $t$, $\obshist^t=(o^0, a^0, \dots, o^t)$ \\
        $\vec{\Theta}^t$ & the joint history space at $t$\\
        $\vec{\Theta}^t_\pi$ & the set of $\obshist^t$ consistent with policy $\pi$\\
        \\
        
        $\delta^{t}$ & the decision rule (a temporal structure of policy) at $t$\\ 
        $\delta^{t,*}$ & the optimal decision rule at $t$ following $\psi^{t-1, *}$\\
        $\delta^{t,\circledast}_\psi$ & the optimal decision rule at $t$ following $\psi^{t-1}$\\
        $\Delta^t$ & the decision rule space at $t$\\
        $\psi^{t}$ & the past joint policy until $t$, $\psi^{t} = \delta^{[0, t)}$\\
        $\psi^{t, *}$ & the optimal past joint policy until $t$, $\psi^{t, *} = \delta^{[0, t), *}$\\
        $\psi^{t, \circledast}$ & the past joint policy until $t$ with non-optimal $\psi^{t-1}$ and optimal $\delta^{t-1, \circledast}_\psi$\\
        $\Psi^{t}$ & the past joint policy space at $t$\\
        $\xi^t$ & the subsequent joint policy from $t$, $\xi^{t} = \delta^{[t, h)}$\\
        $\xi^{t, *}$ & the optimal subsequent joint policy from $t$, $\xi^{t} = \delta^{[t, h), *}$\\
        $\xi^{t, \circledast}_\psi$ & the optimal subsequent joint policy from $t$ following non-optimal $\psi^t$\\
        $\pi$ & the joint pure policy $\pi=\delta^{[0, h)}$\\
        $\pi^*$ & the joint optimal pure policy $\pi^*=\delta^{[0, h), *}$\\
        \\

        $R(\obshist^t, \psi^{t+1})$ & the immediate reward function following $\psi^{t+1}$\\
        $Q(\obshist^t, \psi^{t+1})$ & the history-policy value function following $\psi^{t+1}$\\
        $Q^*(\obshist^t, \psi^{t+1})$ & the optimal history-policy value function following $\psi^{t+1}$\\
        $Q^\circledast(\obshist^t, \psi^{t+1})$ & the sequentially rational optimal history-policy value function following $\psi^{t+1}$\\
    \hline \hline
    \end{tabular}
\end{table}
\newpage

\section{Normative Optimal Q-Value Function}

\begin{definition} \label{defn:normative-Q}
    The optimal Q-value function $Q^*$ in Dec-POMDP, the expected cumulative reward over time steps $[t,h)$ induced by optimal joint policy $\pi^{*}$, $\forall \obshist^t\in \vec{\Theta}^t_{\psi^{t, *}}, \forall \psi^{t+1}\in(\psi^{t, *},\Delta^t)$, is defined as,
    \begin{equation}
        Q^*(\obshist^t, \psi^{t+1}) = \left\{
        \begin{aligned}
        &R(\obshist^t, \psi^{t+1}), &t=h-1 \\ 
        &R(\obshist^t, \psi^{t+1}) + \sum_{o^{t+1} \in \mathcal{O}} P(o^{t+1}|\obshist^t, \psi^{t+1}) Q^*(\obshist^{t+1}, \pi^*(\obshist^{t+1})). &0\leqslant t < h-1 \\
        \end{aligned}
        \right .\label{eq:normative-Q}
    \end{equation}
\end{definition}
Here, $\pi^*(\obshist^{t+1})\equiv \psi^{t+2, *}$ because of the consistent optimality of policy.

\begin{proposition} \label{prop:problem}
    In Dec-POMDP, deriving an optimal policy from the normative optimal history-policy value function defined in Equ. \ref{eq:normative-Q} is impractical (clarifying Sec. 4.3.3, \cite{Oliehoek08JAIR}).
\end{proposition}

\begin{proof}
    We check the optima in 2 steps. The independent and dependent variables are marked in red.
    
    To calculate the Pareto optima of Bayesian game at $t$,
    \begin{equation}
    \red{\delta^{t, *}}
    = \argmax_{\delta^t}\sum_{\obshist^t \in \vec{\Theta}^t_{\psi^{t, *}}} P(\obshist^t|\psi^{t, *}) \red{Q^*}(\obshist^t, (\psi^{t, *}, \delta^t)),
    \end{equation}
    note that calculating $\delta^{t,*}$ depends on $\psi^{t, *} = \delta^{[0, t), *}$ and $Q^*(\obshist^t, \cdot)$. 
    
    According to Definition. \ref{defn:normative-Q}, the optimal Bellman equation can be written as,
    \begin{equation}
    \red{Q^*}(\obshist^t, \psi^{t+1}) = R(\obshist^t, \psi^{t+1}) + \sum_{o^{t+1} \in \mathcal{O}} P(o^{t+1}|\obshist^t, \psi^{t+1}) \max_{\delta^{t+1}}Q^*(\obshist^{t+1}, (\red{\psi^{t+1, *}}, \delta^{t+1})),
    \end{equation}
    when $0\leqslant t < h-1$. This indicates that $Q^*(\obshist^t, \cdot)$ depends on $\psi^{t+1, *}$.\footnote{The dependency of $P(o^{t+1}|\obshist^t, \psi^{t+1})$ is not a problem and can be solved just like how the stochasticity $P(s^{t+1}|s^t, a)$ tackled by double learning in Sec. 6.7, \cite{sutton2018reinforcement}.} Consequently, calculating $\delta^{t,*}$ inherently depends on $\delta^{[0, t], *}$ (includes itself), making it self-dependent and impractical to solve.\footnote{Single-agent (PO)MDP, where the belief states are acquirable, does not have such a problem because the Q-value function is not necessarily history-dependent, thanks to Markovian property.}
\end{proof}

\section{Sequentially Rational Optimal Q-Value Function}

To make optimal Q-value in Dec-POMDP computable, \cite{Oliehoek08JAIR} defined another form of Q-value function and eliminated the dependency on past optimality.

\begin{definition}
    The sequentially rational optimal Q-value function $Q^\circledast$ in Dec-POMDP, the expected cumulative reward over time steps $[t,h)$ induced by optimal subsequent joint policy $\xi^{t, \circledast}_\psi$, $\forall \obshist^t\in \vec{\Theta}^t_{\Psi^{t}}, \forall\psi^{t+1}\in\Psi^{t+1}$, is defined as,
    \begin{equation}
        Q^\circledast(\obshist^t, \psi^{t+1}) = \left\{
        \begin{aligned}
        &R(\obshist^t, \psi^{t+1}), &t=h-1\\ 
        &R(\obshist^t, \psi^{t+1}) + \sum_{o^{t+1} \in \mathcal{O}} P(o^{t+1}|\obshist^t, \psi^{t+1}) Q^\circledast(\obshist^{t+1}, \psi^{t+2, \circledast}), &0\leqslant t < h-1 \\
        \end{aligned}
        \right .\label{eq:SR-Q}
    \end{equation}
    where $\psi^{t+2, \circledast}=(\psi^{t+1}, \delta^{t+1, \circledast}_{\psi}), \forall \ \psi^{t+1} \in \Psi^{t+1}$.
\end{definition}

Note that the only difference of $Q^\circledast$ from $Q^*$ is $\psi^{t+2, \circledast}$, consequently expanding $Q^*$'s candidates of history from $\obshist^t \in \vec{\Theta}^t_{\psi^{t, *}}$ to $\obshist^t \in \vec{\Theta}^t_{\Psi^{t}}$ and policy from $\psi^{t+1}\in(\psi^{t, *},\Delta^t)$ to $\psi^{t+1}\in(\Psi^t,\Delta^t)$.

Beyond solving the problem of Proposition \ref{prop:problem}, another advantage of $Q^\circledast$ is that it allows for the computation of optimal subsequent policy $\xi^{t, *}_\psi$ following any past policy $\psi^{t}$. This is beneficial in online applications where agents may occasionally deviate from the optimal policy.

\section{Open Questions}

\begin{itemize}
    \item We have seen some advantages of defining the optimal Q-value function as $Q^\circledast$, what are the downsides to defining it this way (e.g., high computational costs)?
\end{itemize}

\bibliography{ref}
\bibliographystyle{ref}

\clearpage
\appendix
% \section{Appendix}

\end{document}
