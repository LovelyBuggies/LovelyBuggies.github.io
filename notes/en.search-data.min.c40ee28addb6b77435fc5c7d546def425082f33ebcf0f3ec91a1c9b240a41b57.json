[{"id":0,"href":"/notes/docs/rl/from-pg-2-ppo/","title":"From PG 2 PPO","section":"RL","content":""},{"id":1,"href":"/notes/docs/rl/understanding-bellman/","title":"Understanding Bellman","section":"RL","content":" Bellman Equation and Operator # Definition # The Bellman Equation and optimal Bellman Equation for V-values are, (Sutton and Barto 2018),1 $$\\begin{aligned} V^\\pi(s) \u0026amp;\\doteq \\mathbb{E}{a \\sim \\pi(\\cdot|s)} \\left[ Q^\\pi(s, a) \\right], \\ \u0026amp;= \\mathbb{E}{a \\sim \\pi(\\cdot|s)} \\left[ R(s, a) + \\gamma \\mathbb{E}{s' \\sim P(\\cdot|s,a)} \\left[V^\\pi(s')\\right] \\right], \\ V^*(s) \u0026amp;\\doteq \\max{a} \\left[ Q^(s, a) \\right], \\ \u0026amp;= \\max_{a} \\left[ R(s, a) + \\gamma \\mathbb{E}_{s' \\sim P(\\cdot|s,a)} \\left[V^(s')\\right] \\right]. \\end{aligned}$$ and the Bellman Equation and optimal Bellman Equation for Q-values are, $$\\begin{aligned} Q^\\pi(s, a) \u0026amp;\\doteq R(s, a) + \\gamma \\mathbb{E}{s'\\sim P(\\cdot|s,a)} \\left[V^\\pi(s')\\right], \\ \u0026amp;= R(s, a) + \\gamma \\mathbb{E}{s'\\sim P(\\cdot|s,a)} \\left[\\mathbb{E}{a'\\sim\\pi(a'|s')} Q^\\pi(s', a')\\right]. \\ Q^*(s, a) \u0026amp;\\doteq R(s, a) + \\gamma \\mathbb{E}{s'\\sim P(\\cdot|s,a)} \\left[V^(s')\\right], \\ \u0026amp;= R(s, a) + \\gamma \\mathbb{E}{s'\\sim P(\\cdot|s,a)} \\left[\\max{a'} Q^(s', a')\\right]. \\end{aligned}$$ where $V^\\pi(s)$ and $Q^\\pi(s,a)$ are value representations following policy $\\pi$, e.g., vectors and functions. $$\\tilde{\\pi}(s) \\doteq \\mathop{\\mathrm{argmax}}a Q^\\pi (s,a).$$ Bellman Equations establish relations between states and succeeding states, which can be applied as updating rules for value prediction. A succinct representation is to define the Bellman Equation as a unary mathematical operator. The V-value Bellman and optimal Bellman Operators are, $$\\begin{aligned} (\\mathcal{T}^\\pi\\circ V^\\pi)(s) \u0026amp;\\doteq \\mathbb{E}{a \\sim \\pi(\\cdot|s)} \\left[ R(s, a) + \\gamma \\mathbb{E}{s' \\sim P(\\cdot|s,a)} \\left[V^\\pi(s')\\right] \\right], \\ (\\mathcal{T}^*\\circ V^\\pi)(s) \u0026amp;\\doteq \\max_a \\left[ R(s, a) + \\gamma \\mathbb{E}{s' \\sim P(\\cdot|s,a)} \\left[V^\\pi(s')\\right] \\right]. \\end{aligned}$$ The Bellman and optimal Bellman Operators $\\mathcal{T}^\\pi$ for Q-values are, $$\\begin{aligned} (\\mathcal{T}^\\pi\\circ Q^\\pi)(s, a) \u0026amp;\\doteq R(s, a) + \\gamma \\mathbb{E}{s' \\sim P(\\cdot|s,a)} \\left[ \\mathbb{E}{a' \\sim \\pi(a'|s')} Q^\\pi(s', a') \\right], \\ (\\mathcal{T}^*\\circ Q^\\pi)(s, a) \u0026amp;\\doteq R(s, a) + \\gamma \\mathbb{E}{s' \\sim P(\\cdot|s,a)} \\left[ \\max{a'} Q^\\pi(s', a') \\right]. \\end{aligned}$$\nCurse of Dimension # Why do we mostly use MDP (where the future evolution is independent of its history) and hence Bellman Equations to model RL problems? (Bellman 1957) coined the “curse of dimension”, which describes the exponential increase in the state space size as dimensionality grows, making calculations extremely complex. Breaking this curse often requires altering the problem or its constraints, though complete solutions are not always achievable.\nFor convenience, we use Q-value as the representative in the following parts of this article.\nImportant Properties # Proposition 1 ($\\gamma$-contraction). Given any $Q,\\ Q' \\mapsto \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}$, Bellman Operators are $\\gamma$-contraction Operators in $L^\\infty$ norm, $$\\begin{aligned} |\\mathcal{T}^\\pi \\circ Q - \\mathcal{T}^\\pi \\circ Q'|\\infty \u0026amp;\\leqslant \\gamma |Q-Q'|\\infty,\\ \\text{and }|\\mathcal{T}^ \\circ Q - \\mathcal{T}^* \\circ Q'|\\infty \u0026amp;\\leqslant \\gamma |Q-Q'|\\infty. \\end{aligned}$$*\nCorollary 1 (Fixed-point Iteration). For any $Q^0 \\mapsto \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}$, after $k\\to \\infty$ iterations of Bellman transformation, $Q^{\\pi, \\infty} \\doteq \\lim_{k \\to\\infty} (\\mathcal{T}^\\pi)^k \\circ Q^0$, or $Q^{, \\infty} \\doteq \\lim_{k\\to\\infty} (\\mathcal{T}^)^k \\circ Q^0$, according to Banach’s Fixed Point Theorem, $$\\begin{gathered} Q^{\\pi, \\infty}=Q^{, \\infty}=Q^, \\ \\text{which \\textbf{uniquely} satisfies } \\mathcal{T}^\\pi \\circ Q^ = Q^, \\text{ or } \\mathcal{T}^ \\circ Q^* = Q^. \\end{gathered}$$\nTheorem 1 (Fundamental theorem). Any memoryless policy that is greedy to $Q^$ (deterministically maximizes) is optimal (Szepesvári 2010), $$\\begin{aligned} \\tilde{\\pi}^{} \\doteq \\mathop{\\mathrm{argmax}}_aQ^ = \\pi^. \\end{aligned}$$\nProposition 2 (Monotone). Bellman Operators are monotonic. For any Q-values $Q,Q' \\mapsto \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}$, $$\\begin{aligned} \\left (Q\\leqslant Q'\\right ) \u0026amp;\\Leftrightarrow \\left (\\mathcal{T}^\\pi \\circ Q\\leqslant \\mathcal{T}^\\pi \\circ Q'\\right ),\\ \\left (Q\\leqslant Q'\\right )\u0026amp;\\Leftrightarrow \\left (\\mathcal{T}^ \\circ Q\\leqslant \\mathcal{T}^* \\circ Q'\\right ). \\end{aligned}$$*\nBellman Backup for Planning # Dynamic Programming # According to the Fundamental Theorem, we can find $\\pi^$ efficiently once having access to $Q^$, without the need to find the policy whose Q-function dominates the others’ brute-force-ly. To avoid the Bellman Curse of Dimensionality, we can apply Dynamic Programming (DP) methods to solve MDPs by keeping track of Q-values during calculations, thanks to Bellman recursions.\nValue Iteration # Value iteration (so-called backward induction) involves iteratively applying $\\mathcal{T}^$ to arbitrarily initialized values $Q^0$ until convergence. According to Corollary 1 and Theorem 1, value iteration converges to $Q^$ as $k \\to \\infty$, then an optimal policy $\\pi^$ can be derived by greedifying $Q^$.\nPolicy Iteration # Policy iteration starts with an arbitrary policy $\\pi^0$ and values $Q^0$. In each iterative step $k$, $Q^{\\pi, k}$ is calculated by applying Bellman Operator $\\mathcal{T}^{\\pi, k}$ that follows current policy ${\\pi^k}$ to $Q^{\\pi, {k-1}}$ from the last iteration, and then $\\pi^{k+1}$ is derived from greedifying $Q^{\\pi, k}$. This process is repeated until convergence, and policy iteration can produce optimal policy after sufficient iterations.\nBellman Residual for Learning # TD-Learning with Look-up Table # When the transition model is unavailable (model-free), we can use the residuals (RHS minus LHS) of the Bellman Equations as learning objective, $$\\begin{aligned} (\\mathcal{B}^\\pi\\circ Q) (s,a) \u0026amp;\\doteq r + \\gamma Q(s', \\pi(s')) - Q(s, a),\\ (\\mathcal{B}^*\\circ Q) (s,a) \u0026amp;\\doteq r + \\gamma \\max_{a'} Q(s', a') - Q(s, a). \\end{aligned}$$ Assuming that our sampling and parameter updating roughly follow the true state distribution $\\mu(s)$, the expectation of Bellman residual will be closed to zero at the optima. This approach is often called temporal difference (TD) learning.\nTD-learning # In TD-learning with learning rate $\\alpha$, the update rule for Q-values is, $$Q(s, a) \\leftarrow Q(s, a) + \\alpha (\\mathcal{B}^\\pi\\circ Q) (s,a). \\label{eq:td-learning}$$ According to Stochastic Approximation Theorem, let $k$ be the visitation times of state-action pair, and learning rates $0 \\leqslant \\alpha^k \u0026lt; 1$ satisfies $\\forall (s, a)$, $\\sum_{k=1}^\\infty \\alpha^k(s, a) = \\infty,\\sum_{k=1}^\\infty [\\alpha^k(s, a)]^2 \u0026lt; \\infty$. Following TD-learning updates, $Q^{\\pi, k}(s, a)$ converges to $Q^*(s, a)$ as $k \\to \\infty$ ((Jaakkola, Jordan, and Singh 1994)).\nQ-learning # In Q-learning that relies on optimal Bellman Equation, the Q-value update is, $$Q(s, a) \\leftarrow Q(s, a) + \\alpha (\\mathcal{B}^\\circ Q) (s,a). \\label{eq:q-learning}$$ According to Stochastic Approximation Theorem, let $k$ be the visitation times of state-action pair, and learning rates $0 \\leqslant \\alpha^k \u0026lt; 1$ satisfies $\\forall (s, a)$, $\\sum_{k=1}^\\infty \\alpha^k(s, a) = \\infty, \\sum_{k=1}^\\infty [\\alpha^k(s, a)]^2 \u0026lt; \\infty$. Following Q-learning updates, $Q^{, k}(s, a)$ converges to $Q^*(s, a)$ as $k \\to \\infty$ ((Watkins and Dayan 1992)). The deep version of Q-learning algorithm, Deep Q-Network (DQN), is shown in Appendix.\nHowever, the nice property of convergence only holds in the tabular case and cannot be extended to a function approximation as discussed later.\nTD-learning with Function Approximation # To introduce generalization to the value function, we represent the approximated Q-value in a parameterized functional form. Our goal is to minimize the mean squared value error, $$\\mathcal{L}(\\theta) = \\frac{1}{2}\\sum_{s \\in \\mathcal{S}} \\mu(s) \\Big[ Q^\\text{target} - Q_\\theta(s, a) \\Big]^2,$$ where $Q^\\text{target}$ is the ground truth and $Q_\\theta$ is the prediction. Just like TD-learning, the Bellman residual can be applied for the value function approximation.\nSemi gradient for Bellman Residual # Similar to stochastic gradient methods with unbiased target estimators, if we use the Bellman Equation to get target Q-value $Q^\\text{target}$, but here we just ignore its potential gradient change, the gradient ascent for Bellman residual is, $$\\begin{aligned} \\Delta_\\text{semi} \\theta \u0026amp;= -\\frac{1}{2}\\alpha \\nabla_\\theta \\Big[Q^\\text{target} - Q_\\theta(s, a) \\Big]^2 \\ \u0026amp;= \\alpha \\Big[Q^\\text{target} - Q_\\theta(s, a) \\Big] \\nabla_\\theta Q_\\theta(s, a), \\text{ where } Q^\\text{target} = r + \\gamma Q_{\\textcolor{red}{\\theta}}(s', a')\\label{eq:semi-grad} \\end{aligned}$$ Since we neglects a part of the gradient of $Q^\\text{target}$, it is called semi gradient for Bellman residual ($\\theta$ in red). Though semi-gradient methods are fast and simple, they could have divergence issue, e.g., Baird’s counter-example (the star problem).\nFull Gradient for Bellman Residual # The full Bellman residual gradient should include all gradient components, including the gradient of the target estimation, $$\\begin{aligned} \\Delta_\\text{full} \\theta \u0026amp;= -\\frac{1}{2}\\alpha \\nabla_\\theta \\Big[ r + \\gamma Q_\\theta(s', a') - Q_\\theta(s, a) \\Big]^2 \\ \u0026amp; = -\\alpha \\Big[ r + \\gamma Q_\\theta(s', a') - Q_\\theta(s, a) \\Big] \\Big[ \\gamma\\nabla_\\theta Q_\\theta(s', a') - \\nabla_\\theta Q_\\theta(s, a) \\Big]. \\end{aligned}$$ If the approximation system is general enough and the value functions are continuous, the full Bellman residual gradient is guaranteed to converge to the optima. However, this is at the sacrifice of learning speed, as illustrated by the hall problem.\nHybrid Gradient for Bellman Residual # In contrast to Figure 1 where $\\Delta_\\text{semi}$ boosts $\\Delta_\\text{full}$, Figure 3 represents the case where the semi gradient may diverge. (Baird 1995) combined these 2 methods: to keep stable, $\\Delta_\\text{B}$ should stay in the same direction as $\\Delta_\\text{full}$ (above the perpendicular axis); meanwhile, $\\Delta_\\text{B}$ should stay as close as possible to $\\Delta_\\text{semi}$ to increase learning speed. $$\\begin{aligned} \\Delta_\\text{B} \\theta \u0026amp;= (1 - \\omega) \\cdot \\Delta_\\text{semi}\\theta + \\omega \\cdot \\Delta_\\text{full}\\theta, \\ \u0026amp;=-\\alpha \\Big[ r + \\gamma Q_\\theta(s', a') - Q_\\theta(s, a) \\Big] \\Big[\\omega \\gamma \\nabla_\\theta Q_\\theta(s', a') - \\nabla_\\theta Q_\\theta(s, a) \\Big],\\ \u0026amp;\\text{s.t.,} \\ \\Delta_\\text{B}\\theta \\cdot \\Delta_\\text{full}\\theta\\geqslant 0 \\Leftrightarrow \\omega \\geqslant \\frac{\\Delta_\\text{semi}\\theta \\cdot \\Delta_\\text{full}\\theta}{\\Delta_\\text{semi}\\theta \\cdot \\Delta_\\text{full}\\theta - \\Delta_\\text{full}\\theta \\cdot \\Delta_\\text{full}\\theta}. \\end{aligned}$$\nGradient ascent by semi-gradient. Gradient descent by semi-gradient. Epoch-wise gradient vectors for Bellman residual gradients. Baird, Leemon C. 1995. “Residual Algorithms: Reinforcement Learning with Function Approximation.” In Machine Learning Proceedings 1995, 30–37. Elsevier.\nBellman, Richard. 1957. Dynamic Programming. Princeton, NJ: Princeton University Press.\nJaakkola, Thomas, Michael I. Jordan, and Satinder P. Singh. 1994. “On the Convergence of Stochastic Iterative Dynamic Programming Algorithms.” Neural Computation 6 (6): 1185–1201. https://doi.org/10.1162/neco.1994.6.6.1185.\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. 2nd ed. MIT Press.\nSzepesvári, Csaba. 2010. Algorithms for Reinforcement Learning. Vol. 4. Synthesis Lectures on Artificial Intelligence and Machine Learning 1. Morgan \u0026amp; Claypool Publishers. https://doi.org/10.2200/S00268ED1V01Y201005AIM009.\nWatkins, Christopher J. C. H., and Peter Dayan. 1992. “Q-Learning.” Machine Learning 8 (3–4): 279–92. https://doi.org/10.1007/BF00992698.\nThe Bellman Equations shown in the main article are stochastic, the deterministic one can be derived as, $V(s) = \\max_{a} \\left{ R(s, a) + \\gamma V(s')\\right}$, where $s'\\gets T(s,a)$ is a deterministic succeeding state of $s$ following $a$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":2,"href":"/notes/docs/","title":"Docs","section":"LovelyBuggies' Blog","content":" title: \u0026quot;Blog\u0026quot; weight: 1 # Welcome to the Blog. Use the sidebar to browse sections. To add new posts, see “How to Add a Blog/Note”.\n"},{"id":3,"href":"/notes/docs/rl/marl/introduction-2-decpomdp/","title":"Introduction 2 DecPOMDP","section":"MARL","content":""},{"id":4,"href":"/notes/docs/rl/marl/optimal-q-value-functions-for-dec-pomdp/","title":"Optimal Q Value Functions for Dec POMDP","section":"MARL","content":" Notions # $s^t$ the state at $t$ with problem horizon $h$ $o^t$ the joint observation of agents $o^t=\\langle o_1^t, \\dots, o_n^t \\rangle$ at $t$ $\\mathcal{O}$ the joint observation space $\\vec{\\theta}^t$ the joint observation-action history until $t$, $\\vec{\\theta}^t=(o^0, a^0, \\dots, o^t)$ $\\vec{\\Theta}^t$ the joint history space at $t$ $\\vec{\\Theta}^t_\\pi$ the set of $\\vec{\\theta}^t$ consistent with policy $\\pi$ $\\delta^{t}$ the decision rule (a temporal structure of policy) at $t$ $\\delta^{t,*}$ the optimal decision rule at $t$ following $\\psi^{t-1, *}$ $\\delta^{t,\\circledast}_\\psi$ the optimal decision rule at $t$ following $\\psi^{t-1}$ $\\Delta^t$ the decision rule space at $t$ $\\psi^{t}$ the past joint policy until $t$, $\\psi^{t} = \\delta^{[0, t)}$ $\\psi^{t, *}$ the optimal past joint policy until $t$, $\\psi^{t, *} = \\delta^{[0, t), *}$ $\\psi^{t, \\circledast}$ the past joint policy until $t$ with non-optimal $\\psi^{t-1}$ and optimal $\\delta^{t-1, \\circledast}_\\psi$ $\\Psi^{t}$ the past joint policy space at $t$ $\\xi^t$ the subsequent joint policy from $t$, $\\xi^{t} = \\delta^{[t, h)}$ $\\xi^{t, *}$ the optimal subsequent joint policy from $t$, $\\xi^{t} = \\delta^{[t, h), *}$ $\\xi^{t, \\circledast}_\\psi$ the optimal subsequent joint policy from $t$ following non-optimal $\\psi^t$ $\\pi$ the joint pure policy $\\pi=\\delta^{[0, h)}$ $\\pi^*$ the joint optimal pure policy $\\pi^*=\\delta^{[0, h), *}$ $R(\\vec{\\theta}^t, \\psi^{t+1})$ the immediate reward function following $\\psi^{t+1}$ $Q(\\vec{\\theta}^t, \\psi^{t+1})$ the history-policy value function following $\\psi^{t+1}$ $Q^*(\\vec{\\theta}^t, \\psi^{t+1})$ the optimal history-policy value function following $\\psi^{t+1}$ $Q^\\circledast(\\vec{\\theta}^t, \\psi^{t+1})$ the sequentially rational optimal history-policy value function following $\\psi^{t+1}$ Normative Optimal Q-Value Function # Definition 1. The optimal Q-value function $Q^$ in Dec-POMDP, the expected cumulative reward over time steps $[t,h)$ induced by optimal joint policy $\\pi^{}$, $\\forall \\vec{\\theta}^t\\in \\vec{\\Theta}^t_{\\psi^{t, }}, \\forall \\psi^{t+1}\\in(\\psi^{t, },\\Delta^t)$, is defined as, $$Q^(\\vec{\\theta}^t, \\psi^{t+1}) = \\left{ \\begin{aligned} \u0026amp;R(\\vec{\\theta}^t, \\psi^{t+1}), \u0026amp;t=h-1 \\ \u0026amp;R(\\vec{\\theta}^t, \\psi^{t+1}) + \\sum_{o^{t+1} \\in \\mathcal{O}} P(o^{t+1}|\\vec{\\theta}^t, \\psi^{t+1}) Q^(\\vec{\\theta}^{t+1}, \\pi^(\\vec{\\theta}^{t+1})). \u0026amp;0\\leqslant t \u0026lt; h-1 \\ \\end{aligned} \\right .\\label{eq:normative-Q}$$*\nHere, $\\pi^*(\\vec{\\theta}^{t+1})\\equiv \\psi^{t+2, *}$ because of the consistent optimality of policy.\nProposition 1. In Dec-POMDP, deriving an optimal policy from the normative optimal history-policy value function defined in Equ. [eq:normative-Q] is impractical (clarifying Sec. 4.3.3, (Oliehoek, Spaan, and Vlassis 2008)).\nProof. We check the optima in 2 steps. The independent and dependent variables are marked in red.\nTo calculate the Pareto optima of Bayesian game at $t$, $$\\textcolor{red}{\\delta^{t, *}} = \\mathop{\\mathrm{argmax}}{\\delta^t}\\sum{\\vec{\\theta}^t \\in \\vec{\\Theta}^t_{\\psi^{t, *}}} P(\\vec{\\theta}^t|\\psi^{t, }) \\textcolor{red}{Q^}(\\vec{\\theta}^t, (\\psi^{t, }, \\delta^t)),$$ note that calculating $\\delta^{t,}$ depends on $\\psi^{t, *} = \\delta^{[0, t), }$ and $Q^(\\vec{\\theta}^t, \\cdot)$.\nAccording to Definition. 1, the optimal Bellman equation can be written as, $$\\textcolor{red}{Q^}(\\vec{\\theta}^t, \\psi^{t+1}) = R(\\vec{\\theta}^t, \\psi^{t+1}) + \\sum_{o^{t+1} \\in \\mathcal{O}} P(o^{t+1}|\\vec{\\theta}^t, \\psi^{t+1}) \\max_{\\delta^{t+1}}Q^(\\vec{\\theta}^{t+1}, (\\textcolor{red}{\\psi^{t+1, }}, \\delta^{t+1})),$$ when $0\\leqslant t \u0026lt; h-1$. This indicates that $Q^(\\vec{\\theta}^t, \\cdot)$ depends on $\\psi^{t+1, }$.1 Consequently, calculating $\\delta^{t,}$ inherently depends on $\\delta^{[0, t], *}$ (includes itself), making it self-dependent and impractical to solve.2 ◻\nSequentially Rational Optimal Q-Value Function # To make optimal Q-value in Dec-POMDP computable, (Oliehoek, Spaan, and Vlassis 2008) defined another form of Q-value function and eliminated the dependency on past optimality.\nDefinition 2. The sequentially rational optimal Q-value function $Q^\\circledast$ in Dec-POMDP, the expected cumulative reward over time steps $[t,h)$ induced by optimal subsequent joint policy $\\xi^{t, \\circledast}\\psi$, $\\forall \\vec{\\theta}^t\\in \\vec{\\Theta}^t{\\Psi^{t}}, \\forall\\psi^{t+1}\\in\\Psi^{t+1}$, is defined as, $$Q^\\circledast(\\vec{\\theta}^t, \\psi^{t+1}) = \\left{ \\begin{aligned} \u0026amp;R(\\vec{\\theta}^t, \\psi^{t+1}), \u0026amp;t=h-1\\ \u0026amp;R(\\vec{\\theta}^t, \\psi^{t+1}) + \\sum_{o^{t+1} \\in \\mathcal{O}} P(o^{t+1}|\\vec{\\theta}^t, \\psi^{t+1}) Q^\\circledast(\\vec{\\theta}^{t+1}, \\psi^{t+2, \\circledast}), \u0026amp;0\\leqslant t \u0026lt; h-1 \\ \\end{aligned} \\right .\\label{eq:SR-Q}$$ where $\\psi^{t+2, \\circledast}=(\\psi^{t+1}, \\delta^{t+1, \\circledast}_{\\psi}), \\forall \\ \\psi^{t+1} \\in \\Psi^{t+1}$.\nNote that the only difference of $Q^\\circledast$ from $Q^$ is $\\psi^{t+2, \\circledast}$, consequently expanding $Q^$’s candidates of history from $\\vec{\\theta}^t \\in \\vec{\\Theta}^t_{\\psi^{t, *}}$ to $\\vec{\\theta}^t \\in \\vec{\\Theta}^t_{\\Psi^{t}}$ and policy from $\\psi^{t+1}\\in(\\psi^{t, *},\\Delta^t)$ to $\\psi^{t+1}\\in(\\Psi^t,\\Delta^t)$.\nBeyond solving the problem of Proposition 1, another advantage of $Q^\\circledast$ is that it allows for the computation of optimal subsequent policy $\\xi^{t, *}_\\psi$ following any past policy $\\psi^{t}$. This is beneficial in online applications where agents may occasionally deviate from the optimal policy.\nOpen Questions # We have seen some advantages of defining the optimal Q-value function as $Q^\\circledast$, what are the downsides to defining it this way (e.g., high computational costs)? Oliehoek, Frans A., Matthijs T. J. Spaan, and Nikos Vlassis. 2008. “Optimal and Approximate Q-Value Functions for Decentralized POMDPs.” Journal of Artificial Intelligence Research 32: 289–353.\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. 2nd ed. MIT Press.\nThe dependency of $P(o^{t+1}|\\vec{\\theta}^t, \\psi^{t+1})$ is not a problem and can be solved just like how the stochasticity $P(s^{t+1}|s^t, a)$ tackled by double learning in Sec. 6.7, (Sutton and Barto 2018).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSingle-agent (PO)MDP, where the belief states are acquirable, does not have such a problem because the Q-value function is not necessarily history-dependent, thanks to Markovian property.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":5,"href":"/notes/docs/large-language-models/reward-modeling-llm/","title":"Reward Modeling LLM","section":"LLM","content":" Reward Models in RLHF # As LLMs scale, their raw outputs (optimized primarily for next-token prediction) often diverge from expected traits. To enable RL fine-tuning from human feedbacks (RLHF), reward models are introduced as trainable proxies for human preference. Once trained, it can generalize preference signals to unseen inputs, making alignment more scalable by reducing reliance on slow and costly human annotations. It also allows flexible fine-tuning toward different objectives, such as helpfulness, truthfulness, or safety.\nA typical alignment pipeline consists of 3 stages: supervised fine-tuning (SFT), reward modeling, and RL. After an initial SFT based on base transformer with curated human-labeled data, a reward model is constructed to predict human preferences over model-generated responses. This model is then used to guide further optimization by encouraging outputs that maximize the predicted reward. For example, ChatGPT employs reward models trained on ranked annotations to guide its generation toward preferred outputs (Achiam, Adler, and Agarwal 2024); DeepSeek and LLaMA 2 include explicit reward modeling components in their alignment pipelines, using pairwise preferences to train reward models that inform subsequent learning (Shao, Wang, and Zhu 2024; Touvron, Martin, and Stone 2023).\nRole of the reward models in RLHF (reward model in blue). Achiam, Josh, Steven Adler, and Sandhini Agarwal. 2024. “GPT-4 Technical Report.” https://arxiv.org/abs/2303.08774.\nShao, Zhihong, Peiyi Wang, and Qihao Zhu. 2024. “DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.” https://arxiv.org/abs/2402.03300.\nTouvron, Hugo, Louis Martin, and Kevin Stone. 2023. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” https://arxiv.org/abs/2307.09288.\n"}]