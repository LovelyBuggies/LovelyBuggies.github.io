<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LovelyBuggies&#39; Blog</title>
    <link>https://lovelybuggies.github.io/notes/</link>
    <description>Recent content on LovelyBuggies&#39; Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://lovelybuggies.github.io/notes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>From PG 2 PPO</title>
      <link>https://lovelybuggies.github.io/notes/docs/rl/from-pg-2-ppo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lovelybuggies.github.io/notes/docs/rl/from-pg-2-ppo/</guid>
      <description>&lt;link rel=&#34;stylesheet&#34; href=&#34;https://lovelybuggies.github.io/notes/katex/katex.min.css&#34; /&gt;&#xA;&lt;script defer src=&#34;https://lovelybuggies.github.io/notes/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xA;&#xA;  &lt;script defer src=&#34;https://lovelybuggies.github.io/notes/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body, {&#xA;  &amp;#34;delimiters&amp;#34;: [&#xA;    {&amp;#34;left&amp;#34;: &amp;#34;$$&amp;#34;, &amp;#34;right&amp;#34;: &amp;#34;$$&amp;#34;, &amp;#34;display&amp;#34;: true},&#xA;    {&amp;#34;left&amp;#34;: &amp;#34;$&amp;#34;, &amp;#34;right&amp;#34;: &amp;#34;$&amp;#34;, &amp;#34;display&amp;#34;: false},&#xA;    {&amp;#34;left&amp;#34;: &amp;#34;\\(&amp;#34;, &amp;#34;right&amp;#34;: &amp;#34;\\)&amp;#34;, &amp;#34;display&amp;#34;: false},&#xA;    {&amp;#34;left&amp;#34;: &amp;#34;\\[&amp;#34;, &amp;#34;right&amp;#34;: &amp;#34;\\]&amp;#34;, &amp;#34;display&amp;#34;: true}&#xA;  ],&#xA;  &amp;#34;throwOnError&amp;#34;: false,&#xA;  &amp;#34;strict&amp;#34;: false,&#xA;  &amp;#34;trust&amp;#34;: true,&#xA;  &amp;#34;macros&amp;#34;: {&#xA;    &amp;#34;\\label&amp;#34;: &amp;#34;\\htmlId{#1}{}&amp;#34;,&#xA;    &amp;#34;\\eqref&amp;#34;: &amp;#34;(\\ref{#1})&amp;#34;&#xA;  }&#xA;}&#xA;);&#34;&gt;&lt;/script&gt;&#xA;&#xA;&#xA;&lt;h1 id=&#34;policy-gradient-pg&#34;&gt;&#xA;  Policy Gradient (PG)&#xA;  &#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#policy-gradient-pg&#34;&gt;#&lt;/a&gt;&#xA;  &#xA;&lt;/h1&gt;&#xA;&lt;p&gt;Compared with value-based methods (Q-learning), Policy-based methods aim directly at learning the parameterized policy that can select actions without consulting a value function. PG methods seek to maximize a performance measure $J(\theta)$ with the policy’s parameter $\theta$, where the updates approximate gradient ascent in $J$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding Bellman</title>
      <link>https://lovelybuggies.github.io/notes/docs/rl/understanding-bellman/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lovelybuggies.github.io/notes/docs/rl/understanding-bellman/</guid>
      <description>&lt;link rel=&#34;stylesheet&#34; href=&#34;https://lovelybuggies.github.io/notes/katex/katex.min.css&#34; /&gt;&#xA;&lt;script defer src=&#34;https://lovelybuggies.github.io/notes/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xA;&#xA;  &lt;script defer src=&#34;https://lovelybuggies.github.io/notes/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body, {&#xA;  &amp;#34;delimiters&amp;#34;: [&#xA;    {&amp;#34;left&amp;#34;: &amp;#34;$$&amp;#34;, &amp;#34;right&amp;#34;: &amp;#34;$$&amp;#34;, &amp;#34;display&amp;#34;: true},&#xA;    {&amp;#34;left&amp;#34;: &amp;#34;$&amp;#34;, &amp;#34;right&amp;#34;: &amp;#34;$&amp;#34;, &amp;#34;display&amp;#34;: false},&#xA;    {&amp;#34;left&amp;#34;: &amp;#34;\\(&amp;#34;, &amp;#34;right&amp;#34;: &amp;#34;\\)&amp;#34;, &amp;#34;display&amp;#34;: false},&#xA;    {&amp;#34;left&amp;#34;: &amp;#34;\\[&amp;#34;, &amp;#34;right&amp;#34;: &amp;#34;\\]&amp;#34;, &amp;#34;display&amp;#34;: true}&#xA;  ],&#xA;  &amp;#34;throwOnError&amp;#34;: false,&#xA;  &amp;#34;strict&amp;#34;: false,&#xA;  &amp;#34;trust&amp;#34;: true,&#xA;  &amp;#34;macros&amp;#34;: {&#xA;    &amp;#34;\\label&amp;#34;: &amp;#34;\\htmlId{#1}{}&amp;#34;,&#xA;    &amp;#34;\\eqref&amp;#34;: &amp;#34;(\\ref{#1})&amp;#34;&#xA;  }&#xA;}&#xA;);&#34;&gt;&lt;/script&gt;&#xA;&#xA;&#xA;&lt;h1 id=&#34;bellman-equation-and-operator&#34;&gt;&#xA;  Bellman Equation and Operator&#xA;  &#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#bellman-equation-and-operator&#34;&gt;#&lt;/a&gt;&#xA;  &#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;definition&#34;&gt;&#xA;  Definition&#xA;  &#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#definition&#34;&gt;#&lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;The Bellman Equation and optimal Bellman Equation for V-values are, (Sutton and Barto 2018).&#xA;&lt;blockquote class=&#34;book-hint info&#34;&gt;&#xA;  &#xA;Note on determinism: The deterministic form is $V(s) = \max_{a} \{ R(s, a) + \gamma V(s&#39;)\}$, where $s&#39;\gets T(s,a)$.&#xA;&#xA;&lt;/blockquote&gt;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introduction 2 DecPOMDP</title>
      <link>https://lovelybuggies.github.io/notes/docs/rl/marl/introduction-2-decpomdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lovelybuggies.github.io/notes/docs/rl/marl/introduction-2-decpomdp/</guid>
      <description>&lt;h1 id=&#34;dec-pomdp&#34;&gt;&#xA;  Dec-POMDP&#xA;  &#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#dec-pomdp&#34;&gt;#&lt;/a&gt;&#xA;  &#xA;&lt;/h1&gt;&#xA;&lt;div class=&#34;definition&#34;&gt;&#xA;&lt;p&gt;&lt;strong&gt;Definition 1&lt;/strong&gt;. &lt;em&gt;A Dec-POMDP is a tuple $\langle \mathbb{I}, \mathcal{S}, {\mathbb{A}_i}, T, R, {\mathbb{O}_i}, O, \mathcal{H}, \gamma\rangle$:&lt;/em&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;em&gt;$\mathbb{I}$ is a finite sets of agents, $|\mathbb{I}|=n$;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;em&gt;$\mathcal{S}$ is a set of states with designated initial state distribution $b^0$;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;em&gt;$\mathbb{A}_i$ is a set of actions for agent $i$ with $\mathbb{A}\doteq \times_i \mathbb{A}_i$ the set of joint actions;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;em&gt;$T$ is the state transition probability function, $T$: $\mathcal{S} \times \mathbb{A} \times \mathcal{S} \rightarrow [0, 1]$, that specifies the probability of transitioning from state $s \in \mathcal{S}$ to $s&#39; \in \mathcal{S}$ when the actions $\boldsymbol{a} \in \mathbb{A}$ are taken by agents (i.e., $T(s, \boldsymbol{a}, s&#39;)=P(s&#39;|s, \textbf{a})$);&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Optimal Q Value Functions for Dec POMDP</title>
      <link>https://lovelybuggies.github.io/notes/docs/rl/marl/optimal-q-value-functions-for-dec-pomdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lovelybuggies.github.io/notes/docs/rl/marl/optimal-q-value-functions-for-dec-pomdp/</guid>
      <description>&lt;h1 id=&#34;notions&#34;&gt;&#xA;  Notions&#xA;  &#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#notions&#34;&gt;#&lt;/a&gt;&#xA;  &#xA;&lt;/h1&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$s^t$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the state at $t$ with problem horizon $h$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$o^t$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the joint observation of agents $o^t=\langle o_1^t, \dots, o_n^t \rangle$ at $t$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\mathcal{O}$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the joint observation space&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\vec{\theta}^t$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the joint observation-action history until $t$, $\vec{\theta}^t=(o^0, a^0, \dots, o^t)$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\vec{\Theta}^t$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the joint history space at $t$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\vec{\Theta}^t_\pi$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the set of $\vec{\theta}^t$ consistent with policy $\pi$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\delta^{t}$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the decision rule (a temporal structure of policy) at $t$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\delta^{t,*}$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the optimal decision rule at $t$ following $\psi^{t-1, *}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\delta^{t,\circledast}_\psi$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the optimal decision rule at $t$ following $\psi^{t-1}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\Delta^t$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the decision rule space at $t$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\psi^{t}$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the past joint policy until $t$, $\psi^{t} = \delta^{[0, t)}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\psi^{t, *}$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the optimal past joint policy until $t$, $\psi^{t, *} = \delta^{[0, t), *}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\psi^{t, \circledast}$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the past joint policy until $t$ with non-optimal $\psi^{t-1}$ and optimal $\delta^{t-1, \circledast}_\psi$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\Psi^{t}$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the past joint policy space at $t$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\xi^t$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the subsequent joint policy from $t$, $\xi^{t} = \delta^{[t, h)}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\xi^{t, *}$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the optimal subsequent joint policy from $t$, $\xi^{t} = \delta^{[t, h), *}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\xi^{t, \circledast}_\psi$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the optimal subsequent joint policy from $t$ following non-optimal $\psi^t$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\pi$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the joint pure policy $\pi=\delta^{[0, h)}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\pi^*$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the joint optimal pure policy $\pi^*=\delta^{[0, h), *}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$R(\vec{\theta}^t, \psi^{t+1})$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the immediate reward function following $\psi^{t+1}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$Q(\vec{\theta}^t, \psi^{t+1})$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the history-policy value function following $\psi^{t+1}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$Q^*(\vec{\theta}^t, \psi^{t+1})$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the optimal history-policy value function following $\psi^{t+1}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$Q^\circledast(\vec{\theta}^t, \psi^{t+1})$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the sequentially rational optimal history-policy value function following $\psi^{t+1}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h1 id=&#34;normative-optimal-q-value-function&#34;&gt;&#xA;  Normative Optimal Q-Value Function&#xA;  &#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#normative-optimal-q-value-function&#34;&gt;#&lt;/a&gt;&#xA;  &#xA;&lt;/h1&gt;&#xA;&lt;div id=&#34;defn:normative-Q&#34; class=&#34;definition&#34;&gt;&#xA;&lt;p&gt;&lt;strong&gt;Definition 1&lt;/strong&gt;. &lt;em&gt;The optimal Q-value function $Q^&lt;/em&gt;$ in Dec-POMDP, the expected cumulative reward over time steps $[t,h)$ induced by optimal joint policy $\pi^{&lt;em&gt;}$, $\forall \vec{\theta}^t\in \vec{\Theta}^t_{\psi^{t, &lt;em&gt;}}, \forall \psi^{t+1}\in(\psi^{t, &lt;em&gt;},\Delta^t)$, is defined as, $$Q^&lt;/em&gt;(\vec{\theta}^t, \psi^{t+1}) = \left{&#xA;\begin{aligned}&#xA;&amp;amp;R(\vec{\theta}^t, \psi^{t+1}), &amp;amp;t=h-1 \&#xA;&amp;amp;R(\vec{\theta}^t, \psi^{t+1}) + \sum_{o^{t+1} \in \mathcal{O}} P(o^{t+1}|\vec{\theta}^t, \psi^{t+1}) Q^&lt;/em&gt;(\vec{\theta}^{t+1}, \pi^&lt;/em&gt;(\vec{\theta}^{t+1})). &amp;amp;0\leqslant t &amp;lt; h-1 \&#xA;\end{aligned}&#xA;\right .\label{eq:normative-Q}$$*&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reward Modeling LLM</title>
      <link>https://lovelybuggies.github.io/notes/docs/large-language-models/reward-modeling-llm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lovelybuggies.github.io/notes/docs/large-language-models/reward-modeling-llm/</guid>
      <description>&lt;link rel=&#34;stylesheet&#34; href=&#34;https://lovelybuggies.github.io/notes/katex/katex.min.css&#34; /&gt;&#xA;&lt;script defer src=&#34;https://lovelybuggies.github.io/notes/katex/katex.min.js&#34;&gt;&lt;/script&gt;&#xA;&#xA;  &lt;script defer src=&#34;https://lovelybuggies.github.io/notes/katex/auto-render.min.js&#34; onload=&#34;renderMathInElement(document.body, {&#xA;  &amp;#34;delimiters&amp;#34;: [&#xA;    {&amp;#34;left&amp;#34;: &amp;#34;$$&amp;#34;, &amp;#34;right&amp;#34;: &amp;#34;$$&amp;#34;, &amp;#34;display&amp;#34;: true},&#xA;    {&amp;#34;left&amp;#34;: &amp;#34;$&amp;#34;, &amp;#34;right&amp;#34;: &amp;#34;$&amp;#34;, &amp;#34;display&amp;#34;: false},&#xA;    {&amp;#34;left&amp;#34;: &amp;#34;\\(&amp;#34;, &amp;#34;right&amp;#34;: &amp;#34;\\)&amp;#34;, &amp;#34;display&amp;#34;: false},&#xA;    {&amp;#34;left&amp;#34;: &amp;#34;\\[&amp;#34;, &amp;#34;right&amp;#34;: &amp;#34;\\]&amp;#34;, &amp;#34;display&amp;#34;: true}&#xA;  ],&#xA;  &amp;#34;throwOnError&amp;#34;: false,&#xA;  &amp;#34;strict&amp;#34;: false,&#xA;  &amp;#34;trust&amp;#34;: true,&#xA;  &amp;#34;macros&amp;#34;: {&#xA;    &amp;#34;\\label&amp;#34;: &amp;#34;\\htmlId{#1}{}&amp;#34;,&#xA;    &amp;#34;\\eqref&amp;#34;: &amp;#34;(\\ref{#1})&amp;#34;&#xA;  }&#xA;}&#xA;);&#34;&gt;&lt;/script&gt;&#xA;&#xA;&#xA;&lt;h1 id=&#34;reward-models-in-rlhf&#34;&gt;&#xA;  Reward Models in RLHF&#xA;  &#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#reward-models-in-rlhf&#34;&gt;#&lt;/a&gt;&#xA;  &#xA;&lt;/h1&gt;&#xA;&lt;p&gt;As LLMs scale, their raw outputs (optimized primarily for next-token prediction) often diverge from expected traits. To enable RL fine-tuning from human feedbacks (RLHF), reward models are introduced as trainable proxies for human preference. Once trained, it can generalize preference signals to unseen inputs, making alignment more scalable by reducing reliance on slow and costly human annotations. It also allows flexible fine-tuning toward different objectives, such as helpfulness, truthfulness, or safety.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Welcome</title>
      <link>https://lovelybuggies.github.io/notes/docs/welcome/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lovelybuggies.github.io/notes/docs/welcome/</guid>
      <description>&lt;h1 id=&#34;welcome&#34;&gt;&#xA;  Welcome&#xA;  &#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#welcome&#34;&gt;#&lt;/a&gt;&#xA;  &#xA;&lt;/h1&gt;&#xA;&lt;p&gt;This is a fresh Markdown post rendered with the Hugo Book theme. Use it as a template for your future notes and posts.&lt;/p&gt;&#xA;&lt;h2 id=&#34;whats-included&#34;&gt;&#xA;  What’s Included&#xA;  &#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#whats-included&#34;&gt;#&lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Sections and ToC are automatic based on headings.&lt;/li&gt;&#xA;&lt;li&gt;KaTeX math support is enabled: $V(s) = \max_a Q(s,a)$.&lt;/li&gt;&#xA;&lt;li&gt;Syntax-highlighted code blocks:&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Q&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Q&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;tips&#34;&gt;&#xA;  Tips&#xA;  &#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#tips&#34;&gt;#&lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;blockquote class=&#34;book-hint info&#34;&gt;&#xA;  &#xA;Place images next to this file, e.g., `docs/welcome/diagram.png`, and reference with `![Alt](diagram.png)`.&#xA;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Add front matter fields like &lt;code&gt;tags&lt;/code&gt;, &lt;code&gt;weight&lt;/code&gt;, or &lt;code&gt;draft: true&lt;/code&gt; as needed.&lt;/li&gt;&#xA;&lt;li&gt;For math blocks, wrap in &lt;code&gt;$$ ... $$&lt;/code&gt; for display equations.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;next-steps&#34;&gt;&#xA;  Next Steps&#xA;  &#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#next-steps&#34;&gt;#&lt;/a&gt;&#xA;  &#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Create your own post by copying this folder and editing the front matter and content. Example:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
