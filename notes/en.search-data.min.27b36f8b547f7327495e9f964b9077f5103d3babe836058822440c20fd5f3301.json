[{"id":0,"href":"/notes/docs/","title":"Docs","section":"LovelyBuggies' Blog","content":" title: \u0026ldquo;Blog\u0026rdquo; weight: 1 # Welcome to the Blog. Use the sidebar to browse sections. To add new posts, see “How to Add a Blog/Note”.\n"},{"id":1,"href":"/notes/docs/rl/from-pg-2-ppo/","title":"From PG 2 PPO","section":"RL","content":" Policy Gradient (PG) # Compared with value-based methods (Q-learning), Policy-based methods aim directly at learning the parameterized policy that can select actions without consulting a value function. PG methods seek to maximize a performance measure $J(\\theta)$ with the policy’s parameter $\\theta$, where the updates approximate gradient ascent in $J$,1 $$\\label{eq:pg} \\theta^{(i+1)} \\leftarrow \\theta^{(i)} + \\alpha\\nabla J(\\theta^{(i)}).$$ There are 2 main advantages of PG methods,\nApproximating policy can approach a deterministic policy, whereas $\\epsilon$-greedy always has probability of selecting a random action;\nWith continuous policy parameterization, the action probabilities change smoothly as a function of the learned parameter, whereas $\\epsilon$-greedy may change dramatically for an arbitrarily small change in the estimated action values.\nSince the major purpose of this article is to introduce PPO methods from PG, we omit some other important forms of PG here. Readers can find them in the Appendix.\nPG Theorem # An intuitive way to calculate Equation [eq:pg] is to replace $J(\\theta)$ with $V^{\\pi_{\\theta}} (s_0)$.2 However, the calculation is hard as it directly depends on both the action selection and indirectly the distribution of states following the target selection. PG theorem provides a nice reformulation of the derivative of the objective function to not involve the state distribution derivation.\nTheorem 1. Taking the state-value function as the optimizing target, the objective gradient follows, $$\\label{equ:pgthem} \\nabla J(\\theta) \\propto \\sum_s d^\\pi(s) \\sum_a Q^\\pi(s,a) \\nabla \\pi(a|s),$$ where $d^\\pi(s)$ is the stationary distribution of the policy $\\pi_{\\theta}$.\nTo sample with expectation equals or approximates the expression Equ. [equ:pgthem], $$\\label{equ:pgtheorem-sample} \\begin{aligned} \\nabla J(\\theta) \u0026amp;\\propto \\sum_s d^\\pi(s)\\sum_a Q^\\pi (s,a) \\nabla\\pi(a|s) \\ \u0026amp;= \\mathbb{E}{d^\\pi}\\left[\\sum_a Q^\\pi (s,a) \\nabla\\pi(a|s) \\right] \\ \u0026amp;= \\mathbb{E}{d^\\pi}\\left[\\sum_a \\pi(a|s) Q^\\pi (s,a) \\frac{\\nabla\\pi(a|s)}{\\pi(a|s)} \\right] \\ \u0026amp;= \\mathbb{E}{\\pi}\\left[Q^\\pi (s,a) \\frac{\\nabla\\pi(a|s)}{\\pi(a|s)} \\right] \\ \u0026amp;= \\mathbb{E}{\\pi}\\left[Q^\\pi (s,a) \\nabla\\ln\\pi(a|s) \\right]. \\end{aligned}$$ The eligibility vector $\\nabla\\ln\\pi(a|s)$ is the only place the policy parameterization appears, which can be omitted $L(\\theta)=\\mathbb{E}_{\\pi}[Q^\\pi(s,a)]$ since it will be automatically recovered when differentiating.\nPG with Baseline # Theorem 2. PG theorem can be generalized to include a comparison of the action value to an arbitrary baseline $b(s)$, as long as $b(s)$ does not depend on $a$, and this will reduce the variance while keeping it unbiased. $$\\label{equ:reinforce-baseline} \\begin{aligned} \\nabla J(\\theta) \u0026amp;\\propto \\sum_s d^\\pi(s)\\sum_a (Q^\\pi (s,a) -b(s)) \\nabla\\pi(a|s) \\ \u0026amp;= \\mathbb{E}_{\\pi} \\left[(Q^\\pi(s,a) -b(s)) \\nabla\\ln\\pi(a|s)\\right]. \\end{aligned}$$\nAccording to the Theorem 2, the expected return $Q(s,a)$ in Theorem 1 can be replaced by $G$ (expected return of the full or following trajectory by Monte Carlo), $A$ (advantage by Generalized Advantage Estimation or state-value prediction), and $\\delta$ (TD-residual by critic prediction).\nOff-Policy PG # Off-policy sampling reuses any past episodes, which has a higher efficiency and brings more exploration. To make PG off-policy, we adjust it with an importance weight $\\frac{\\pi(a|s)}{\\beta(a|s)}$ to correct the mismatch between behavior and target policies.\n$$\\label{equ:pgthem-off-policy} \\begin{aligned}\n\\nabla J(\\theta) \u0026amp;=\\nabla \\left(\\sum_s d^\\beta(s) V^\\pi(s)\\right)\\ \u0026amp;=\\nabla\\left(\\sum_s d^\\beta(s) \\sum_a \\pi(a|s) Q^\\pi(s,a)\\right)\\ \u0026amp;=\\sum_s d^\\beta(s) \\sum_a (\\nabla \\pi(a|s) Q^\\pi(s,a)+\\pi(a|s) \\nabla Q^\\pi(s,a))\\ \u0026amp;\\stackrel{\\text{(i)}}{\\approx} \\sum_s d^\\beta(s) \\sum_a Q^\\pi(s,a) \\nabla \\pi(a|s) \\ \u0026amp;=\\mathbb{E}{d^\\beta}\\left[\\sum_a \\beta(a|s) \\frac{\\pi(a|s)}{\\beta(a|s)} Q^\\pi(s,a) \\frac{\\nabla \\pi(a|s)}{\\pi(a|s)}\\right]\\ \u0026amp;=\\mathbb{E}{\\beta}\\left[\\frac{\\pi(a|s)}{\\beta(a|s)} Q^\\pi(s,a) \\nabla\\ln \\pi(a|s)\\right], \\end{aligned}$$ where $d^\\beta(s)$ is the stationary distribution of the behavior policy $\\beta$, and $Q^\\pi$ is the Q-function estimated regard to the target policy $\\pi$. Because of hard computation in reality (i), we ignore the approximation term $\\nabla Q^\\pi(s,a)$.\nProximal Policy Optimization (PPO) # In this section, we introduce standard PPO and it variants in different domains.\nClip-PPO # (Schulman et al. 2017) proposed the standard PPO that uses a clipped surrogate objective to ensure the policy updates are small and controlled (proximal). Since the advantage under current policy is intangible, we can use Generalized Advantage Estimation (GAE) of the last policy to estimate $\\hat{A}^{\\pi_{\\theta_{\\text{old}}}}$ to reduce the variance of policy gradient methods and maintain low bias (Schulman et al. 2015), $$\\label{equ:Clip-PPO} J^{\\text{CLIP}}(\\theta) = \\mathbb{E}{\\pi{\\theta_{\\text{old}}}} \\left[ \\min \\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)} \\hat{A}^{\\pi_{\\theta_{\\text{old}}}}(s, a), \\text{clip}(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)}, 1 - \\epsilon, 1 + \\epsilon) \\hat{A}^{\\pi_{\\theta_{\\text{old}}}}(s, a) \\right) \\right],$$ where $\\hat{A}^\\text{GAE}t = \\sum{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}$, $\\delta$ is the TD error, and $\\lambda$ is a hyperparameter controlling the trade-off between bias and variance. Note that the clipping could also occur in the value network to stabilize the training process.\nThe objective function can be augmented with an entropy term to encourage exploration, $$\\label{equ:PPO} J^{\\text{CLIP+}}(\\theta) =\\mathbb{E}{\\pi{\\theta_{\\text{old}}}} \\left[J^{\\text{CLIP}}(\\theta)- c\\sum_{a} \\pi_{\\theta}(a|s) \\log \\pi_{\\theta}(a|s))\\right].$$\nInitialize: policy parameter $\\theta$ for actor network $\\pi_{\\theta}$, parameter $w$ for critic network $V_{w}$, replay memory $\\mathcal{D}$ Generate an episode following policy $\\pi_{\\theta_{\\text{old}}}$ and store it into $\\mathcal{D}$ Estimate reward-to-go $\\hat{R}$ and $\\hat{A}^{\\pi_{\\theta_{\\text{old}}}}$ using GAE Compute $J^{\\text{CLIP+}}(\\theta)$ for all samples according to Equ. [equ:PPO] $w \\leftarrow w + \\alpha_w \\frac{1}{N}\\sum_i\\nabla_w (V_w(s_i)-\\hat{R}(s_i, a_i))^2$ $\\theta \\leftarrow \\theta + \\alpha_\\theta \\frac{1}{N}\\sum_i \\nabla_\\theta J^{\\text{CLIP+H}}(\\theta)$ $\\theta_{\\text{old}} \\leftarrow \\theta$\nKL-PPO # Another formulation of PPO to improve training stability, so-called Trust Region Policy Optimization (TRPO), enforces a KL divergence constraint on the size of the policy update at each iteration (Schulman et al. 2017). $$\\label{alg:TRPO} \\begin{gathered} J^{\\text{KL}}(\\theta) = \\mathbb{E}{\\pi{\\theta_{\\text{old}}}} \\left[ \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{\\text{old}}}(a|s)} \\hat{A}^{\\pi_{\\theta_{\\text{old}}}}(s, a)-c \\mathcal{D}\\text{KL}(\\pi{\\theta_{\\text{old}}} | \\pi_{\\theta}) \\right], \\end{gathered}$$ where $\\mathcal{D}\\text{KL}(\\pi{\\theta_{\\text{old}}} | \\pi_{\\theta}) = \\sum_{a} \\pi_{\\theta_{\\text{old}}}(a | s) \\log \\frac{\\pi_{\\theta_{\\text{old}}}(a | s)}{\\pi_{\\theta}(a| s)}$.\nSometimes, the KL-penalty can be combined with policy clipping to achieve better performance in practice.\nAdaptive-KL-PPO # (Schulman et al. 2017) also mentioned Adaptive-KL-PPO, where the KL penalty coefficient is adjusted dynamically. If the policy update is too aggressive $\\left( \\mathcal{D}\\text{KL} \\gg \\mathcal{D}\\text{threshold} \\right)$, $c$ is increased to penalize large updates; else if the update is too conservative $\\left( \\mathcal{D}\\text{KL} \\ll \\mathcal{D}\\text{threshold} \\right)$, $c$ is decreased to allow larger updates.\nMulti-Agent PPO # In the multi-agent setting, the PPO algorithm can be implemented independently (IPPO) or by a centralized critic (MAPPO). In IPPO, each agent has its own actor and critic and learns independently according to a joint reward (Witt et al. 2020). Like IPPO, MAPPO employs weight sharing between agents’ critics, and the advantage in MAPPO is estimated through joint GAE (Yu et al. 2022). $$\\label{equ:MAPPO} \\small \\begin{gathered} J^\\text{IPPO}(\\theta_i) = \\mathbb{E}{\\pi{\\theta_{i, \\text{old}}}} \\left[ \\min \\left( \\frac{\\pi_{\\theta_i}(a|s)}{\\pi_{\\theta_{i, \\text{old}}}(a|s)} \\hat{A}^{\\pi_{\\theta_{i, \\text{old}}}}(s, a), \\text{clip}(\\frac{\\pi_{\\theta_i}(a|s)}{\\pi_{\\theta_{i, \\text{old}}}(a|s)}, 1 - \\epsilon, 1 + \\epsilon) \\hat{A}^{\\pi_{\\theta_{i,\\text{old}}}}(s, a) \\right) \\right],\\ J^\\text{MAPPO}(\\theta_i) = \\mathbb{E}{\\pi{\\theta_{\\text{old}}}} \\left[ \\min \\left( \\frac{\\pi_{\\theta_i}(a|s)}{\\pi_{\\theta_{i, \\text{old}}}(a|s)} \\hat{\\boldsymbol{A}}^{\\pi_{\\theta_{\\text{old}}}}(s, a), \\text{clip}(\\frac{\\pi_{\\theta_i}(a|s)}{\\pi_{\\theta_{i, \\text{old}}}(a|s)}, 1 - \\epsilon, 1 + \\epsilon) \\hat{\\boldsymbol{A}}^{\\pi_{\\theta_{\\text{old}}}}(s, a) \\right) \\right]. \\end{gathered}$$\nNote that there are some other instantiations of IPPO, but not all of them are vulnerable to non-convergence issues. The one with full actor critic parameter or information sharing can be regarded as a centralized method. Besides, for cases where a general solution is still intangible even with parameter sharing (e.g. the exclusive game), heterogeneous-agent PPO allows the agents to take turns learning by using others’ information, which can work well with strong assumptions.3\nGroup Relative Policy Optimization (GRPO) # As DeepSeek has made a splash in the LLM community, the RL method GRPO involved has received a lot of attention (Zhihong Shao 2024). GRPO is a variant of PPO, where the advantage is estimated using group-relative comparisons rather than GAE. This approach eliminates the critic model, which improves the training efficiency and stability. The DeepSeek framework consists of: (i) a frozen reference model, which is a stable baseline for computing rewards; (ii) a given reward model, responsible for evaluating generated outputs and assigning scores; (iii) a value model, which estimates the expected return of a given state to aid in policy optimization; and (iv) a policy model, which generates $|\\mathcal{G}|$ responses and is continuously updated to improve performance based on feedback from the other components. The learning objective for GRPO is, $$\\small J^\\text{GRPO}(\\theta) = \\mathbb{E}{\\pi{\\theta_\\text{old}}, i \\in \\mathcal{G}} \\left[ \\min \\left( \\frac{\\pi_{\\theta}(a_{i} | s, \\vec{a}{i})}{\\pi{\\theta_\\text{old}}(a_{i} | s, \\vec{a}{i})} \\hat{A}^\\mathcal{G}, \\text{clip}(\\frac{\\pi{\\theta}(a_{i} | s, \\vec{a}{i})}{\\pi{\\theta_\\text{old}}(a_{i} | s, \\vec{a}{i})}, 1 - \\epsilon, 1 + \\epsilon) \\hat{A}^{\\mathcal{G}}\\right)-c\\mathcal{D}\\text{KL}(\\pi_\\text{ref} | \\pi_{\\theta})\\right],$$ where the advantage $\\hat{A}^\\mathcal{G}i=\\frac{r_i-\\text{mean}(r)}{\\text{std}(r)}$ is estimated by grouped actions produced at the same state. $\\mathcal{D}\\text{KL}(\\pi_\\text{ref} | \\pi_{\\theta})=\\frac{\\pi_{\\text{ref}}(a_{i} \\mid s, \\vec{a}{i})}{\\pi{\\theta}(a_{i} \\mid s, \\vec{a}_{i})}\n\\ln \\frac{\\pi_{\\text{ref}}(a_{i} \\mid s, \\vec{a}{i})}{\\pi{\\theta}(a_{i} \\mid s, \\vec{a}_i)} - 1$ is a positive unbiased estimator, which measures the difference between the policy of trained model and reference model (like direct policy optimization). Schulman, John, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2015. “High-Dimensional Continuous Control Using Generalized Advantage Estimation.” arXiv Preprint arXiv:1506.02438. https://arxiv.org/abs/1506.02438.\nSchulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. “Proximal Policy Optimization Algorithms.” https://arxiv.org/abs/1707.06347.\nWitt, Christian Schroeder de, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip H. S. Torr, Mingfei Sun, and Shimon Whiteson. 2020. “Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?” https://arxiv.org/abs/2011.09533.\nYu, Chao, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. 2022. “The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games.” https://arxiv.org/abs/2103.01955.\nZhihong Shao, Qihao Zhu, Peiyi Wang. 2024. “DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.” CoRR. https://arxiv.org/abs/2402.03300.\nAll methods following this schema are PG, whether or not they also learn an approximate value function.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTo simplify the notation, we omit $\\theta$ in the subscripts, superscripts, and gradient operators, assuming $\\pi$ is a function of $\\theta$ and all gradients are implicit with respect to $\\theta$, i.e., $V^{\\pi}\\equiv V^{\\pi_{\\theta}}$, $Q^{\\pi}\\equiv Q^{\\pi_{\\theta}}$ and $\\nabla\\equiv\\nabla_{\\theta}$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA great example is PettingZoo’s agent cycle and parallel environments.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":2,"href":"/notes/docs/rl/marl/introduction-2-decpomdp/","title":"Introduction 2 DecPOMDP","section":"MARL","content":" Dec-POMDP # Definition 1. A Dec-POMDP is a tuple $\\langle \\mathbb{I}, \\mathcal{S}, {\\mathbb{A}_i}, T, R, {\\mathbb{O}_i}, O, \\mathcal{H}, \\gamma\\rangle$:\n$\\mathbb{I}$ is a finite sets of agents, $|\\mathbb{I}|=n$;\n$\\mathcal{S}$ is a set of states with designated initial state distribution $b^0$;\n$\\mathbb{A}_i$ is a set of actions for agent $i$ with $\\mathbb{A}\\doteq \\times_i \\mathbb{A}_i$ the set of joint actions;\n$T$ is the state transition probability function, $T$: $\\mathcal{S} \\times \\mathbb{A} \\times \\mathcal{S} \\rightarrow [0, 1]$, that specifies the probability of transitioning from state $s \\in \\mathcal{S}$ to $s\u0026rsquo; \\in \\mathcal{S}$ when the actions $\\boldsymbol{a} \\in \\mathbb{A}$ are taken by agents (i.e., $T(s, \\boldsymbol{a}, s\u0026rsquo;)=P(s\u0026rsquo;|s, \\textbf{a})$);\n$R$ is the joint reward function, where $R$: $\\mathcal{S} \\times \\mathbb{A} \\rightarrow \\mathbb{R}$;\n$\\mathbb{O}_i$ is a set of observations for each agent $i$, with $\\mathbb{O}\\doteq\\times_i \\mathbb{O}_i$ the set of joint observations;\n$O$ is an observation probability function, $O$: $\\mathbb{A} \\times \\mathcal{S} \\times \\mathbb{O}$, that specifies the probability of seeing observation $\\boldsymbol{o}\u0026rsquo; \\in \\mathbb{O}$ given the actions $\\boldsymbol{a} \\in \\mathbb{A}$ are taken and state $s\u0026rsquo; \\in \\mathcal{S}$ is observed (i.e., $O(\\boldsymbol{a}, s\u0026rsquo;, \\boldsymbol{o}\u0026rsquo;)=P(\\boldsymbol{o}\u0026rsquo;|\\textbf{a},s\u0026rsquo;)$);\n$\\mathcal{H}$ is the horizon (the number of steps until termination);\n$\\gamma$ is the discount factor for the return.\nA solution to a Dec-POMDP is a joint policy $\\boldsymbol{\\pi}:\\mathbb{H}_i\\to\\mathbb{A}_i, \\forall i \\in \\mathbb{I}$ over joint observation-action history $\\boldsymbol{h}={\\boldsymbol{a}^{0}, \\boldsymbol{o}^{1}, \\cdots \\boldsymbol{o}^{\\mathcal{H}-1}}$, an optimal solution maximizes the expected return, $$\\boldsymbol{\\pi}^=\\mathop{\\mathrm{argmax}}{\\boldsymbol{\\pi}}\\mathbb{E}\\left[\\textstyle\\sum{t=0}^{\\mathcal{H}-1}R(\\boldsymbol{h}, \\boldsymbol{\\pi}(\\boldsymbol{h}))\\middle|b^0\\right].$$*\nIn Dec-POMDP, the Bellman recursive formulation of the history V-function is, $$\\label{eq:decpomdp-V} \\begin{aligned} V^{\\boldsymbol{\\pi}}(\\boldsymbol{h}) \u0026amp;= \\sum_{s} P(s|b^0, \\boldsymbol{h})\\left[R(s, \\boldsymbol{\\pi}(\\boldsymbol{h}))+\\gamma \\sum_{s\u0026rsquo;}P(s\u0026rsquo;|s, \\boldsymbol{\\pi}(\\boldsymbol{h})) \\sum_{\\boldsymbol{o}\u0026rsquo;}P(\\boldsymbol{o}\u0026rsquo;|\\boldsymbol{\\pi}(\\boldsymbol{h}), s\u0026rsquo;) V^{\\boldsymbol{\\pi}}(\\boldsymbol{h}\u0026rsquo;) \\right]\\ \u0026amp;\\equiv R(\\boldsymbol{h}, \\boldsymbol{\\pi})+\\gamma\\sum_{\\boldsymbol{o}\u0026rsquo;}P(\\boldsymbol{o}\u0026rsquo;|\\boldsymbol{h}, \\boldsymbol{\\pi}) V^{\\boldsymbol{\\pi}}(\\boldsymbol{h}\u0026rsquo;), \\end{aligned}$$ the Bellman recursive formulation of the history-policy Q-function is,1 $$\\label{eq:decpomdp-q} \\small \\begin{aligned} Q^{\\boldsymbol{\\pi}}(\\boldsymbol{h}, \\boldsymbol{\\pi}) \u0026amp;= \\sum_{s} P(s|b^0, \\boldsymbol{h})\\left[R(s, \\boldsymbol{\\pi}(\\boldsymbol{h}))+\\gamma\\sum_{s\u0026rsquo;}P(s\u0026rsquo;|s, \\boldsymbol{\\pi}(\\boldsymbol{h})) \\sum_{\\boldsymbol{o}\u0026rsquo;}P(\\boldsymbol{o}\u0026rsquo;|\\boldsymbol{\\pi}(\\boldsymbol{h}), s\u0026rsquo;) Q^{\\boldsymbol{\\pi}}(\\boldsymbol{h}\u0026rsquo;, \\boldsymbol{\\pi}(\\boldsymbol{h}\u0026rsquo;)) \\right]\\ \u0026amp;\\equiv R(\\boldsymbol{h}, \\boldsymbol{\\pi})+\\gamma\\sum_{\\boldsymbol{o}\u0026rsquo;}P(\\boldsymbol{o}\u0026rsquo;|\\boldsymbol{h}, \\boldsymbol{\\pi}) Q^{\\boldsymbol{\\pi}}(\\boldsymbol{h}\u0026rsquo;, \\boldsymbol{\\pi}) . \\end{aligned}$$\nSubclasses # Centralized Control # MMDP is a fully observable version of Dec-POMDP, but it does not specify decentralized control. Dec-MDP assumes that the joint observations uniquely determine the state, while agents still act with local observations. Similarly, MPOMDP does not specify whether the control is decentralized, which could have a centralized policy $\\mathbb{H}\\to\\mathbb{A}$.\nIndependent Variables # A decentralized control model might be factorized with independent local variables, e.g., transition-independence (TI) $T(s, \\boldsymbol{a}, s\u0026rsquo;)=\\Pi_{i=1}^{n} T(s_i, a_i, s_i\u0026rsquo;)$ and reward-independence (RI) $R(s,\\boldsymbol{\\pi})=f_\\text{mono}(\\langle R(s_i, \\pi_i)\\rangle_{i=1}^{n})$. Network-distributed POMDP (ND-POMDP) represents the factored one with TI and block-RI, i.e., $R(s,\\boldsymbol{\\pi})=f_\\text{mono}(\\langle R(s_{i, \\mathcal{N}(i)}, \\pi_{i, \\mathcal{N}(i)})\\rangle_{i=1}^{n})$, where ${\\mathcal{N}(i)}$ are the neighbors of $i$.\nComplexity # The worst-case complexity of finite-horizon problems is: (by (Amato et al. 2013))\nModel Complexity MDP P-complete MMDP (Cen-MMDP) P-complete Dec-MDP NEXP-complete Dec-MDP with TI no RI NP-complete Dec-MDP with RI no TI NEXP-complete Dec-MDP with TI and RI P-complete POMDP PSPACE-complete MPOMDP (Cen-MPOMDP) PSPACE-complete Dec-POMDP NEXP-complete ND-POMDP NEXP-complete Theorem 1. An MDP is P-complete in finite and infinite horizons (Papadimitriou and Tsitsiklis 1987).\nTheorem 2. A finite POMDP is PSPACE-complete (Papadimitriou and Tsitsiklis 1987).\nTheorem 3. The complexity of an infinite POMDP is undecidable (Madani, Hanks, and Condon 1999), leading to the undecidability of the infinite Dec-POMDP complexity.\nTheorem 4. A finite $\\text{Dec-POMDP}{n\\geqslant2}$ is NEXP-complete, and a finite $\\text{Dec-MDP}{n\\geqslant3}$ is also NEXP-complete (Bernstein et al. 2002).\nFact 1. A Dec-MDP with TI and RI can be solved independently, resulting in P-complete.\nTheorem 5. A Dec-MDP with TI and joint reward is NP-complete, a Dec-MDP with RI but no TI is NEXP-complete (Becker et al. 2004)\nFact 2. An ND-POMDP has the same worst-case complexity as a Dec-POMDP (Nair et al. 2005).\nPlanning Methods # Policy Structure # Calculating a shared belief state in Dec-POMDP is hard, because the policy can not be recovered from the value function. The policies are normally maintained in a policy tree or FSC. Policies can be extracted by starting at the root (or initial node) and continuing to the subtree (or next node) based on observations, and can be evaluated by summing the rewards weighted by transition probability.2\nOptimal Approaches # Bottom-up # DP uses joint belief to find optimal solutions with policy pruning (Hansen, Bernstein, and Zilberstein 2004).\n$$V^{t+1}(b^t) = \\max_{a \\in \\mathcal{A}} \\left{ \\sum_{s \\in \\mathcal{S}} b^t(s) \\left[ R(s, a) + \\sum_{o \\in \\mathcal{O}} \\mathcal{P}(o \\mid s, a) V^t(b^{t+1}) \\right] \\right}.$$\nInput: Depth-$t$ policy trees $Q_i^t$ and value vectors $V_i^t$ for each $i$\nPerform exhaustive backups to get $Q_i^{t+1}$, and compute $V_i^{t+1}$ accordingly for each $i$ Find a policy tree $q_j \\in Q_i^{t+1}$ that satisfies $\\exists \\ v_k \\in {V_i^{t+1} \\setminus v_j},b^{t+1} v_k \\geq b^{t+1} v_j, \\forall \\ b^{t+1}$ $Q_i^{t+1} \\gets {Q_i^{t+1} \\setminus q_j}$, and $V_i^{t+1} \\gets {V_i^{t+1} \\setminus v_j}$ accordingly\nOutput: Depth-$t+1$ policy trees $Q_i^{t+1}$ and value vectors $V_i^{t+1}$ for each $i$\nTop-down # The policy tree can also be built using heuristic search like MAA* (Szer, Charpillet, and Zilberstein 2005).\nInitialize: Joint policy tree root $\\Pi\\gets\\times_i \\mathbb{A}_i$\nInput: Depth-$t$ joint policy tree $\\Pi$\nSelect $a^=\\mathop{\\mathrm{argmax}}_{a\\in \\Pi}F^\\mathcal{H}(b^0, a)$ (heuristic and value) Expand $a^$ to $a^\\circledast$, and $\\Pi \\gets {\\Pi \\cup a^\\circledast}$ $\\Pi \\gets {\\Pi \\setminus a}$ $\\Pi \\gets {\\Pi \\setminus a^*}$\nInput: Updated joint policy set $\\Pi$.\nApproximation Approaches # The algorithms below improve scalability to larger problems over optimal methods, but do not possess any bounds on solution quality.\nMBDP # Memory-bounded dynamic programming (MBDP) techniques mitigate the scalability problem of DP (which generates and evaluates all joint policy trees before pruning) by keeping a fixed number of policy trees for each agent at each step (Seuken and Zilberstein 2007b). Several approaches have improved on MBDP by limiting (Seuken and Zilberstein 2007a) or compressing (Carlin and Zilberstein 2008) observations, replacing exhaustive backup with branch-and-bound search in the space of joint policy trees (Dibangoye, Mouaddib, and Chaib-draa 2009) as well as constraint optimization (Kumar and Zilberstein 2010) and linear programming (Wu, Zilberstein, and Chen 2010) to increase the efficiency of selecting the best trees at each step.\nJESP # The joint equilibrium search for policies (JESP) (Nair et al. 2003) uses alternating best response. Initial policies are generated for all agents, and then all but one is held fixed. The remaining agent can then calculate the best response (local optimum) to the fixed policies. The policy of this agent becomes fixed and the next agent calculates the best response. These best-response calculations to fixed other agent policies continue until no agent changes its policy.\nComplexity Classes # Assuming $c$ and $k$ are constants, $\\mathcal{C}$ is a complexity class, the table shows complexity terminologies.\nP the set of problems solvable in polynomial time, e.g., $O(n^k)$ NP the set of problems solvable nondeterministically in polynomial time EXP the set of problems solvable in exponential time, e.g., $O(c^{n^k})$ NEXP the set of problems solvable nondeterministically in exponential time PSPACE the set of problems solvable in polynomial space (P and NP $\\subset$ PSPACE), e.g., $O(c^{n^k})$ $\\mathcal{C}$-hard a problem that all problems in $\\mathcal{C}$ are reducible to within polynomial time $\\mathcal{C}$-complete a problem that is contained in $\\mathcal{C}$ and $\\mathcal{C}$-hard Amato, Christopher, Girish Chowdhary, Alborz Geramifard, N. Kemal Üre, and Mykel J. Kochenderfer. 2013. “Decentralized Control of Partially Observable Markov Decision Processes.” In 52nd IEEE Conference on Decision and Control, 2398–2405. https://doi.org/10.1109/CDC.2013.6760239.\nBecker, Raphen, Shlomo Zilberstein, Victor Lesser, and Claudia V. Goldman. 2004. “Solving Transition Independent Decentralized Markov Decision Processes.” Journal of Artificial Intelligence Research 22: 423–55.\nBernstein, Daniel S., Robert Givan, Neil Immerman, and Shlomo Zilberstein. 2002. “The Complexity of Decentralized Control of Markov Decision Processes.” Mathematics of Operations Research 27 (4): 819–40.\nCarlin, A., and S. Zilberstein. 2008. “Value-Based Observation Compression for DEC-POMDPs.” In Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems.\nChi, Cheng, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. 2024. “Diffusion Policy: Visuomotor Policy Learning via Action Diffusion.” The International Journal of Robotics Research.\nDibangoye, J. S., A.-I. Mouaddib, and B. Chaib-draa. 2009. “Point-Based Incremental Pruning Heuristic for Solving Finite-Horizon DEC-POMDPs.” In Proceedings of the Eighth International Conference on Autonomous Agents and Multiagent Systems.\nHansen, Eric A, Daniel S Bernstein, and Shlomo Zilberstein. 2004. “Dynamic Programming for Partially Observable Stochastic Games.” In AAAI, 4:709–15.\nKumar, A., and S. Zilberstein. 2010. “Point-Based Backup for Decentralized POMDPs: Complexity and New Algorithms.” In Proceedings of the Ninth International Conference on Autonomous Agents and Multiagent Systems, 1315–22.\nMadani, Omid, Steve Hanks, and Anne Condon. 1999. “On the Undecidability of Probabilistic Planning and Infinite-Horizon Partially Observable Markov Decision Problems.” In, 541–48. AAAI ’99/IAAI ’99. Orlando, Florida, USA: American Association for Artificial Intelligence.\nNair, Ranjit, Milind Tambe, Makoto Yokoo, David V. Pynadath, and Stacy Marsella. 2005. “Networked Distributed POMDPs: A Synthesis of Distributed Constraint Optimization and POMDPs.” In Proceedings of the 20th National Conference on Artificial Intelligence (AAAI-05), 133–39. AAAI Press.\nNair, Ranjit, Milind Tambe, Makoto Yokoo, David Pynadath, and Stacy Marsella. 2003. “Taming Decentralized POMDPs: Towards Efficient Policy Computation for Multiagent Settings.” In IJCAI, 3:705–11.\nPapadimitriou, Christos H., and John N. Tsitsiklis. 1987. “The Complexity of Markov Decision Processes.” Mathematics of Operations Research 12 (3): 441–50. https://doi.org/10.1287/moor.12.3.441.\nSeuken, Sven, and Shlomo Zilberstein. 2007a. “Improved Memory-Bounded Dynamic Programming for Decentralized POMDPs,” 344–51.\n———. 2007b. “Memory-Bounded Dynamic Programming for DECPOMDPs.” Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI), 2009–15.\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. 2nd ed. MIT Press.\nSzer, Daniel, François Charpillet, and Shlomo Zilberstein. 2005. “MAA*: A Heuristic Search Algorithm for Solving Decentralized POMDPs.” In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence (UAI), 576–90. AUAI Press.\nWu, F., S. Zilberstein, and X. Chen. 2010. “Point-Based Policy Generation for Decentralized POMDPs.” In Proceedings of the Ninth International Conference on Autonomous Agents and Multiagent Systems, 1307–14.\nThe definition of the value function is pretty flexible, which can be based on various aspects, such as the value of a state, a belief state, an observation, a state history, an observation history, an action (single-step policy), a policy (action history), observation-action history, and their combinations.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPolicy can also be represented by other forms, like approximating functions (Sutton and Barto 2018), neural networks, diffusion models (Chi et al. 2024), etc.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":3,"href":"/notes/docs/rl/marl/optimal-q-value-functions-for-dec-pomdp/","title":"Optimal Q Value Functions for Dec POMDP","section":"MARL","content":" Notions # $s^t$ the state at $t$ with problem horizon $h$ $o^t$ the joint observation of agents $o^t=\\langle o_1^t, \\dots, o_n^t \\rangle$ at $t$ $\\mathcal{O}$ the joint observation space $\\vec{\\theta}^t$ the joint observation-action history until $t$, $\\vec{\\theta}^t=(o^0, a^0, \\dots, o^t)$ $\\vec{\\Theta}^t$ the joint history space at $t$ $\\vec{\\Theta}^t_\\pi$ the set of $\\vec{\\theta}^t$ consistent with policy $\\pi$ $\\delta^{t}$ the decision rule (a temporal structure of policy) at $t$ $\\delta^{t,*}$ the optimal decision rule at $t$ following $\\psi^{t-1, *}$ $\\delta^{t,\\circledast}_\\psi$ the optimal decision rule at $t$ following $\\psi^{t-1}$ $\\Delta^t$ the decision rule space at $t$ $\\psi^{t}$ the past joint policy until $t$, $\\psi^{t} = \\delta^{[0, t)}$ $\\psi^{t, *}$ the optimal past joint policy until $t$, $\\psi^{t, *} = \\delta^{[0, t), *}$ $\\psi^{t, \\circledast}$ the past joint policy until $t$ with non-optimal $\\psi^{t-1}$ and optimal $\\delta^{t-1, \\circledast}_\\psi$ $\\Psi^{t}$ the past joint policy space at $t$ $\\xi^t$ the subsequent joint policy from $t$, $\\xi^{t} = \\delta^{[t, h)}$ $\\xi^{t, *}$ the optimal subsequent joint policy from $t$, $\\xi^{t} = \\delta^{[t, h), *}$ $\\xi^{t, \\circledast}_\\psi$ the optimal subsequent joint policy from $t$ following non-optimal $\\psi^t$ $\\pi$ the joint pure policy $\\pi=\\delta^{[0, h)}$ $\\pi^*$ the joint optimal pure policy $\\pi^*=\\delta^{[0, h), *}$ $R(\\vec{\\theta}^t, \\psi^{t+1})$ the immediate reward function following $\\psi^{t+1}$ $Q(\\vec{\\theta}^t, \\psi^{t+1})$ the history-policy value function following $\\psi^{t+1}$ $Q^*(\\vec{\\theta}^t, \\psi^{t+1})$ the optimal history-policy value function following $\\psi^{t+1}$ $Q^\\circledast(\\vec{\\theta}^t, \\psi^{t+1})$ the sequentially rational optimal history-policy value function following $\\psi^{t+1}$ Normative Optimal Q-Value Function # Definition 1. The optimal Q-value function $Q^$ in Dec-POMDP, the expected cumulative reward over time steps $[t,h)$ induced by optimal joint policy $\\pi^{}$, $\\forall \\vec{\\theta}^t\\in \\vec{\\Theta}^t_{\\psi^{t, }}, \\forall \\psi^{t+1}\\in(\\psi^{t, },\\Delta^t)$, is defined as, $$Q^(\\vec{\\theta}^t, \\psi^{t+1}) = \\left{ \\begin{aligned} \u0026amp;R(\\vec{\\theta}^t, \\psi^{t+1}), \u0026amp;t=h-1 \\ \u0026amp;R(\\vec{\\theta}^t, \\psi^{t+1}) + \\sum_{o^{t+1} \\in \\mathcal{O}} P(o^{t+1}|\\vec{\\theta}^t, \\psi^{t+1}) Q^(\\vec{\\theta}^{t+1}, \\pi^(\\vec{\\theta}^{t+1})). \u0026amp;0\\leqslant t \u0026lt; h-1 \\ \\end{aligned} \\right .\\label{eq:normative-Q}$$*\nHere, $\\pi^*(\\vec{\\theta}^{t+1})\\equiv \\psi^{t+2, *}$ because of the consistent optimality of policy.\nProposition 1. In Dec-POMDP, deriving an optimal policy from the normative optimal history-policy value function defined in Equ. [eq:normative-Q] is impractical (clarifying Sec. 4.3.3, (Oliehoek, Spaan, and Vlassis 2008)).\nProof. We check the optima in 2 steps. The independent and dependent variables are marked in red.\nTo calculate the Pareto optima of Bayesian game at $t$, $$\\textcolor{red}{\\delta^{t, *}} = \\mathop{\\mathrm{argmax}}{\\delta^t}\\sum{\\vec{\\theta}^t \\in \\vec{\\Theta}^t_{\\psi^{t, *}}} P(\\vec{\\theta}^t|\\psi^{t, }) \\textcolor{red}{Q^}(\\vec{\\theta}^t, (\\psi^{t, }, \\delta^t)),$$ note that calculating $\\delta^{t,}$ depends on $\\psi^{t, *} = \\delta^{[0, t), }$ and $Q^(\\vec{\\theta}^t, \\cdot)$.\nAccording to Definition. 1, the optimal Bellman equation can be written as, $$\\textcolor{red}{Q^}(\\vec{\\theta}^t, \\psi^{t+1}) = R(\\vec{\\theta}^t, \\psi^{t+1}) + \\sum_{o^{t+1} \\in \\mathcal{O}} P(o^{t+1}|\\vec{\\theta}^t, \\psi^{t+1}) \\max_{\\delta^{t+1}}Q^(\\vec{\\theta}^{t+1}, (\\textcolor{red}{\\psi^{t+1, }}, \\delta^{t+1})),$$ when $0\\leqslant t \u0026lt; h-1$. This indicates that $Q^(\\vec{\\theta}^t, \\cdot)$ depends on $\\psi^{t+1, }$.1 Consequently, calculating $\\delta^{t,}$ inherently depends on $\\delta^{[0, t], *}$ (includes itself), making it self-dependent and impractical to solve.2 ◻\nSequentially Rational Optimal Q-Value Function # To make optimal Q-value in Dec-POMDP computable, (Oliehoek, Spaan, and Vlassis 2008) defined another form of Q-value function and eliminated the dependency on past optimality.\nDefinition 2. The sequentially rational optimal Q-value function $Q^\\circledast$ in Dec-POMDP, the expected cumulative reward over time steps $[t,h)$ induced by optimal subsequent joint policy $\\xi^{t, \\circledast}\\psi$, $\\forall \\vec{\\theta}^t\\in \\vec{\\Theta}^t{\\Psi^{t}}, \\forall\\psi^{t+1}\\in\\Psi^{t+1}$, is defined as, $$Q^\\circledast(\\vec{\\theta}^t, \\psi^{t+1}) = \\left{ \\begin{aligned} \u0026amp;R(\\vec{\\theta}^t, \\psi^{t+1}), \u0026amp;t=h-1\\ \u0026amp;R(\\vec{\\theta}^t, \\psi^{t+1}) + \\sum_{o^{t+1} \\in \\mathcal{O}} P(o^{t+1}|\\vec{\\theta}^t, \\psi^{t+1}) Q^\\circledast(\\vec{\\theta}^{t+1}, \\psi^{t+2, \\circledast}), \u0026amp;0\\leqslant t \u0026lt; h-1 \\ \\end{aligned} \\right .\\label{eq:SR-Q}$$ where $\\psi^{t+2, \\circledast}=(\\psi^{t+1}, \\delta^{t+1, \\circledast}_{\\psi}), \\forall \\ \\psi^{t+1} \\in \\Psi^{t+1}$.\nNote that the only difference of $Q^\\circledast$ from $Q^$ is $\\psi^{t+2, \\circledast}$, consequently expanding $Q^$’s candidates of history from $\\vec{\\theta}^t \\in \\vec{\\Theta}^t_{\\psi^{t, *}}$ to $\\vec{\\theta}^t \\in \\vec{\\Theta}^t_{\\Psi^{t}}$ and policy from $\\psi^{t+1}\\in(\\psi^{t, *},\\Delta^t)$ to $\\psi^{t+1}\\in(\\Psi^t,\\Delta^t)$.\nBeyond solving the problem of Proposition 1, another advantage of $Q^\\circledast$ is that it allows for the computation of optimal subsequent policy $\\xi^{t, *}_\\psi$ following any past policy $\\psi^{t}$. This is beneficial in online applications where agents may occasionally deviate from the optimal policy.\nOpen Questions # We have seen some advantages of defining the optimal Q-value function as $Q^\\circledast$, what are the downsides to defining it this way (e.g., high computational costs)? Oliehoek, Frans A., Matthijs T. J. Spaan, and Nikos Vlassis. 2008. “Optimal and Approximate Q-Value Functions for Decentralized POMDPs.” Journal of Artificial Intelligence Research 32: 289–353.\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. 2nd ed. MIT Press.\nThe dependency of $P(o^{t+1}|\\vec{\\theta}^t, \\psi^{t+1})$ is not a problem and can be solved just like how the stochasticity $P(s^{t+1}|s^t, a)$ tackled by double learning in Sec. 6.7, (Sutton and Barto 2018).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSingle-agent (PO)MDP, where the belief states are acquirable, does not have such a problem because the Q-value function is not necessarily history-dependent, thanks to Markovian property.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":4,"href":"/notes/docs/large-language-models/reward-modeling-llm/","title":"Reward Modeling LLM","section":"LLM","content":" Reward Models in RLHF # As LLMs scale, their raw outputs (optimized primarily for next-token prediction) often diverge from expected traits. To enable RL fine-tuning from human feedbacks (RLHF), reward models are introduced as trainable proxies for human preference. Once trained, it can generalize preference signals to unseen inputs, making alignment more scalable by reducing reliance on slow and costly human annotations. It also allows flexible fine-tuning toward different objectives, such as helpfulness, truthfulness, or safety.\nA typical alignment pipeline consists of 3 stages: supervised fine-tuning (SFT), reward modeling, and RL. After an initial SFT based on base transformer with curated human-labeled data, a reward model is constructed to predict human preferences over model-generated responses. This model is then used to guide further optimization by encouraging outputs that maximize the predicted reward. For example, ChatGPT employs reward models trained on ranked annotations to guide its generation toward preferred outputs (Achiam, Adler, and Agarwal 2024); DeepSeek and LLaMA 2 include explicit reward modeling components in their alignment pipelines, using pairwise preferences to train reward models that inform subsequent learning (Shao, Wang, and Zhu 2024; Touvron, Martin, and Stone 2023).\nRole of the reward models in RLHF (reward model in blue). Achiam, Josh, Steven Adler, and Sandhini Agarwal. 2024. “GPT-4 Technical Report.” https://arxiv.org/abs/2303.08774.\nShao, Zhihong, Peiyi Wang, and Qihao Zhu. 2024. “DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.” https://arxiv.org/abs/2402.03300.\nTouvron, Hugo, Louis Martin, and Kevin Stone. 2023. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” https://arxiv.org/abs/2307.09288.\n"},{"id":5,"href":"/notes/docs/rl/understanding-bellman/","title":"Understanding Bellman","section":"RL","content":" Bellman Equation and Operator # Definition # The Bellman Equation and optimal Bellman Equation for V-values are, (Sutton and Barto 2018),1 $$\\begin{aligned} V^\\pi(s) \u0026amp;\\doteq \\mathbb{E}{a \\sim \\pi(\\cdot|s)} \\left[ Q^\\pi(s, a) \\right], \\ \u0026amp;= \\mathbb{E}{a \\sim \\pi(\\cdot|s)} \\left[ R(s, a) + \\gamma \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)} \\left[V^\\pi(s\u0026rsquo;)\\right] \\right], \\ V^*(s) \u0026amp;\\doteq \\max{a} \\left[ Q^(s, a) \\right], \\ \u0026amp;= \\max_{a} \\left[ R(s, a) + \\gamma \\mathbb{E}_{s\u0026rsquo; \\sim P(\\cdot|s,a)} \\left[V^(s\u0026rsquo;)\\right] \\right]. \\end{aligned}$$ and the Bellman Equation and optimal Bellman Equation for Q-values are, $$\\begin{aligned} Q^\\pi(s, a) \u0026amp;\\doteq R(s, a) + \\gamma \\mathbb{E}{s\u0026rsquo;\\sim P(\\cdot|s,a)} \\left[V^\\pi(s\u0026rsquo;)\\right], \\ \u0026amp;= R(s, a) + \\gamma \\mathbb{E}{s\u0026rsquo;\\sim P(\\cdot|s,a)} \\left[\\mathbb{E}{a\u0026rsquo;\\sim\\pi(a\u0026rsquo;|s\u0026rsquo;)} Q^\\pi(s\u0026rsquo;, a\u0026rsquo;)\\right]. \\ Q^*(s, a) \u0026amp;\\doteq R(s, a) + \\gamma \\mathbb{E}{s\u0026rsquo;\\sim P(\\cdot|s,a)} \\left[V^(s\u0026rsquo;)\\right], \\ \u0026amp;= R(s, a) + \\gamma \\mathbb{E}{s\u0026rsquo;\\sim P(\\cdot|s,a)} \\left[\\max{a\u0026rsquo;} Q^(s\u0026rsquo;, a\u0026rsquo;)\\right]. \\end{aligned}$$ where $V^\\pi(s)$ and $Q^\\pi(s,a)$ are value representations following policy $\\pi$, e.g., vectors and functions. $$\\tilde{\\pi}(s) \\doteq \\mathop{\\mathrm{argmax}}a Q^\\pi (s,a).$$ Bellman Equations establish relations between states and succeeding states, which can be applied as updating rules for value prediction. A succinct representation is to define the Bellman Equation as a unary mathematical operator. The V-value Bellman and optimal Bellman Operators are, $$\\begin{aligned} (\\mathcal{T}^\\pi\\circ V^\\pi)(s) \u0026amp;\\doteq \\mathbb{E}{a \\sim \\pi(\\cdot|s)} \\left[ R(s, a) + \\gamma \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)} \\left[V^\\pi(s\u0026rsquo;)\\right] \\right], \\ (\\mathcal{T}^*\\circ V^\\pi)(s) \u0026amp;\\doteq \\max_a \\left[ R(s, a) + \\gamma \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)} \\left[V^\\pi(s\u0026rsquo;)\\right] \\right]. \\end{aligned}$$ The Bellman and optimal Bellman Operators $\\mathcal{T}^\\pi$ for Q-values are, $$\\begin{aligned} (\\mathcal{T}^\\pi\\circ Q^\\pi)(s, a) \u0026amp;\\doteq R(s, a) + \\gamma \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)} \\left[ \\mathbb{E}{a\u0026rsquo; \\sim \\pi(a\u0026rsquo;|s\u0026rsquo;)} Q^\\pi(s\u0026rsquo;, a\u0026rsquo;) \\right], \\ (\\mathcal{T}^*\\circ Q^\\pi)(s, a) \u0026amp;\\doteq R(s, a) + \\gamma \\mathbb{E}{s\u0026rsquo; \\sim P(\\cdot|s,a)} \\left[ \\max{a\u0026rsquo;} Q^\\pi(s\u0026rsquo;, a\u0026rsquo;) \\right]. \\end{aligned}$$\nCurse of Dimension # Why do we mostly use MDP (where the future evolution is independent of its history) and hence Bellman Equations to model RL problems? (Bellman 1957) coined the “curse of dimension”, which describes the exponential increase in the state space size as dimensionality grows, making calculations extremely complex. Breaking this curse often requires altering the problem or its constraints, though complete solutions are not always achievable.\nFor convenience, we use Q-value as the representative in the following parts of this article.\nImportant Properties # Proposition 1 ($\\gamma$-contraction). Given any $Q,\\ Q\u0026rsquo; \\mapsto \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}$, Bellman Operators are $\\gamma$-contraction Operators in $L^\\infty$ norm, $$\\begin{aligned} |\\mathcal{T}^\\pi \\circ Q - \\mathcal{T}^\\pi \\circ Q\u0026rsquo;|\\infty \u0026amp;\\leqslant \\gamma |Q-Q\u0026rsquo;|\\infty,\\ \\text{and }|\\mathcal{T}^ \\circ Q - \\mathcal{T}^* \\circ Q\u0026rsquo;|\\infty \u0026amp;\\leqslant \\gamma |Q-Q\u0026rsquo;|\\infty. \\end{aligned}$$*\nCorollary 1 (Fixed-point Iteration). For any $Q^0 \\mapsto \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}$, after $k\\to \\infty$ iterations of Bellman transformation, $Q^{\\pi, \\infty} \\doteq \\lim_{k \\to\\infty} (\\mathcal{T}^\\pi)^k \\circ Q^0$, or $Q^{, \\infty} \\doteq \\lim_{k\\to\\infty} (\\mathcal{T}^)^k \\circ Q^0$, according to Banach’s Fixed Point Theorem, $$\\begin{gathered} Q^{\\pi, \\infty}=Q^{, \\infty}=Q^, \\ \\text{which \\textbf{uniquely} satisfies } \\mathcal{T}^\\pi \\circ Q^ = Q^, \\text{ or } \\mathcal{T}^ \\circ Q^* = Q^. \\end{gathered}$$\nTheorem 1 (Fundamental theorem). Any memoryless policy that is greedy to $Q^$ (deterministically maximizes) is optimal (Szepesvári 2010), $$\\begin{aligned} \\tilde{\\pi}^{} \\doteq \\mathop{\\mathrm{argmax}}_aQ^ = \\pi^. \\end{aligned}$$\nProposition 2 (Monotone). Bellman Operators are monotonic. For any Q-values $Q,Q\u0026rsquo; \\mapsto \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}$, $$\\begin{aligned} \\left (Q\\leqslant Q\u0026rsquo;\\right ) \u0026amp;\\Leftrightarrow \\left (\\mathcal{T}^\\pi \\circ Q\\leqslant \\mathcal{T}^\\pi \\circ Q\u0026rsquo;\\right ),\\ \\left (Q\\leqslant Q\u0026rsquo;\\right )\u0026amp;\\Leftrightarrow \\left (\\mathcal{T}^ \\circ Q\\leqslant \\mathcal{T}^* \\circ Q\u0026rsquo;\\right ). \\end{aligned}$$*\nBellman Backup for Planning # Dynamic Programming # According to the Fundamental Theorem, we can find $\\pi^$ efficiently once having access to $Q^$, without the need to find the policy whose Q-function dominates the others’ brute-force-ly. To avoid the Bellman Curse of Dimensionality, we can apply Dynamic Programming (DP) methods to solve MDPs by keeping track of Q-values during calculations, thanks to Bellman recursions.\nValue Iteration # Value iteration (so-called backward induction) involves iteratively applying $\\mathcal{T}^$ to arbitrarily initialized values $Q^0$ until convergence. According to Corollary 1 and Theorem 1, value iteration converges to $Q^$ as $k \\to \\infty$, then an optimal policy $\\pi^$ can be derived by greedifying $Q^$.\nPolicy Iteration # Policy iteration starts with an arbitrary policy $\\pi^0$ and values $Q^0$. In each iterative step $k$, $Q^{\\pi, k}$ is calculated by applying Bellman Operator $\\mathcal{T}^{\\pi, k}$ that follows current policy ${\\pi^k}$ to $Q^{\\pi, {k-1}}$ from the last iteration, and then $\\pi^{k+1}$ is derived from greedifying $Q^{\\pi, k}$. This process is repeated until convergence, and policy iteration can produce optimal policy after sufficient iterations.\nBellman Residual for Learning # TD-Learning with Look-up Table # When the transition model is unavailable (model-free), we can use the residuals (RHS minus LHS) of the Bellman Equations as learning objective, $$\\begin{aligned} (\\mathcal{B}^\\pi\\circ Q) (s,a) \u0026amp;\\doteq r + \\gamma Q(s\u0026rsquo;, \\pi(s\u0026rsquo;)) - Q(s, a),\\ (\\mathcal{B}^*\\circ Q) (s,a) \u0026amp;\\doteq r + \\gamma \\max_{a\u0026rsquo;} Q(s\u0026rsquo;, a\u0026rsquo;) - Q(s, a). \\end{aligned}$$ Assuming that our sampling and parameter updating roughly follow the true state distribution $\\mu(s)$, the expectation of Bellman residual will be closed to zero at the optima. This approach is often called temporal difference (TD) learning.\nTD-learning # In TD-learning with learning rate $\\alpha$, the update rule for Q-values is, $$Q(s, a) \\leftarrow Q(s, a) + \\alpha (\\mathcal{B}^\\pi\\circ Q) (s,a). \\label{eq:td-learning}$$ According to Stochastic Approximation Theorem, let $k$ be the visitation times of state-action pair, and learning rates $0 \\leqslant \\alpha^k \u0026lt; 1$ satisfies $\\forall (s, a)$, $\\sum_{k=1}^\\infty \\alpha^k(s, a) = \\infty,\\sum_{k=1}^\\infty [\\alpha^k(s, a)]^2 \u0026lt; \\infty$. Following TD-learning updates, $Q^{\\pi, k}(s, a)$ converges to $Q^*(s, a)$ as $k \\to \\infty$ ((Jaakkola, Jordan, and Singh 1994)).\nQ-learning # In Q-learning that relies on optimal Bellman Equation, the Q-value update is, $$Q(s, a) \\leftarrow Q(s, a) + \\alpha (\\mathcal{B}^\\circ Q) (s,a). \\label{eq:q-learning}$$ According to Stochastic Approximation Theorem, let $k$ be the visitation times of state-action pair, and learning rates $0 \\leqslant \\alpha^k \u0026lt; 1$ satisfies $\\forall (s, a)$, $\\sum_{k=1}^\\infty \\alpha^k(s, a) = \\infty, \\sum_{k=1}^\\infty [\\alpha^k(s, a)]^2 \u0026lt; \\infty$. Following Q-learning updates, $Q^{, k}(s, a)$ converges to $Q^*(s, a)$ as $k \\to \\infty$ ((Watkins and Dayan 1992)). The deep version of Q-learning algorithm, Deep Q-Network (DQN), is shown in Appendix.\nHowever, the nice property of convergence only holds in the tabular case and cannot be extended to a function approximation as discussed later.\nTD-learning with Function Approximation # To introduce generalization to the value function, we represent the approximated Q-value in a parameterized functional form. Our goal is to minimize the mean squared value error, $$\\mathcal{L}(\\theta) = \\frac{1}{2}\\sum_{s \\in \\mathcal{S}} \\mu(s) \\Big[ Q^\\text{target} - Q_\\theta(s, a) \\Big]^2,$$ where $Q^\\text{target}$ is the ground truth and $Q_\\theta$ is the prediction. Just like TD-learning, the Bellman residual can be applied for the value function approximation.\nSemi gradient for Bellman Residual # Similar to stochastic gradient methods with unbiased target estimators, if we use the Bellman Equation to get target Q-value $Q^\\text{target}$, but here we just ignore its potential gradient change, the gradient ascent for Bellman residual is, $$\\begin{aligned} \\Delta_\\text{semi} \\theta \u0026amp;= -\\frac{1}{2}\\alpha \\nabla_\\theta \\Big[Q^\\text{target} - Q_\\theta(s, a) \\Big]^2 \\ \u0026amp;= \\alpha \\Big[Q^\\text{target} - Q_\\theta(s, a) \\Big] \\nabla_\\theta Q_\\theta(s, a), \\text{ where } Q^\\text{target} = r + \\gamma Q_{\\textcolor{red}{\\theta}}(s\u0026rsquo;, a\u0026rsquo;)\\label{eq:semi-grad} \\end{aligned}$$ Since we neglects a part of the gradient of $Q^\\text{target}$, it is called semi gradient for Bellman residual ($\\theta$ in red). Though semi-gradient methods are fast and simple, they could have divergence issue, e.g., Baird’s counter-example (the star problem).\nFull Gradient for Bellman Residual # The full Bellman residual gradient should include all gradient components, including the gradient of the target estimation, $$\\begin{aligned} \\Delta_\\text{full} \\theta \u0026amp;= -\\frac{1}{2}\\alpha \\nabla_\\theta \\Big[ r + \\gamma Q_\\theta(s\u0026rsquo;, a\u0026rsquo;) - Q_\\theta(s, a) \\Big]^2 \\ \u0026amp; = -\\alpha \\Big[ r + \\gamma Q_\\theta(s\u0026rsquo;, a\u0026rsquo;) - Q_\\theta(s, a) \\Big] \\Big[ \\gamma\\nabla_\\theta Q_\\theta(s\u0026rsquo;, a\u0026rsquo;) - \\nabla_\\theta Q_\\theta(s, a) \\Big]. \\end{aligned}$$ If the approximation system is general enough and the value functions are continuous, the full Bellman residual gradient is guaranteed to converge to the optima. However, this is at the sacrifice of learning speed, as illustrated by the hall problem.\nHybrid Gradient for Bellman Residual # In contrast to Figure 1 where $\\Delta_\\text{semi}$ boosts $\\Delta_\\text{full}$, Figure 3 represents the case where the semi gradient may diverge. (Baird 1995) combined these 2 methods: to keep stable, $\\Delta_\\text{B}$ should stay in the same direction as $\\Delta_\\text{full}$ (above the perpendicular axis); meanwhile, $\\Delta_\\text{B}$ should stay as close as possible to $\\Delta_\\text{semi}$ to increase learning speed. $$\\begin{aligned} \\Delta_\\text{B} \\theta \u0026amp;= (1 - \\omega) \\cdot \\Delta_\\text{semi}\\theta + \\omega \\cdot \\Delta_\\text{full}\\theta, \\ \u0026amp;=-\\alpha \\Big[ r + \\gamma Q_\\theta(s\u0026rsquo;, a\u0026rsquo;) - Q_\\theta(s, a) \\Big] \\Big[\\omega \\gamma \\nabla_\\theta Q_\\theta(s\u0026rsquo;, a\u0026rsquo;) - \\nabla_\\theta Q_\\theta(s, a) \\Big],\\ \u0026amp;\\text{s.t.,} \\ \\Delta_\\text{B}\\theta \\cdot \\Delta_\\text{full}\\theta\\geqslant 0 \\Leftrightarrow \\omega \\geqslant \\frac{\\Delta_\\text{semi}\\theta \\cdot \\Delta_\\text{full}\\theta}{\\Delta_\\text{semi}\\theta \\cdot \\Delta_\\text{full}\\theta - \\Delta_\\text{full}\\theta \\cdot \\Delta_\\text{full}\\theta}. \\end{aligned}$$\nGradient ascent by semi-gradient. Gradient descent by semi-gradient. Epoch-wise gradient vectors for Bellman residual gradients. Baird, Leemon C. 1995. “Residual Algorithms: Reinforcement Learning with Function Approximation.” In Machine Learning Proceedings 1995, 30–37. Elsevier.\nBellman, Richard. 1957. Dynamic Programming. Princeton, NJ: Princeton University Press.\nJaakkola, Thomas, Michael I. Jordan, and Satinder P. Singh. 1994. “On the Convergence of Stochastic Iterative Dynamic Programming Algorithms.” Neural Computation 6 (6): 1185–1201. https://doi.org/10.1162/neco.1994.6.6.1185.\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. 2nd ed. MIT Press.\nSzepesvári, Csaba. 2010. Algorithms for Reinforcement Learning. Vol. 4. Synthesis Lectures on Artificial Intelligence and Machine Learning 1. Morgan \u0026amp; Claypool Publishers. https://doi.org/10.2200/S00268ED1V01Y201005AIM009.\nWatkins, Christopher J. C. H., and Peter Dayan. 1992. “Q-Learning.” Machine Learning 8 (3–4): 279–92. https://doi.org/10.1007/BF00992698.\nThe Bellman Equations shown in the main article are stochastic, the deterministic one can be derived as, $V(s) = \\max_{a} \\left{ R(s, a) + \\gamma V(s\u0026rsquo;)\\right}$, where $s\u0026rsquo;\\gets T(s,a)$ is a deterministic succeeding state of $s$ following $a$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"}]