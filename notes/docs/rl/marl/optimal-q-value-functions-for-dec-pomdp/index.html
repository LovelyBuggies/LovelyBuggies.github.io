<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="


  



  Notions
  
  #
  


  
      
          
          
      
  
  
      
          $s^t$
          the state at $t$ with problem horizon $h$
      
      
          $o^t$
          the joint observation of agents $o^t=\langle o_1^t, \dots, o_n^t \rangle$ at $t$
      
      
          $\mathcal{O}$
          the joint observation space
      
      
          $\vec{\theta}^t$
          the joint observation-action history until $t$, $\vec{\theta}^t=(o^0, a^0, \dots, o^t)$
      
      
          $\vec{\Theta}^t$
          the joint history space at $t$
      
      
          $\vec{\Theta}^t_\pi$
          the set of $\vec{\theta}^t$ consistent with policy $\pi$
      
      
          
          
      
      
          $\delta^{t}$
          the decision rule (a temporal structure of policy) at $t$
      
      
          $\delta^{t,*}$
          the optimal decision rule at $t$ following $\psi^{t-1, *}$
      
      
          $\delta^{t,\circledast}_\psi$
          the optimal decision rule at $t$ following $\psi^{t-1}$
      
      
          $\Delta^t$
          the decision rule space at $t$
      
      
          $\psi^{t}$
          the past joint policy until $t$, $\psi^{t} = \delta^{[0, t)}$
      
      
          $\psi^{t, *}$
          the optimal past joint policy until $t$, $\psi^{t, *} = \delta^{[0, t), *}$
      
      
          $\psi^{t, \circledast}$
          the past joint policy until $t$ with non-optimal $\psi^{t-1}$ and optimal $\delta^{t-1, \circledast}_\psi$
      
      
          $\Psi^{t}$
          the past joint policy space at $t$
      
      
          $\xi^t$
          the subsequent joint policy from $t$, $\xi^{t} = \delta^{[t, h)}$
      
      
          $\xi^{t, *}$
          the optimal subsequent joint policy from $t$, $\xi^{t} = \delta^{[t, h), *}$
      
      
          $\xi^{t, \circledast}_\psi$
          the optimal subsequent joint policy from $t$ following non-optimal $\psi^t$
      
      
          $\pi$
          the joint pure policy $\pi=\delta^{[0, h)}$
      
      
          $\pi^*$
          the joint optimal pure policy $\pi^*=\delta^{[0, h), *}$
      
      
          
          
      
      
          $R(\vec{\theta}^t, \psi^{t&#43;1})$
          the immediate reward function following $\psi^{t&#43;1}$
      
      
          $Q(\vec{\theta}^t, \psi^{t&#43;1})$
          the history-policy value function following $\psi^{t&#43;1}$
      
      
          $Q^*(\vec{\theta}^t, \psi^{t&#43;1})$
          the optimal history-policy value function following $\psi^{t&#43;1}$
      
      
          $Q^\circledast(\vec{\theta}^t, \psi^{t&#43;1})$
          the sequentially rational optimal history-policy value function following $\psi^{t&#43;1}$
      
  


  Normative Optimal Q-Value Function
  
  #
  


Definition 1. The optimal Q-value function $Q^$ in Dec-POMDP, the expected cumulative reward over time steps $[t,h)$ induced by optimal joint policy $\pi^{}$, $\forall \vec{\theta}^t\in \vec{\Theta}^t_{\psi^{t, }}, \forall \psi^{t&#43;1}\in(\psi^{t, },\Delta^t)$, is defined as, $$Q^(\vec{\theta}^t, \psi^{t&#43;1}) = \left{
\begin{aligned}
&amp;R(\vec{\theta}^t, \psi^{t&#43;1}), &amp;t=h-1 \
&amp;R(\vec{\theta}^t, \psi^{t&#43;1}) &#43; \sum_{o^{t&#43;1} \in \mathcal{O}} P(o^{t&#43;1}|\vec{\theta}^t, \psi^{t&#43;1}) Q^(\vec{\theta}^{t&#43;1}, \pi^(\vec{\theta}^{t&#43;1})). &amp;0\leqslant t &lt; h-1 \
\end{aligned}
\right .\label{eq:normative-Q}$$*">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="https://lovelybuggies.github.io/notes/docs/rl/marl/optimal-q-value-functions-for-dec-pomdp/">
  <meta property="og:site_name" content="LovelyBuggies&#39; Blog">
  <meta property="og:title" content="Optimal Q Value Functions for Dec POMDP">
  <meta property="og:description" content="Notions # $s^t$ the state at $t$ with problem horizon $h$ $o^t$ the joint observation of agents $o^t=\langle o_1^t, \dots, o_n^t \rangle$ at $t$ $\mathcal{O}$ the joint observation space $\vec{\theta}^t$ the joint observation-action history until $t$, $\vec{\theta}^t=(o^0, a^0, \dots, o^t)$ $\vec{\Theta}^t$ the joint history space at $t$ $\vec{\Theta}^t_\pi$ the set of $\vec{\theta}^t$ consistent with policy $\pi$ $\delta^{t}$ the decision rule (a temporal structure of policy) at $t$ $\delta^{t,*}$ the optimal decision rule at $t$ following $\psi^{t-1, *}$ $\delta^{t,\circledast}_\psi$ the optimal decision rule at $t$ following $\psi^{t-1}$ $\Delta^t$ the decision rule space at $t$ $\psi^{t}$ the past joint policy until $t$, $\psi^{t} = \delta^{[0, t)}$ $\psi^{t, *}$ the optimal past joint policy until $t$, $\psi^{t, *} = \delta^{[0, t), *}$ $\psi^{t, \circledast}$ the past joint policy until $t$ with non-optimal $\psi^{t-1}$ and optimal $\delta^{t-1, \circledast}_\psi$ $\Psi^{t}$ the past joint policy space at $t$ $\xi^t$ the subsequent joint policy from $t$, $\xi^{t} = \delta^{[t, h)}$ $\xi^{t, *}$ the optimal subsequent joint policy from $t$, $\xi^{t} = \delta^{[t, h), *}$ $\xi^{t, \circledast}_\psi$ the optimal subsequent joint policy from $t$ following non-optimal $\psi^t$ $\pi$ the joint pure policy $\pi=\delta^{[0, h)}$ $\pi^*$ the joint optimal pure policy $\pi^*=\delta^{[0, h), *}$ $R(\vec{\theta}^t, \psi^{t&#43;1})$ the immediate reward function following $\psi^{t&#43;1}$ $Q(\vec{\theta}^t, \psi^{t&#43;1})$ the history-policy value function following $\psi^{t&#43;1}$ $Q^*(\vec{\theta}^t, \psi^{t&#43;1})$ the optimal history-policy value function following $\psi^{t&#43;1}$ $Q^\circledast(\vec{\theta}^t, \psi^{t&#43;1})$ the sequentially rational optimal history-policy value function following $\psi^{t&#43;1}$ Normative Optimal Q-Value Function # Definition 1. The optimal Q-value function $Q^$ in Dec-POMDP, the expected cumulative reward over time steps $[t,h)$ induced by optimal joint policy $\pi^{}$, $\forall \vec{\theta}^t\in \vec{\Theta}^t_{\psi^{t, }}, \forall \psi^{t&#43;1}\in(\psi^{t, },\Delta^t)$, is defined as, $$Q^(\vec{\theta}^t, \psi^{t&#43;1}) = \left{ \begin{aligned} &amp;R(\vec{\theta}^t, \psi^{t&#43;1}), &amp;t=h-1 \ &amp;R(\vec{\theta}^t, \psi^{t&#43;1}) &#43; \sum_{o^{t&#43;1} \in \mathcal{O}} P(o^{t&#43;1}|\vec{\theta}^t, \psi^{t&#43;1}) Q^(\vec{\theta}^{t&#43;1}, \pi^(\vec{\theta}^{t&#43;1})). &amp;0\leqslant t &lt; h-1 \ \end{aligned} \right .\label{eq:normative-Q}$$*">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="docs">
<title>Optimal Q Value Functions for Dec POMDP | LovelyBuggies&#39; Blog</title>
<link rel="icon" href="/notes/favicon.png" >
<link rel="manifest" href="/notes/manifest.json">
<link rel="canonical" href="https://lovelybuggies.github.io/notes/docs/rl/marl/optimal-q-value-functions-for-dec-pomdp/">
<link rel="stylesheet" href="/notes/book.min.a482db4ff40e4539a82f3a9bfb0e3c5f7db0799cb9fde583a4a16768fd8e5c80.css" integrity="sha256-pILbT/QORTmoLzqb&#43;w48X32weZy5/eWDpKFnaP2OXIA=" crossorigin="anonymous"><!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr" class="book-kind-page book-type-docs book-layout-">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    
<aside class="book-menu">
  <div class="book-menu-content">
    
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>LovelyBuggies&#39; Blog</span>
  </a>
</h2>
















  
  <ul>
    
      
        <li>
          
  
  

  
    <span>RL</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/notes/docs/rl/from-pg-2-ppo/" class="">From PG 2 PPO</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/notes/docs/rl/understanding-bellman/" class="">Understanding Bellman</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <span>MARL</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/notes/docs/rl/marl/introduction-2-decpomdp/" class="">Introduction 2 DecPOMDP</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/notes/docs/rl/marl/optimal-q-value-functions-for-dec-pomdp/" class="active">Optimal Q Value Functions for Dec POMDP</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <span>LLM</span>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/notes/docs/large-language-models/reward-modeling-llm/" class="">Reward Modeling LLM</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/notes/docs/welcome/" class="">Welcome</a>
  

        </li>
      
    
  </ul>














</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>



  </div>
</aside>
 

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/notes/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>Optimal Q Value Functions for Dec POMDP</h3>

  <label for="toc-control">
    
    <img src="/notes/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents"></nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article">
<link rel="stylesheet" href="/notes/katex/katex.min.css" />
<script defer src="/notes/katex/katex.min.js"></script>

  <script defer src="/notes/katex/auto-render.min.js" onload="renderMathInElement(document.body, {
  &#34;delimiters&#34;: [
    {&#34;left&#34;: &#34;$$&#34;, &#34;right&#34;: &#34;$$&#34;, &#34;display&#34;: true},
    {&#34;left&#34;: &#34;$&#34;, &#34;right&#34;: &#34;$&#34;, &#34;display&#34;: false},
    {&#34;left&#34;: &#34;\\(&#34;, &#34;right&#34;: &#34;\\)&#34;, &#34;display&#34;: false},
    {&#34;left&#34;: &#34;\\[&#34;, &#34;right&#34;: &#34;\\]&#34;, &#34;display&#34;: true}
  ],
  &#34;throwOnError&#34;: false,
  &#34;strict&#34;: false,
  &#34;trust&#34;: true,
  &#34;macros&#34;: {
    &#34;\\label&#34;: &#34;\\htmlId{#1}{}&#34;,
    &#34;\\eqref&#34;: &#34;(\\ref{#1})&#34;
  }
}
);"></script>


<h1 id="notions">
  Notions
  
  <a class="anchor" href="#notions">#</a>
  
</h1>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: left"></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">$s^t$</td>
          <td style="text-align: left">the state at $t$ with problem horizon $h$</td>
      </tr>
      <tr>
          <td style="text-align: left">$o^t$</td>
          <td style="text-align: left">the joint observation of agents $o^t=\langle o_1^t, \dots, o_n^t \rangle$ at $t$</td>
      </tr>
      <tr>
          <td style="text-align: left">$\mathcal{O}$</td>
          <td style="text-align: left">the joint observation space</td>
      </tr>
      <tr>
          <td style="text-align: left">$\vec{\theta}^t$</td>
          <td style="text-align: left">the joint observation-action history until $t$, $\vec{\theta}^t=(o^0, a^0, \dots, o^t)$</td>
      </tr>
      <tr>
          <td style="text-align: left">$\vec{\Theta}^t$</td>
          <td style="text-align: left">the joint history space at $t$</td>
      </tr>
      <tr>
          <td style="text-align: left">$\vec{\Theta}^t_\pi$</td>
          <td style="text-align: left">the set of $\vec{\theta}^t$ consistent with policy $\pi$</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left">$\delta^{t}$</td>
          <td style="text-align: left">the decision rule (a temporal structure of policy) at $t$</td>
      </tr>
      <tr>
          <td style="text-align: left">$\delta^{t,*}$</td>
          <td style="text-align: left">the optimal decision rule at $t$ following $\psi^{t-1, *}$</td>
      </tr>
      <tr>
          <td style="text-align: left">$\delta^{t,\circledast}_\psi$</td>
          <td style="text-align: left">the optimal decision rule at $t$ following $\psi^{t-1}$</td>
      </tr>
      <tr>
          <td style="text-align: left">$\Delta^t$</td>
          <td style="text-align: left">the decision rule space at $t$</td>
      </tr>
      <tr>
          <td style="text-align: left">$\psi^{t}$</td>
          <td style="text-align: left">the past joint policy until $t$, $\psi^{t} = \delta^{[0, t)}$</td>
      </tr>
      <tr>
          <td style="text-align: left">$\psi^{t, *}$</td>
          <td style="text-align: left">the optimal past joint policy until $t$, $\psi^{t, *} = \delta^{[0, t), *}$</td>
      </tr>
      <tr>
          <td style="text-align: left">$\psi^{t, \circledast}$</td>
          <td style="text-align: left">the past joint policy until $t$ with non-optimal $\psi^{t-1}$ and optimal $\delta^{t-1, \circledast}_\psi$</td>
      </tr>
      <tr>
          <td style="text-align: left">$\Psi^{t}$</td>
          <td style="text-align: left">the past joint policy space at $t$</td>
      </tr>
      <tr>
          <td style="text-align: left">$\xi^t$</td>
          <td style="text-align: left">the subsequent joint policy from $t$, $\xi^{t} = \delta^{[t, h)}$</td>
      </tr>
      <tr>
          <td style="text-align: left">$\xi^{t, *}$</td>
          <td style="text-align: left">the optimal subsequent joint policy from $t$, $\xi^{t} = \delta^{[t, h), *}$</td>
      </tr>
      <tr>
          <td style="text-align: left">$\xi^{t, \circledast}_\psi$</td>
          <td style="text-align: left">the optimal subsequent joint policy from $t$ following non-optimal $\psi^t$</td>
      </tr>
      <tr>
          <td style="text-align: left">$\pi$</td>
          <td style="text-align: left">the joint pure policy $\pi=\delta^{[0, h)}$</td>
      </tr>
      <tr>
          <td style="text-align: left">$\pi^*$</td>
          <td style="text-align: left">the joint optimal pure policy $\pi^*=\delta^{[0, h), *}$</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left">$R(\vec{\theta}^t, \psi^{t+1})$</td>
          <td style="text-align: left">the immediate reward function following $\psi^{t+1}$</td>
      </tr>
      <tr>
          <td style="text-align: left">$Q(\vec{\theta}^t, \psi^{t+1})$</td>
          <td style="text-align: left">the history-policy value function following $\psi^{t+1}$</td>
      </tr>
      <tr>
          <td style="text-align: left">$Q^*(\vec{\theta}^t, \psi^{t+1})$</td>
          <td style="text-align: left">the optimal history-policy value function following $\psi^{t+1}$</td>
      </tr>
      <tr>
          <td style="text-align: left">$Q^\circledast(\vec{\theta}^t, \psi^{t+1})$</td>
          <td style="text-align: left">the sequentially rational optimal history-policy value function following $\psi^{t+1}$</td>
      </tr>
  </tbody>
</table>
<h1 id="normative-optimal-q-value-function">
  Normative Optimal Q-Value Function
  
  <a class="anchor" href="#normative-optimal-q-value-function">#</a>
  
</h1>
<div id="defn:normative-Q" class="definition">
<p><strong>Definition 1</strong>. <em>The optimal Q-value function $Q^</em>$ in Dec-POMDP, the expected cumulative reward over time steps $[t,h)$ induced by optimal joint policy $\pi^{<em>}$, $\forall \vec{\theta}^t\in \vec{\Theta}^t_{\psi^{t, <em>}}, \forall \psi^{t+1}\in(\psi^{t, <em>},\Delta^t)$, is defined as, $$Q^</em>(\vec{\theta}^t, \psi^{t+1}) = \left{
\begin{aligned}
&amp;R(\vec{\theta}^t, \psi^{t+1}), &amp;t=h-1 \
&amp;R(\vec{\theta}^t, \psi^{t+1}) + \sum_{o^{t+1} \in \mathcal{O}} P(o^{t+1}|\vec{\theta}^t, \psi^{t+1}) Q^</em>(\vec{\theta}^{t+1}, \pi^</em>(\vec{\theta}^{t+1})). &amp;0\leqslant t &lt; h-1 \
\end{aligned}
\right .\label{eq:normative-Q}$$*</p>
</div>
<p>Here, $\pi^*(\vec{\theta}^{t+1})\equiv \psi^{t+2, *}$ because of the consistent optimality of policy.</p>
<div id="prop:problem" class="proposition">
<p><strong>Proposition 1</strong>. <em>In Dec-POMDP, deriving an optimal policy from the normative optimal history-policy value function defined in Equ. <a href="#eq:normative-Q" data-reference-type="ref" data-reference="eq:normative-Q">[eq:normative-Q]</a> is impractical (clarifying Sec. 4.3.3, (Oliehoek, Spaan, and Vlassis 2008)).</em></p>
</div>
<div class="proof">
<p><em>Proof.</em> We check the optima in 2 steps. The independent and dependent variables are marked in red.</p>
<p>To calculate the Pareto optima of Bayesian game at $t$, $$\textcolor{red}{\delta^{t, *}}
= \mathop{\mathrm{argmax}}<em>{\delta^t}\sum</em>{\vec{\theta}^t \in \vec{\Theta}^t_{\psi^{t, *}}} P(\vec{\theta}^t|\psi^{t, <em>}) \textcolor{red}{Q^</em>}(\vec{\theta}^t, (\psi^{t, <em>}, \delta^t)),$$ note that calculating $\delta^{t,</em>}$ depends on $\psi^{t, *} = \delta^{[0, t), <em>}$ and $Q^</em>(\vec{\theta}^t, \cdot)$.</p>
<p>According to Definition. <a href="#defn:normative-Q" data-reference-type="ref" data-reference="defn:normative-Q">1</a>, the optimal Bellman equation can be written as, $$\textcolor{red}{Q^<em>}(\vec{\theta}^t, \psi^{t+1}) = R(\vec{\theta}^t, \psi^{t+1}) + \sum_{o^{t+1} \in \mathcal{O}} P(o^{t+1}|\vec{\theta}^t, \psi^{t+1}) \max_{\delta^{t+1}}Q^</em>(\vec{\theta}^{t+1}, (\textcolor{red}{\psi^{t+1, <em>}}, \delta^{t+1})),$$ when $0\leqslant t &lt; h-1$. This indicates that $Q^</em>(\vec{\theta}^t, \cdot)$ depends on $\psi^{t+1, <em>}$. Consequently, calculating $\delta^{t,</em>}$ inherently depends on $\delta^{[0, t], *}$ (includes itself), making it self-dependent and impractical to solve. ◻</p>
<blockquote class="book-hint info">
  
Note: The dependency of $P(o^{t+1}\mid\vec{\theta}^t, \psi^{t+1})$ is not problematic and can be handled analogously to how the stochasticity $P(s^{t+1}\mid s^t, a)$ is treated via double learning (Sutton and Barto 2018, Sec. 6.7).

</blockquote>

<blockquote class="book-hint info">
  
Single-agent (PO)MDP, where belief states are available, does not have this issue because the Q-value need not be history-dependent (Markov property).

</blockquote>

</div>
<h1 id="sequentially-rational-optimal-q-value-function">
  Sequentially Rational Optimal Q-Value Function
  
  <a class="anchor" href="#sequentially-rational-optimal-q-value-function">#</a>
  
</h1>
<p>To make optimal Q-value in Dec-POMDP computable, (Oliehoek, Spaan, and Vlassis 2008) defined another form of Q-value function and eliminated the dependency on past optimality.</p>
<div class="definition">
<p><strong>Definition 2</strong>. <em>The sequentially rational optimal Q-value function $Q^\circledast$ in Dec-POMDP, the expected cumulative reward over time steps $[t,h)$ induced by optimal subsequent joint policy $\xi^{t, \circledast}<em>\psi$, $\forall \vec{\theta}^t\in \vec{\Theta}^t</em>{\Psi^{t}}, \forall\psi^{t+1}\in\Psi^{t+1}$, is defined as, $$Q^\circledast(\vec{\theta}^t, \psi^{t+1}) = \left{
\begin{aligned}
&amp;R(\vec{\theta}^t, \psi^{t+1}), &amp;t=h-1\
&amp;R(\vec{\theta}^t, \psi^{t+1}) + \sum_{o^{t+1} \in \mathcal{O}} P(o^{t+1}|\vec{\theta}^t, \psi^{t+1}) Q^\circledast(\vec{\theta}^{t+1}, \psi^{t+2, \circledast}), &amp;0\leqslant t &lt; h-1 \
\end{aligned}
\right .\label{eq:SR-Q}$$ where $\psi^{t+2, \circledast}=(\psi^{t+1}, \delta^{t+1, \circledast}_{\psi}), \forall \ \psi^{t+1} \in \Psi^{t+1}$.</em></p>
</div>
<p>Note that the only difference of $Q^\circledast$ from $Q^<em>$ is $\psi^{t+2, \circledast}$, consequently expanding $Q^</em>$’s candidates of history from $\vec{\theta}^t \in \vec{\Theta}^t_{\psi^{t, *}}$ to $\vec{\theta}^t \in \vec{\Theta}^t_{\Psi^{t}}$ and policy from $\psi^{t+1}\in(\psi^{t, *},\Delta^t)$ to $\psi^{t+1}\in(\Psi^t,\Delta^t)$.</p>
<p>Beyond solving the problem of Proposition <a href="#prop:problem" data-reference-type="ref" data-reference="prop:problem">1</a>, another advantage of $Q^\circledast$ is that it allows for the computation of optimal subsequent policy $\xi^{t, *}_\psi$ following any past policy $\psi^{t}$. This is beneficial in online applications where agents may occasionally deviate from the optimal policy.</p>
<h1 id="open-questions">
  Open Questions
  
  <a class="anchor" href="#open-questions">#</a>
  
</h1>
<ul>
<li>We have seen some advantages of defining the optimal Q-value function as $Q^\circledast$, what are the downsides to defining it this way (e.g., high computational costs)?</li>
</ul>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Oliehoek08JAIR" class="csl-entry">
<p>Oliehoek, Frans A., Matthijs T. J. Spaan, and Nikos Vlassis. 2008. “Optimal and Approximate Q-Value Functions for Decentralized POMDPs.” <em>Journal of Artificial Intelligence Research</em> 32: 289–353.</p>
</div>
<div id="ref-sutton2018reinforcement" class="csl-entry">
<p>Sutton, Richard S., and Andrew G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. 2nd ed. MIT Press.</p>
</div>
</div>
<!-- footnotes converted to hints above -->
</article>
 
      


      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>





  
  
  
  <div class="flex flex-wrap justify-between">
    <span>
    
      <a href="/notes/docs/rl/marl/introduction-2-decpomdp/" class="flex align-center">
        <img src="/notes/svg/backward.svg" class="book-icon" alt="Previous" title="Introduction 2 DecPOMDP" />
        <span>Introduction 2 DecPOMDP</span>
      </a>
    
    </span>
    <span>
    
      <a href="/notes/docs/large-language-models/reward-modeling-llm/" class="flex align-center">
        <span>Reward Modeling LLM</span>
        <img src="/notes/svg/forward.svg" class="book-icon" alt="Next" title="Reward Modeling LLM" />
      </a>
    
    </span>
  </div>
  




  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 
      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    

<aside class="book-toc">
  <div class="book-toc-content">
    
  
<nav id="TableOfContents"></nav>



  </div>
</aside>

 
  </main>

  
</body>
</html>
















