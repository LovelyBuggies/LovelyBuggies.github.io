<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MARL on LovelyBuggies&#39; Blog</title>
    <link>https://lovelybuggies.github.io/notes/docs/rl/marl/</link>
    <description>Recent content in MARL on LovelyBuggies&#39; Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://lovelybuggies.github.io/notes/docs/rl/marl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Introduction 2 DecPOMDP</title>
      <link>https://lovelybuggies.github.io/notes/docs/rl/marl/introduction-2-decpomdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lovelybuggies.github.io/notes/docs/rl/marl/introduction-2-decpomdp/</guid>
      <description>&lt;h1 id=&#34;dec-pomdp&#34;&gt;&#xA;  Dec-POMDP&#xA;  &#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#dec-pomdp&#34;&gt;#&lt;/a&gt;&#xA;  &#xA;&lt;/h1&gt;&#xA;&lt;div class=&#34;definition&#34;&gt;&#xA;&lt;p&gt;&lt;strong&gt;Definition 1&lt;/strong&gt;. &lt;em&gt;A Dec-POMDP is a tuple $\langle \mathbb{I}, \mathcal{S}, {\mathbb{A}_i}, T, R, {\mathbb{O}_i}, O, \mathcal{H}, \gamma\rangle$:&lt;/em&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;em&gt;$\mathbb{I}$ is a finite sets of agents, $|\mathbb{I}|=n$;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;em&gt;$\mathcal{S}$ is a set of states with designated initial state distribution $b^0$;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;em&gt;$\mathbb{A}_i$ is a set of actions for agent $i$ with $\mathbb{A}\doteq \times_i \mathbb{A}_i$ the set of joint actions;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;em&gt;$T$ is the state transition probability function, $T$: $\mathcal{S} \times \mathbb{A} \times \mathcal{S} \rightarrow [0, 1]$, that specifies the probability of transitioning from state $s \in \mathcal{S}$ to $s&#39; \in \mathcal{S}$ when the actions $\boldsymbol{a} \in \mathbb{A}$ are taken by agents (i.e., $T(s, \boldsymbol{a}, s&#39;)=P(s&#39;|s, \textbf{a})$);&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Optimal Q Value Functions for Dec POMDP</title>
      <link>https://lovelybuggies.github.io/notes/docs/rl/marl/optimal-q-value-functions-for-dec-pomdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://lovelybuggies.github.io/notes/docs/rl/marl/optimal-q-value-functions-for-dec-pomdp/</guid>
      <description>&lt;h1 id=&#34;notions&#34;&gt;&#xA;  Notions&#xA;  &#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#notions&#34;&gt;#&lt;/a&gt;&#xA;  &#xA;&lt;/h1&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$s^t$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the state at $t$ with problem horizon $h$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$o^t$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the joint observation of agents $o^t=\langle o_1^t, \dots, o_n^t \rangle$ at $t$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\mathcal{O}$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the joint observation space&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\vec{\theta}^t$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the joint observation-action history until $t$, $\vec{\theta}^t=(o^0, a^0, \dots, o^t)$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\vec{\Theta}^t$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the joint history space at $t$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\vec{\Theta}^t_\pi$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the set of $\vec{\theta}^t$ consistent with policy $\pi$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\delta^{t}$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the decision rule (a temporal structure of policy) at $t$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\delta^{t,*}$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the optimal decision rule at $t$ following $\psi^{t-1, *}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\delta^{t,\circledast}_\psi$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the optimal decision rule at $t$ following $\psi^{t-1}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\Delta^t$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the decision rule space at $t$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\psi^{t}$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the past joint policy until $t$, $\psi^{t} = \delta^{[0, t)}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\psi^{t, *}$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the optimal past joint policy until $t$, $\psi^{t, *} = \delta^{[0, t), *}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\psi^{t, \circledast}$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the past joint policy until $t$ with non-optimal $\psi^{t-1}$ and optimal $\delta^{t-1, \circledast}_\psi$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\Psi^{t}$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the past joint policy space at $t$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\xi^t$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the subsequent joint policy from $t$, $\xi^{t} = \delta^{[t, h)}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\xi^{t, *}$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the optimal subsequent joint policy from $t$, $\xi^{t} = \delta^{[t, h), *}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\xi^{t, \circledast}_\psi$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the optimal subsequent joint policy from $t$ following non-optimal $\psi^t$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\pi$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the joint pure policy $\pi=\delta^{[0, h)}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$\pi^*$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the joint optimal pure policy $\pi^*=\delta^{[0, h), *}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$R(\vec{\theta}^t, \psi^{t+1})$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the immediate reward function following $\psi^{t+1}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$Q(\vec{\theta}^t, \psi^{t+1})$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the history-policy value function following $\psi^{t+1}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$Q^*(\vec{\theta}^t, \psi^{t+1})$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the optimal history-policy value function following $\psi^{t+1}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;$Q^\circledast(\vec{\theta}^t, \psi^{t+1})$&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;the sequentially rational optimal history-policy value function following $\psi^{t+1}$&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h1 id=&#34;normative-optimal-q-value-function&#34;&gt;&#xA;  Normative Optimal Q-Value Function&#xA;  &#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#normative-optimal-q-value-function&#34;&gt;#&lt;/a&gt;&#xA;  &#xA;&lt;/h1&gt;&#xA;&lt;div id=&#34;defn:normative-Q&#34; class=&#34;definition&#34;&gt;&#xA;&lt;p&gt;&lt;strong&gt;Definition 1&lt;/strong&gt;. &lt;em&gt;The optimal Q-value function $Q^&lt;/em&gt;$ in Dec-POMDP, the expected cumulative reward over time steps $[t,h)$ induced by optimal joint policy $\pi^{&lt;em&gt;}$, $\forall \vec{\theta}^t\in \vec{\Theta}^t_{\psi^{t, &lt;em&gt;}}, \forall \psi^{t+1}\in(\psi^{t, &lt;em&gt;},\Delta^t)$, is defined as, $$Q^&lt;/em&gt;(\vec{\theta}^t, \psi^{t+1}) = \left{&#xA;\begin{aligned}&#xA;&amp;amp;R(\vec{\theta}^t, \psi^{t+1}), &amp;amp;t=h-1 \&#xA;&amp;amp;R(\vec{\theta}^t, \psi^{t+1}) + \sum_{o^{t+1} \in \mathcal{O}} P(o^{t+1}|\vec{\theta}^t, \psi^{t+1}) Q^&lt;/em&gt;(\vec{\theta}^{t+1}, \pi^&lt;/em&gt;(\vec{\theta}^{t+1})). &amp;amp;0\leqslant t &amp;lt; h-1 \&#xA;\end{aligned}&#xA;\right .\label{eq:normative-Q}$$*&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
