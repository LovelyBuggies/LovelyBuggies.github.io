<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="


  



  Bellman Equation and Operator
  
  #
  


  Definition
  
  #
  

The Bellman Equation and optimal Bellman Equation for V-values are, (Sutton and Barto 2018),1 $$\begin{aligned}
V^\pi(s) &\doteq \mathbb{E}{a \sim \pi(\cdot|s)} \left[ Q^\pi(s, a) \right], \
&= \mathbb{E}{a \sim \pi(\cdot|s)} \left[ R(s, a) + \gamma \mathbb{E}{s&rsquo; \sim P(\cdot|s,a)} \left[V^\pi(s&rsquo;)\right] \right],  \
V^*(s) &\doteq \max{a} \left[ Q^(s, a) \right], \
&= \max_{a} \left[ R(s, a) + \gamma \mathbb{E}_{s&rsquo; \sim P(\cdot|s,a)} \left[V^(s&rsquo;)\right] \right].
\end{aligned}$$ and the Bellman Equation and optimal Bellman Equation for Q-values are, $$\begin{aligned}
Q^\pi(s, a) &\doteq R(s, a) + \gamma \mathbb{E}{s&rsquo;\sim P(\cdot|s,a)} \left[V^\pi(s&rsquo;)\right], \
&= R(s, a) + \gamma \mathbb{E}{s&rsquo;\sim P(\cdot|s,a)} \left[\mathbb{E}{a&rsquo;\sim\pi(a&rsquo;|s&rsquo;)} Q^\pi(s&rsquo;, a&rsquo;)\right]. \
Q^*(s, a) &\doteq R(s, a) + \gamma \mathbb{E}{s&rsquo;\sim P(\cdot|s,a)} \left[V^(s&rsquo;)\right], \
&= R(s, a) + \gamma \mathbb{E}{s&rsquo;\sim P(\cdot|s,a)} \left[\max{a&rsquo;} Q^(s&rsquo;, a&rsquo;)\right].
\end{aligned}$$ where $V^\pi(s)$ and $Q^\pi(s,a)$ are value representations following policy $\pi$, e.g., vectors and functions. $$\tilde{\pi}(s) \doteq \mathop{\mathrm{argmax}}a Q^\pi (s,a).$$ Bellman Equations establish relations between states and succeeding states, which can be applied as updating rules for value prediction. A succinct representation is to define the Bellman Equation as a unary mathematical operator. The V-value Bellman and optimal Bellman Operators are, $$\begin{aligned}
(\mathcal{T}^\pi\circ V^\pi)(s) &\doteq \mathbb{E}{a \sim \pi(\cdot|s)} \left[ R(s, a) + \gamma \mathbb{E}{s&rsquo; \sim P(\cdot|s,a)} \left[V^\pi(s&rsquo;)\right] \right], \
(\mathcal{T}^*\circ V^\pi)(s) &\doteq \max_a \left[ R(s, a) + \gamma \mathbb{E}{s&rsquo; \sim P(\cdot|s,a)} \left[V^\pi(s&rsquo;)\right] \right].
\end{aligned}$$ The Bellman and optimal Bellman Operators $\mathcal{T}^\pi$ for Q-values are, $$\begin{aligned}
(\mathcal{T}^\pi\circ Q^\pi)(s, a) &\doteq R(s, a) + \gamma \mathbb{E}{s&rsquo; \sim P(\cdot|s,a)} \left[ \mathbb{E}{a&rsquo; \sim \pi(a&rsquo;|s&rsquo;)} Q^\pi(s&rsquo;, a&rsquo;) \right],  \
(\mathcal{T}^*\circ Q^\pi)(s, a) &\doteq R(s, a) + \gamma \mathbb{E}{s&rsquo; \sim P(\cdot|s,a)} \left[ \max{a&rsquo;} Q^\pi(s&rsquo;, a&rsquo;) \right].
\end{aligned}$$"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://lovelybuggies.github.io/notes/docs/rl/understanding-bellman/"><meta property="og:site_name" content="LovelyBuggies' Blog"><meta property="og:title" content="Understanding Bellman"><meta property="og:description" content="Bellman Equation and Operator # Definition # The Bellman Equation and optimal Bellman Equation for V-values are, (Sutton and Barto 2018),1 $$\begin{aligned} V^\pi(s) &\doteq \mathbb{E}{a \sim \pi(\cdot|s)} \left[ Q^\pi(s, a) \right], \ &= \mathbb{E}{a \sim \pi(\cdot|s)} \left[ R(s, a) + \gamma \mathbb{E}{s’ \sim P(\cdot|s,a)} \left[V^\pi(s’)\right] \right], \ V^*(s) &\doteq \max{a} \left[ Q^(s, a) \right], \ &= \max_{a} \left[ R(s, a) + \gamma \mathbb{E}_{s’ \sim P(\cdot|s,a)} \left[V^(s’)\right] \right]. \end{aligned}$$ and the Bellman Equation and optimal Bellman Equation for Q-values are, $$\begin{aligned} Q^\pi(s, a) &\doteq R(s, a) + \gamma \mathbb{E}{s’\sim P(\cdot|s,a)} \left[V^\pi(s’)\right], \ &= R(s, a) + \gamma \mathbb{E}{s’\sim P(\cdot|s,a)} \left[\mathbb{E}{a’\sim\pi(a’|s’)} Q^\pi(s’, a’)\right]. \ Q^*(s, a) &\doteq R(s, a) + \gamma \mathbb{E}{s’\sim P(\cdot|s,a)} \left[V^(s’)\right], \ &= R(s, a) + \gamma \mathbb{E}{s’\sim P(\cdot|s,a)} \left[\max{a’} Q^(s’, a’)\right]. \end{aligned}$$ where $V^\pi(s)$ and $Q^\pi(s,a)$ are value representations following policy $\pi$, e.g., vectors and functions. $$\tilde{\pi}(s) \doteq \mathop{\mathrm{argmax}}a Q^\pi (s,a).$$ Bellman Equations establish relations between states and succeeding states, which can be applied as updating rules for value prediction. A succinct representation is to define the Bellman Equation as a unary mathematical operator. The V-value Bellman and optimal Bellman Operators are, $$\begin{aligned} (\mathcal{T}^\pi\circ V^\pi)(s) &\doteq \mathbb{E}{a \sim \pi(\cdot|s)} \left[ R(s, a) + \gamma \mathbb{E}{s’ \sim P(\cdot|s,a)} \left[V^\pi(s’)\right] \right], \ (\mathcal{T}^*\circ V^\pi)(s) &\doteq \max_a \left[ R(s, a) + \gamma \mathbb{E}{s’ \sim P(\cdot|s,a)} \left[V^\pi(s’)\right] \right]. \end{aligned}$$ The Bellman and optimal Bellman Operators $\mathcal{T}^\pi$ for Q-values are, $$\begin{aligned} (\mathcal{T}^\pi\circ Q^\pi)(s, a) &\doteq R(s, a) + \gamma \mathbb{E}{s’ \sim P(\cdot|s,a)} \left[ \mathbb{E}{a’ \sim \pi(a’|s’)} Q^\pi(s’, a’) \right], \ (\mathcal{T}^*\circ Q^\pi)(s, a) &\doteq R(s, a) + \gamma \mathbb{E}{s’ \sim P(\cdot|s,a)} \left[ \max{a’} Q^\pi(s’, a’) \right]. \end{aligned}$$"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2025-09-15T20:10:56-04:00"><title>Understanding Bellman | LovelyBuggies' Blog</title><link rel=icon href=/notes/favicon.png><link rel=manifest href=/notes/manifest.json><link rel=canonical href=https://lovelybuggies.github.io/notes/docs/rl/understanding-bellman/><link rel=stylesheet href=/notes/book.min.cfa0a6bf023bcd59c1c86d9df090c59d41586b44bea15a26034358dd3eeb470c.css integrity="sha256-z6CmvwI7zVnByG2d8JDFnUFYa0S+oVomA0NY3T7rRww=" crossorigin=anonymous><script defer src=/notes/fuse.min.js></script><script defer src=/notes/en.search.min.f979a5010a39898721aba51ea49653d0f4da4d10e3c16e18625c7db0ac318d28.js integrity="sha256-+XmlAQo5iYchq6UepJZT0PTaTRDjwW4YYlx9sKwxjSg=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-docs book-layout-"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LovelyBuggies' Blog</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><span>RL</span><ul><li><a href=/notes/docs/rl/from-pg-2-ppo/>From PG 2 PPO</a></li><li><a href=/notes/docs/rl/understanding-bellman/ class=active>Understanding Bellman</a></li><li><span>MARL</span><ul><li><a href=/notes/docs/rl/marl/introduction-2-decpomdp/>Introduction 2 DecPOMDP</a></li><li><a href=/notes/docs/rl/marl/optimal-q-value-functions-for-dec-pomdp/>Optimal Q Value Functions for Dec POMDP</a></li></ul></li></ul></li><li><span>LLM</span><ul><li><a href=/notes/docs/large-language-models/reward-modeling-llm/>Reward Modeling LLM</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/notes/svg/menu.svg class=book-icon alt=Menu></label><h3>Understanding Bellman</h3><label for=toc-control><img src=/notes/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#definition>Definition</a><ul><li></li></ul></li><li><a href=#important-properties>Important Properties</a></li></ul><ul><li><a href=#dynamic-programming>Dynamic Programming</a><ul><li></li></ul></li></ul><ul><li><a href=#td-learning-with-look-up-table>TD-Learning with Look-up Table</a><ul><li></li></ul></li><li><a href=#td-learning-with-function-approximation>TD-learning with Function Approximation</a><ul><li></li></ul></li></ul></nav></aside></header><article class="markdown book-article"><link rel=stylesheet href=/notes/katex/katex.min.css><script defer src=/notes/katex/katex.min.js></script><script defer src=/notes/katex/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><h1 id=bellman-equation-and-operator>Bellman Equation and Operator
<a class=anchor href=#bellman-equation-and-operator>#</a></h1><h2 id=definition>Definition
<a class=anchor href=#definition>#</a></h2><p>The Bellman Equation and optimal Bellman Equation for V-values are, (Sutton and Barto 2018),<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> $$\begin{aligned}
V^\pi(s) &\doteq \mathbb{E}<em>{a \sim \pi(\cdot|s)} \left[ Q^\pi(s, a) \right], \
&= \mathbb{E}</em>{a \sim \pi(\cdot|s)} \left[ R(s, a) + \gamma \mathbb{E}<em>{s&rsquo; \sim P(\cdot|s,a)} \left[V^\pi(s&rsquo;)\right] \right], \
V^*(s) &\doteq \max</em>{a} \left[ Q^<em>(s, a) \right], \
&= \max_{a} \left[ R(s, a) + \gamma \mathbb{E}_{s&rsquo; \sim P(\cdot|s,a)} \left[V^</em>(s&rsquo;)\right] \right].
\end{aligned}$$ and the Bellman Equation and optimal Bellman Equation for Q-values are, $$\begin{aligned}
Q^\pi(s, a) &\doteq R(s, a) + \gamma \mathbb{E}<em>{s&rsquo;\sim P(\cdot|s,a)} \left[V^\pi(s&rsquo;)\right], \
&= R(s, a) + \gamma \mathbb{E}</em>{s&rsquo;\sim P(\cdot|s,a)} \left[\mathbb{E}<em>{a&rsquo;\sim\pi(a&rsquo;|s&rsquo;)} Q^\pi(s&rsquo;, a&rsquo;)\right]. \
Q^*(s, a) &\doteq R(s, a) + \gamma \mathbb{E}</em>{s&rsquo;\sim P(\cdot|s,a)} \left[V^<em>(s&rsquo;)\right], \
&= R(s, a) + \gamma \mathbb{E}<em>{s&rsquo;\sim P(\cdot|s,a)} \left[\max</em>{a&rsquo;} Q^</em>(s&rsquo;, a&rsquo;)\right].
\end{aligned}$$ where $V^\pi(s)$ and $Q^\pi(s,a)$ are value representations following policy $\pi$, e.g., vectors and functions. $$\tilde{\pi}(s) \doteq \mathop{\mathrm{argmax}}<em>a Q^\pi (s,a).$$ Bellman Equations establish relations between states and succeeding states, which can be applied as updating rules for value prediction. A succinct representation is to define the Bellman Equation as a unary mathematical operator. The V-value Bellman and optimal Bellman Operators are, $$\begin{aligned}
(\mathcal{T}^\pi\circ V^\pi)(s) &\doteq \mathbb{E}</em>{a \sim \pi(\cdot|s)} \left[ R(s, a) + \gamma \mathbb{E}<em>{s&rsquo; \sim P(\cdot|s,a)} \left[V^\pi(s&rsquo;)\right] \right], \
(\mathcal{T}^*\circ V^\pi)(s) &\doteq \max_a \left[ R(s, a) + \gamma \mathbb{E}</em>{s&rsquo; \sim P(\cdot|s,a)} \left[V^\pi(s&rsquo;)\right] \right].
\end{aligned}$$ The Bellman and optimal Bellman Operators $\mathcal{T}^\pi$ for Q-values are, $$\begin{aligned}
(\mathcal{T}^\pi\circ Q^\pi)(s, a) &\doteq R(s, a) + \gamma \mathbb{E}<em>{s&rsquo; \sim P(\cdot|s,a)} \left[ \mathbb{E}</em>{a&rsquo; \sim \pi(a&rsquo;|s&rsquo;)} Q^\pi(s&rsquo;, a&rsquo;) \right], \
(\mathcal{T}^*\circ Q^\pi)(s, a) &\doteq R(s, a) + \gamma \mathbb{E}<em>{s&rsquo; \sim P(\cdot|s,a)} \left[ \max</em>{a&rsquo;} Q^\pi(s&rsquo;, a&rsquo;) \right].
\end{aligned}$$</p><h4 id=curse-of-dimension>Curse of Dimension
<a class=anchor href=#curse-of-dimension>#</a></h4><p>Why do we mostly use MDP (where the future evolution is independent of its history) and hence Bellman Equations to model RL problems? (Bellman 1957) coined the “curse of dimension”, which describes the exponential increase in the state space size as dimensionality grows, making calculations extremely complex. Breaking this curse often requires altering the problem or its constraints, though complete solutions are not always achievable.</p><p><em>For convenience, we use Q-value as the representative in the following parts of this article.</em></p><h2 id=important-properties>Important Properties
<a class=anchor href=#important-properties>#</a></h2><div class=proposition><p><strong>Proposition 1</strong> ($\gamma$-contraction). <em>Given any $Q,\ Q&rsquo; \mapsto \mathbb{R}^{|\mathcal{S}| \times |\mathcal{A}|}$, Bellman Operators are $\gamma$-contraction Operators in $L^\infty$ norm, $$\begin{aligned}
|\mathcal{T}^\pi \circ Q - \mathcal{T}^\pi \circ Q&rsquo;|<em>\infty &\leqslant \gamma |Q-Q&rsquo;|</em>\infty,\
\text{and }|\mathcal{T}^</em> \circ Q - \mathcal{T}^* \circ Q&rsquo;|<em>\infty &\leqslant \gamma |Q-Q&rsquo;|</em>\infty.
\end{aligned}$$*</p></div><div id=them:fixpoint class=corollary><p><strong>Corollary 1</strong> (Fixed-point Iteration). <em>For any $Q^0 \mapsto \mathbb{R}^{|\mathcal{S}| \times |\mathcal{A}|}$, after $k\to \infty$ iterations of Bellman transformation, $Q^{\pi, \infty} \doteq \lim_{k \to\infty} (\mathcal{T}^\pi)^k \circ Q^0$, or $Q^{</em>, \infty} \doteq \lim_{k\to\infty} (\mathcal{T}^<em>)^k \circ Q^0$, according to Banach’s Fixed Point Theorem, $$\begin{gathered}
Q^{\pi, \infty}=Q^{</em>, \infty}=Q^<em>, \ \text{which \textbf{uniquely} satisfies } \mathcal{T}^\pi \circ Q^</em> = Q^<em>, \text{ or } \mathcal{T}^</em> \circ Q^* = Q^<em>.
\end{gathered}$$</em></p></div><div id=them:fundamental class=theorem><p><strong>Theorem 1</strong> (Fundamental theorem). <em>Any memoryless policy that is greedy to $Q^</em>$ (<strong>deterministically</strong> maximizes) is optimal (Szepesvári 2010), $$\begin{aligned}
\tilde{\pi}^{<em>} \doteq \mathop{\mathrm{argmax}}_aQ^</em> = \pi^<em>.
\end{aligned}$$</em></p></div><div class=proposition><p><strong>Proposition 2</strong> (Monotone). <em>Bellman Operators are monotonic. For any Q-values $Q,Q&rsquo; \mapsto \mathbb{R}^{|\mathcal{S}| \times |\mathcal{A}|}$, $$\begin{aligned}
\left (Q\leqslant Q&rsquo;\right ) &\Leftrightarrow \left (\mathcal{T}^\pi \circ Q\leqslant \mathcal{T}^\pi \circ Q&rsquo;\right ),\
\left (Q\leqslant Q&rsquo;\right )&\Leftrightarrow \left (\mathcal{T}^</em> \circ Q\leqslant \mathcal{T}^* \circ Q&rsquo;\right ).
\end{aligned}$$*</p></div><h1 id=bellman-backup-for-planning>Bellman Backup for Planning
<a class=anchor href=#bellman-backup-for-planning>#</a></h1><h2 id=dynamic-programming>Dynamic Programming
<a class=anchor href=#dynamic-programming>#</a></h2><p>According to the Fundamental Theorem, we can find $\pi^<em>$ efficiently once having access to $Q^</em>$, without the need to find the policy whose Q-function <strong>dominates</strong> the others’ brute-force-ly. To avoid the Bellman Curse of Dimensionality, we can apply Dynamic Programming (DP) methods to solve MDPs by keeping track of Q-values during calculations, thanks to Bellman recursions.</p><h4 id=value-iteration>Value Iteration
<a class=anchor href=#value-iteration>#</a></h4><p>Value iteration (so-called backward induction) involves iteratively applying $\mathcal{T}^<em>$ to arbitrarily initialized values $Q^0$ until convergence. According to Corollary <a href=#them:fixpoint data-reference-type=ref data-reference=them:fixpoint>1</a> and Theorem <a href=#them:fundamental data-reference-type=ref data-reference=them:fundamental>1</a>, value iteration converges to $Q^</em>$ as $k \to \infty$, then an optimal policy $\pi^<em>$ can be derived by greedifying $Q^</em>$.</p><h4 id=policy-iteration>Policy Iteration
<a class=anchor href=#policy-iteration>#</a></h4><p>Policy iteration starts with an arbitrary policy $\pi^0$ and values $Q^0$. In each iterative step $k$, $Q^{\pi, k}$ is calculated by applying Bellman Operator $\mathcal{T}^{\pi, k}$ that follows current policy ${\pi^k}$ to $Q^{\pi, {k-1}}$ from the last iteration, and then $\pi^{k+1}$ is derived from greedifying $Q^{\pi, k}$. This process is repeated until convergence, and policy iteration can produce optimal policy after sufficient iterations.</p><h1 id=bellman-residual-for-learning>Bellman Residual for Learning
<a class=anchor href=#bellman-residual-for-learning>#</a></h1><h2 id=td-learning-with-look-up-table>TD-Learning with Look-up Table
<a class=anchor href=#td-learning-with-look-up-table>#</a></h2><p>When the transition model is unavailable (model-free), we can use the residuals (RHS minus LHS) of the Bellman Equations as learning objective, $$\begin{aligned}
(\mathcal{B}^\pi\circ Q) (s,a) &\doteq r + \gamma Q(s&rsquo;, \pi(s&rsquo;)) - Q(s, a),\
(\mathcal{B}^*\circ Q) (s,a) &\doteq r + \gamma \max_{a&rsquo;} Q(s&rsquo;, a&rsquo;) - Q(s, a).
\end{aligned}$$ Assuming that our sampling and parameter updating roughly follow the true state distribution $\mu(s)$, the expectation of Bellman residual will be closed to zero at the optima. This approach is often called temporal difference (TD) learning.</p><h4 id=td-learning>TD-learning
<a class=anchor href=#td-learning>#</a></h4><p>In TD-learning with learning rate $\alpha$, the update rule for Q-values is, $$Q(s, a) \leftarrow Q(s, a) + \alpha (\mathcal{B}^\pi\circ Q) (s,a). \label{eq:td-learning}$$ According to Stochastic Approximation Theorem, let $k$ be the visitation times of state-action pair, and learning rates $0 \leqslant \alpha^k &lt; 1$ satisfies $\forall (s, a)$, $\sum_{k=1}^\infty \alpha^k(s, a) = \infty,\sum_{k=1}^\infty [\alpha^k(s, a)]^2 &lt; \infty$. Following TD-learning updates, $Q^{\pi, k}(s, a)$ converges to $Q^*(s, a)$ as $k \to \infty$ ((Jaakkola, Jordan, and Singh 1994)).</p><h4 id=q-learning>Q-learning
<a class=anchor href=#q-learning>#</a></h4><p>In Q-learning that relies on optimal Bellman Equation, the Q-value update is, $$Q(s, a) \leftarrow Q(s, a) + \alpha (\mathcal{B}^<em>\circ Q) (s,a). \label{eq:q-learning}$$ According to Stochastic Approximation Theorem, let $k$ be the visitation times of state-action pair, and learning rates $0 \leqslant \alpha^k &lt; 1$ satisfies $\forall (s, a)$, $\sum_{k=1}^\infty \alpha^k(s, a) = \infty, \sum_{k=1}^\infty [\alpha^k(s, a)]^2 &lt; \infty$. Following Q-learning updates, $Q^{</em>, k}(s, a)$ converges to $Q^*(s, a)$ as $k \to \infty$ ((Watkins and Dayan 1992)). The deep version of Q-learning algorithm, Deep Q-Network (DQN), is shown in Appendix.</p><p>However, the nice property of convergence only holds in the tabular case and cannot be extended to a function approximation as discussed later.</p><h2 id=td-learning-with-function-approximation>TD-learning with Function Approximation
<a class=anchor href=#td-learning-with-function-approximation>#</a></h2><p>To introduce generalization to the value function, we represent the approximated Q-value in a parameterized functional form. Our goal is to minimize the mean squared value error, $$\mathcal{L}(\theta) = \frac{1}{2}\sum_{s \in \mathcal{S}} \mu(s) \Big[ Q^\text{target} - Q_\theta(s, a) \Big]^2,$$ where $Q^\text{target}$ is the ground truth and $Q_\theta$ is the prediction. Just like TD-learning, the Bellman residual can be applied for the value function approximation.</p><h4 id=semi-gradient-for-bellman-residual>Semi gradient for Bellman Residual
<a class=anchor href=#semi-gradient-for-bellman-residual>#</a></h4><p>Similar to stochastic gradient methods with unbiased target estimators, if we use the Bellman Equation to get target Q-value $Q^\text{target}$, but here we just ignore its potential gradient change, the gradient ascent for Bellman residual is, $$\begin{aligned}
\Delta_\text{semi} \theta &= -\frac{1}{2}\alpha \nabla_\theta \Big[Q^\text{target} - Q_\theta(s, a) \Big]^2 \
&= \alpha \Big[Q^\text{target} - Q_\theta(s, a) \Big] \nabla_\theta Q_\theta(s, a), \text{ where } Q^\text{target} = r + \gamma Q_{\textcolor{red}{\theta}}(s&rsquo;, a&rsquo;)\label{eq:semi-grad}
\end{aligned}$$ Since we neglects a part of the gradient of $Q^\text{target}$, it is called semi gradient for Bellman residual ($\theta$ in red). Though semi-gradient methods are fast and simple, they could have divergence issue, e.g., Baird’s counter-example (the star problem).</p><h4 id=full-gradient-for-bellman-residual>Full Gradient for Bellman Residual
<a class=anchor href=#full-gradient-for-bellman-residual>#</a></h4><p>The full Bellman residual gradient should include all gradient components, including the gradient of the target estimation, $$\begin{aligned}
\Delta_\text{full} \theta &= -\frac{1}{2}\alpha \nabla_\theta \Big[ r + \gamma Q_\theta(s&rsquo;, a&rsquo;) - Q_\theta(s, a) \Big]^2 \
& = -\alpha \Big[ r + \gamma Q_\theta(s&rsquo;, a&rsquo;) - Q_\theta(s, a) \Big] \Big[ \gamma\nabla_\theta Q_\theta(s&rsquo;, a&rsquo;) - \nabla_\theta Q_\theta(s, a) \Big].
\end{aligned}$$ If the approximation system is general enough and the value functions are continuous, the full Bellman residual gradient is guaranteed to converge to the optima. However, this is at the sacrifice of learning speed, as illustrated by the hall problem.</p><h4 id=hybrid-gradient-for-bellman-residual>Hybrid Gradient for Bellman Residual
<a class=anchor href=#hybrid-gradient-for-bellman-residual>#</a></h4><p>In contrast to Figure <a href=#subfig:sg-increase data-reference-type=ref data-reference=subfig:sg-increase>1</a> where $\Delta_\text{semi}$ boosts $\Delta_\text{full}$, Figure <a href=#subfig:sg-decrease data-reference-type=ref data-reference=subfig:sg-decrease>3</a> represents the case where the semi gradient may diverge. (Baird 1995) combined these 2 methods: to keep stable, $\Delta_\text{B}$ should stay in the same direction as $\Delta_\text{full}$ (above the perpendicular axis); meanwhile, $\Delta_\text{B}$ should stay as close as possible to $\Delta_\text{semi}$ to increase learning speed. $$\begin{aligned}
\Delta_\text{B} \theta &= (1 - \omega) \cdot \Delta_\text{semi}\theta + \omega \cdot \Delta_\text{full}\theta, \
&=-\alpha \Big[ r + \gamma Q_\theta(s&rsquo;, a&rsquo;) - Q_\theta(s, a) \Big] \Big[\omega \gamma \nabla_\theta Q_\theta(s&rsquo;, a&rsquo;) - \nabla_\theta Q_\theta(s, a) \Big],\
&\text{s.t.,} \ \Delta_\text{B}\theta \cdot \Delta_\text{full}\theta\geqslant 0 \Leftrightarrow \omega \geqslant \frac{\Delta_\text{semi}\theta \cdot \Delta_\text{full}\theta}{\Delta_\text{semi}\theta \cdot \Delta_\text{full}\theta - \Delta_\text{full}\theta \cdot \Delta_\text{full}\theta}.
\end{aligned}$$</p><figure id=subfig:sg-decrease><figure id=subfig:sg-increase><figcaption>Gradient ascent by semi-gradient.</figcaption></figure><figure id=subfig:sg-decrease><figcaption>Gradient descent by semi-gradient.</figcaption></figure><figcaption>Epoch-wise gradient vectors for Bellman residual gradients.</figcaption></figure><div id=refs class="references csl-bib-body hanging-indent"><div id=ref-baird1995residual class=csl-entry><p>Baird, Leemon C. 1995. “Residual Algorithms: Reinforcement Learning with Function Approximation.” In <em>Machine Learning Proceedings 1995</em>, 30–37. Elsevier.</p></div><div id=ref-bellman1957dynamic class=csl-entry><p>Bellman, Richard. 1957. <em>Dynamic Programming</em>. Princeton, NJ: Princeton University Press.</p></div><div id=ref-jaakkola1994convergence class=csl-entry><p>Jaakkola, Thomas, Michael I. Jordan, and Satinder P. Singh. 1994. “On the Convergence of Stochastic Iterative Dynamic Programming Algorithms.” <em>Neural Computation</em> 6 (6): 1185–1201. <a href=https://doi.org/10.1162/neco.1994.6.6.1185>https://doi.org/10.1162/neco.1994.6.6.1185</a>.</p></div><div id=ref-sutton2018reinforcement class=csl-entry><p>Sutton, Richard S., and Andrew G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. 2nd ed. MIT Press.</p></div><div id=ref-szepesvari2010algorithms class=csl-entry><p>Szepesvári, Csaba. 2010. <em>Algorithms for Reinforcement Learning</em>. Vol. 4. Synthesis Lectures on Artificial Intelligence and Machine Learning 1. Morgan & Claypool Publishers. <a href=https://doi.org/10.2200/S00268ED1V01Y201005AIM009>https://doi.org/10.2200/S00268ED1V01Y201005AIM009</a>.</p></div><div id=ref-watkins1992qlearning class=csl-entry><p>Watkins, Christopher J. C. H., and Peter Dayan. 1992. “Q-Learning.” <em>Machine Learning</em> 8 (3–4): 279–92. <a href=https://doi.org/10.1007/BF00992698>https://doi.org/10.1007/BF00992698</a>.</p></div></div><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>The Bellman Equations shown in the main article are stochastic, the deterministic one can be derived as, $V(s) = \max_{a} \left{ R(s, a) + \gamma V(s&rsquo;)\right}$, where $s&rsquo;\gets T(s,a)$ is a deterministic succeeding state of $s$ following $a$.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><div class="flex flex-wrap justify-between"><span><a href=/notes/docs/rl/from-pg-2-ppo/ class="flex align-center"><img src=/notes/svg/backward.svg class=book-icon alt=Previous title="From PG 2 PPO">
<span>From PG 2 PPO</span>
</a></span><span><a href=/notes/docs/rl/marl/introduction-2-decpomdp/ class="flex align-center"><span>Introduction 2 DecPOMDP</span>
<img src=/notes/svg/forward.svg class=book-icon alt=Next title="Introduction 2 DecPOMDP"></a></span></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#definition>Definition</a><ul><li></li></ul></li><li><a href=#important-properties>Important Properties</a></li></ul><ul><li><a href=#dynamic-programming>Dynamic Programming</a><ul><li></li></ul></li></ul><ul><li><a href=#td-learning-with-look-up-table>TD-Learning with Look-up Table</a><ul><li></li></ul></li><li><a href=#td-learning-with-function-approximation>TD-learning with Function Approximation</a><ul><li></li></ul></li></ul></nav></div></aside></main></body></html>