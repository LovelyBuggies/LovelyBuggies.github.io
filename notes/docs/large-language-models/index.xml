<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLM on LovelyBuggies' Blog</title><link>https://lovelybuggies.github.io/notes/docs/large-language-models/</link><description>Recent content in LLM on LovelyBuggies' Blog</description><generator>Hugo</generator><language>en-us</language><atom:link href="https://lovelybuggies.github.io/notes/docs/large-language-models/index.xml" rel="self" type="application/rss+xml"/><item><title>Reward Modeling LLM</title><link>https://lovelybuggies.github.io/notes/docs/large-language-models/reward-modeling-llm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lovelybuggies.github.io/notes/docs/large-language-models/reward-modeling-llm/</guid><description>&lt;link rel="stylesheet" href="https://lovelybuggies.github.io/notes/katex/katex.min.css" /&gt;
&lt;script defer src="https://lovelybuggies.github.io/notes/katex/katex.min.js"&gt;&lt;/script&gt;

 &lt;script defer src="https://lovelybuggies.github.io/notes/katex/auto-render.min.js" onload="renderMathInElement(document.body, {
 &amp;#34;delimiters&amp;#34;: [
 {&amp;#34;left&amp;#34;: &amp;#34;$$&amp;#34;, &amp;#34;right&amp;#34;: &amp;#34;$$&amp;#34;, &amp;#34;display&amp;#34;: true},
 {&amp;#34;left&amp;#34;: &amp;#34;$&amp;#34;, &amp;#34;right&amp;#34;: &amp;#34;$&amp;#34;, &amp;#34;display&amp;#34;: false},
 {&amp;#34;left&amp;#34;: &amp;#34;\\(&amp;#34;, &amp;#34;right&amp;#34;: &amp;#34;\\)&amp;#34;, &amp;#34;display&amp;#34;: false},
 {&amp;#34;left&amp;#34;: &amp;#34;\\[&amp;#34;, &amp;#34;right&amp;#34;: &amp;#34;\\]&amp;#34;, &amp;#34;display&amp;#34;: true}
 ],
 &amp;#34;throwOnError&amp;#34;: false,
 &amp;#34;strict&amp;#34;: false,
 &amp;#34;trust&amp;#34;: true,
 &amp;#34;macros&amp;#34;: {
 &amp;#34;\\label&amp;#34;: &amp;#34;\\htmlId{#1}{}&amp;#34;,
 &amp;#34;\\eqref&amp;#34;: &amp;#34;(\\ref{#1})&amp;#34;
 }
}
);"&gt;&lt;/script&gt;


&lt;h1 id="reward-models-in-rlhf"&gt;
 Reward Models in RLHF
 
 &lt;a class="anchor" href="#reward-models-in-rlhf"&gt;#&lt;/a&gt;
 
&lt;/h1&gt;
&lt;p&gt;As LLMs scale, their raw outputs (optimized primarily for next-token prediction) often diverge from expected traits. To enable RL fine-tuning from human feedbacks (RLHF), reward models are introduced as trainable proxies for human preference. Once trained, it can generalize preference signals to unseen inputs, making alignment more scalable by reducing reliance on slow and costly human annotations. It also allows flexible fine-tuning toward different objectives, such as helpfulness, truthfulness, or safety.&lt;/p&gt;</description></item></channel></rss>