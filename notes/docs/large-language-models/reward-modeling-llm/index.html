<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="


  



  Reward Models in RLHF
  
  #
  

As LLMs scale, their raw outputs (optimized primarily for next-token prediction) often diverge from expected traits. To enable RL fine-tuning from human feedbacks (RLHF), reward models are introduced as trainable proxies for human preference. Once trained, it can generalize preference signals to unseen inputs, making alignment more scalable by reducing reliance on slow and costly human annotations. It also allows flexible fine-tuning toward different objectives, such as helpfulness, truthfulness, or safety."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://lovelybuggies.github.io/notes/docs/large-language-models/reward-modeling-llm/"><meta property="og:site_name" content="LovelyBuggies' Blog"><meta property="og:title" content="Reward Modeling LLM"><meta property="og:description" content="Reward Models in RLHF # As LLMs scale, their raw outputs (optimized primarily for next-token prediction) often diverge from expected traits. To enable RL fine-tuning from human feedbacks (RLHF), reward models are introduced as trainable proxies for human preference. Once trained, it can generalize preference signals to unseen inputs, making alignment more scalable by reducing reliance on slow and costly human annotations. It also allows flexible fine-tuning toward different objectives, such as helpfulness, truthfulness, or safety."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2025-09-15T20:28:52-04:00"><title>Reward Modeling LLM | LovelyBuggies' Blog</title><link rel=icon href=/notes/favicon.png><link rel=manifest href=/notes/manifest.json><link rel=canonical href=https://lovelybuggies.github.io/notes/docs/large-language-models/reward-modeling-llm/><link rel=stylesheet href=/notes/book.min.cfa0a6bf023bcd59c1c86d9df090c59d41586b44bea15a26034358dd3eeb470c.css integrity="sha256-z6CmvwI7zVnByG2d8JDFnUFYa0S+oVomA0NY3T7rRww=" crossorigin=anonymous><script defer src=/notes/fuse.min.js></script><script defer src=/notes/en.search.min.c4e41d6353dcb70087c9c1870f5f42dd96c8f4fdb8a1052606feac18409814c0.js integrity="sha256-xOQdY1PctwCHycGHD19C3ZbI9P24oQUmBv6sGECYFMA=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-docs book-layout-"><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>LovelyBuggies' Blog</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><span>RL</span><ul><li><a href=/notes/docs/rl/from-pg-2-ppo/>From PG 2 PPO</a></li><li><a href=/notes/docs/rl/understanding-bellman/>Understanding Bellman</a></li><li><span>MARL</span><ul><li><a href=/notes/docs/rl/marl/introduction-2-decpomdp/>Introduction 2 DecPOMDP</a></li><li><a href=/notes/docs/rl/marl/optimal-q-value-functions-for-dec-pomdp/>Optimal Q Value Functions for Dec POMDP</a></li></ul></li></ul></li><li><span>LLM</span><ul><li><a href=/notes/docs/large-language-models/reward-modeling-llm/ class=active>Reward Modeling LLM</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/notes/svg/menu.svg class=book-icon alt=Menu></label><h3>Reward Modeling LLM</h3><label for=toc-control><img src=/notes/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents></nav></aside></header><article class="markdown book-article"><link rel=stylesheet href=/notes/katex/katex.min.css><script defer src=/notes/katex/katex.min.js></script><script defer src=/notes/katex/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><h1 id=reward-models-in-rlhf>Reward Models in RLHF
<a class=anchor href=#reward-models-in-rlhf>#</a></h1><p>As LLMs scale, their raw outputs (optimized primarily for next-token prediction) often diverge from expected traits. To enable RL fine-tuning from human feedbacks (RLHF), reward models are introduced as trainable proxies for human preference. Once trained, it can generalize preference signals to unseen inputs, making alignment more scalable by reducing reliance on slow and costly human annotations. It also allows flexible fine-tuning toward different objectives, such as helpfulness, truthfulness, or safety.</p><p>A typical alignment pipeline consists of 3 stages: supervised fine-tuning (SFT), reward modeling, and RL. After an initial SFT based on base transformer with curated human-labeled data, a reward model is constructed to predict human preferences over model-generated responses. This model is then used to guide further optimization by encouraging outputs that maximize the predicted reward. For example, <code>ChatGPT</code> employs reward models trained on ranked annotations to guide its generation toward preferred outputs (Achiam, Adler, and Agarwal 2024); <code>DeepSeek</code> and <code>LLaMA 2</code> include explicit reward modeling components in their alignment pipelines, using pairwise preferences to train reward models that inform subsequent learning (Shao, Wang, and Zhu 2024; Touvron, Martin, and Stone 2023).</p><figure><img src=RLHF.png><figcaption>Role of the reward models in RLHF (reward model <span style=color:blue>in blue</span>).</figcaption></figure><div id=refs class="references csl-bib-body hanging-indent"><div id=ref-openai2024gpt4technicalreport class=csl-entry><p>Achiam, Josh, Steven Adler, and Sandhini Agarwal. 2024. “GPT-4 Technical Report.” <a href=https://arxiv.org/abs/2303.08774>https://arxiv.org/abs/2303.08774</a>.</p></div><div id=ref-shao2024deepseekmathpushinglimitsmathematical class=csl-entry><p>Shao, Zhihong, Peiyi Wang, and Qihao Zhu. 2024. “DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.” <a href=https://arxiv.org/abs/2402.03300>https://arxiv.org/abs/2402.03300</a>.</p></div><div id=ref-llama class=csl-entry><p>Touvron, Hugo, Louis Martin, and Kevin Stone. 2023. “Llama 2: Open Foundation and Fine-Tuned Chat Models.” <a href=https://arxiv.org/abs/2307.09288>https://arxiv.org/abs/2307.09288</a>.</p></div></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><div class="flex flex-wrap justify-between"><span><a href=/notes/docs/rl/marl/optimal-q-value-functions-for-dec-pomdp/ class="flex align-center"><img src=/notes/svg/backward.svg class=book-icon alt=Previous title="Optimal Q Value Functions for Dec POMDP">
<span>Optimal Q Value Functions for Dec POMDP</span>
</a></span><span></span></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents></nav></div></aside></main></body></html>